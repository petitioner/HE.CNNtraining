{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rRvGDbpvVEIU",
        "outputId": "0e96efb2-48fa-41d2-c5e0-cbcbcfbb0344"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 329090998.09it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 109625062.28it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 143991076.34it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5764153.94it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=RegNet_X_400MF_Weights.IMAGENET1K_V1`. You can also use `weights=RegNet_X_400MF_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/regnet_x_400mf-adf1edd5.pth\" to /root/.cache/torch/hub/checkpoints/regnet_x_400mf-adf1edd5.pth\n",
            "100%|██████████| 21.3M/21.3M [00:00<00:00, 25.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RegNet(\n",
            "  (stem): SimpleStemIN(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (trunk_output): Sequential(\n",
            "    (block1): AnyStage(\n",
            "      (block1-0): ResBottleneckBlock(\n",
            "        (proj): Conv2dNormActivation(\n",
            "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=2, bias=False)\n",
            "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (block2): AnyStage(\n",
            "      (block2-0): ResBottleneckBlock(\n",
            "        (proj): Conv2dNormActivation(\n",
            "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=4, bias=False)\n",
            "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block2-1): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
            "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (block3): AnyStage(\n",
            "      (block3-0): ResBottleneckBlock(\n",
            "        (proj): Conv2dNormActivation(\n",
            "          (0): Conv2d(64, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(64, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=10, bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block3-1): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block3-2): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block3-3): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block3-4): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block3-5): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block3-6): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (block4): AnyStage(\n",
            "      (block4-0): ResBottleneckBlock(\n",
            "        (proj): Conv2dNormActivation(\n",
            "          (0): Conv2d(160, 400, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(160, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block4-1): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block4-2): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block4-3): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block4-4): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block4-5): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block4-6): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block4-7): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block4-8): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block4-9): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block4-10): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "      (block4-11): ResBottleneckBlock(\n",
            "        (f): BottleneckTransform(\n",
            "          (a): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (b): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=25, bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (c): Conv2dNormActivation(\n",
            "            (0): Conv2d(400, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (activation): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Sequential()\n",
            ")\n",
            "Sequential()\n",
            "<class 'torch.nn.modules.container.Sequential'>\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 512, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 1024, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 1536, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 2048, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 2560, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 3072, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 3584, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 4096, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 4608, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 5120, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 5632, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 6144, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 6656, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 7168, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 7680, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 8192, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 8704, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 9216, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 9728, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 10240, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 10752, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 11264, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 11776, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 12288, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 12800, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 13312, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 13824, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 14336, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 14848, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 15360, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 15872, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 16384, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 16896, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 17408, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 17920, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 18432, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 18944, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 19456, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 19968, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 20480, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 20992, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 21504, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 22016, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 22528, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 23040, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 23552, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 24064, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 24576, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 25088, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 25600, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 26112, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 26624, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 27136, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 27648, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 28160, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 28672, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 29184, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 29696, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 30208, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 30720, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 31232, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 31744, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 32256, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 32768, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 33280, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 33792, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 34304, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 34816, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 35328, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 35840, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 36352, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 36864, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 37376, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 37888, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 38400, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 38912, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 39424, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 39936, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 40448, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 40960, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 41472, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 41984, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 42496, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 43008, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 43520, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 44032, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 44544, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 45056, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 45568, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 46080, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 46592, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 47104, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 47616, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 48128, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 48640, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 49152, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 49664, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 50176, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 50688, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 51200, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 51712, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 52224, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 52736, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 53248, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 53760, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 54272, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 54784, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 55296, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 55808, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 56320, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 56832, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 57344, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 57856, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 58368, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 58880, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 59392, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of train_dataset: 59904, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([96, 400])\n",
            "Shape of y: torch.Size([96]) torch.int64\n",
            "Shape of model(X): torch.Size([96, 400]) torch.float32\n",
            "Shape of y: torch.Size([96, 1]) torch.int64\n",
            "Shape of train_dataset: 60000, 401\n",
            "train_dataset :60000,\t401\n",
            "<class 'list'>\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 512, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 1024, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 1536, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 2048, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 2560, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 3072, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 3584, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 4096, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 4608, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 5120, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 5632, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 6144, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 6656, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 7168, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 7680, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 8192, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 8704, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 9216, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([512, 400])\n",
            "Shape of y: torch.Size([512]) torch.int64\n",
            "Shape of model(X): torch.Size([512, 400]) torch.float32\n",
            "Shape of y: torch.Size([512, 1]) torch.int64\n",
            "Shape of test_dataset: 9728, 401\n",
            "Shape of model(X) [N, C, H, W]: torch.Size([272, 400])\n",
            "Shape of y: torch.Size([272]) torch.int64\n",
            "Shape of model(X): torch.Size([272, 400]) torch.float32\n",
            "Shape of y: torch.Size([272, 1]) torch.int64\n",
            "Shape of test_dataset: 10000, 401\n",
            "test_dataset :10000,\t401\n",
            "<class 'list'>\n",
            " Testing Accuray at  1 iterations is 0.566900000000 with loss: 0.227601644372\n",
            " Testing Accuray at  2 iterations is 0.092200000000 with loss: 1.753247010022\n",
            " Testing Accuray at  3 iterations is 0.104000000000 with loss: 2.486953018890\n",
            " Testing Accuray at  4 iterations is 0.153000000000 with loss: 3.066949663345\n",
            " Testing Accuray at  5 iterations is 0.322700000000 with loss: 3.997190366066\n",
            " Testing Accuray at  6 iterations is 0.113500000000 with loss: 4.406391936260\n",
            " Testing Accuray at  7 iterations is 0.120000000000 with loss: 4.630136310228\n",
            " Testing Accuray at  8 iterations is 0.103400000000 with loss: 5.669118406585\n",
            " Testing Accuray at  9 iterations is 0.135900000000 with loss: 5.457291863086\n",
            " Testing Accuray at 10 iterations is 0.198900000000 with loss: 5.417728045706\n",
            " Testing Accuray at 11 iterations is 0.186500000000 with loss: 4.332322041892\n",
            " Testing Accuray at 12 iterations is 0.095800000000 with loss: 5.176204536276\n",
            " Testing Accuray at 13 iterations is 0.239200000000 with loss: 1.386111654716\n",
            " Testing Accuray at 14 iterations is 0.293700000000 with loss: 1.390985452207\n",
            " Testing Accuray at 15 iterations is 0.357400000000 with loss: 1.826042023159\n",
            " Testing Accuray at 16 iterations is 0.438100000000 with loss: 1.988751765352\n",
            " Testing Accuray at 17 iterations is 0.543700000000 with loss: 2.130294760587\n",
            " Testing Accuray at 18 iterations is 0.387400000000 with loss: 2.491031336065\n",
            " Testing Accuray at 19 iterations is 0.483500000000 with loss: 2.282188089524\n",
            " Testing Accuray at 20 iterations is 0.511600000000 with loss: 2.099483067570\n",
            " Testing Accuray at 21 iterations is 0.510300000000 with loss: 1.900597570463\n",
            " Testing Accuray at 22 iterations is 0.508800000000 with loss: 1.753336017213\n",
            " Testing Accuray at 23 iterations is 0.555800000000 with loss: 1.534180599430\n",
            " Testing Accuray at 24 iterations is 0.524500000000 with loss: 1.439510533445\n",
            " Testing Accuray at 25 iterations is 0.458400000000 with loss: 1.388255276576\n",
            " Testing Accuray at 26 iterations is 0.470900000000 with loss: 1.138776437494\n",
            " Testing Accuray at 27 iterations is 0.577600000000 with loss: 0.816258188721\n",
            " Testing Accuray at 28 iterations is 0.621200000000 with loss: 0.690529466309\n",
            " Testing Accuray at 29 iterations is 0.442100000000 with loss: 0.857612878331\n",
            " Testing Accuray at 30 iterations is 0.364500000000 with loss: 0.940012328386\n",
            " Testing Accuray at 31 iterations is 0.414100000000 with loss: 0.690446674176\n",
            " Testing Accuray at 32 iterations is 0.535800000000 with loss: 0.393766891435\n",
            " Testing Accuray at 33 iterations is 0.630000000000 with loss: 0.243918376632\n",
            " Testing Accuray at 34 iterations is 0.607800000000 with loss: 0.267350460126\n",
            " Testing Accuray at 35 iterations is 0.425100000000 with loss: 0.556316134159\n",
            " Testing Accuray at 36 iterations is 0.318200000000 with loss: 0.886677118162\n",
            " Testing Accuray at 37 iterations is 0.371300000000 with loss: 0.723356681202\n",
            " Testing Accuray at 38 iterations is 0.570100000000 with loss: 0.359023911045\n",
            " Testing Accuray at 39 iterations is 0.705800000000 with loss: 0.250573335012\n",
            " Testing Accuray at 40 iterations is 0.684100000000 with loss: 0.265886111273\n",
            " Testing Accuray at 41 iterations is 0.669300000000 with loss: 0.286555596667\n",
            " Testing Accuray at 42 iterations is 0.677000000000 with loss: 0.300899787893\n",
            " Testing Accuray at 43 iterations is 0.694900000000 with loss: 0.309054870786\n",
            " Testing Accuray at 44 iterations is 0.699400000000 with loss: 0.319172168295\n",
            " Testing Accuray at 45 iterations is 0.699200000000 with loss: 0.331801002991\n",
            " Testing Accuray at 46 iterations is 0.700600000000 with loss: 0.339213805588\n",
            " Testing Accuray at 47 iterations is 0.705500000000 with loss: 0.336190508688\n",
            " Testing Accuray at 48 iterations is 0.706900000000 with loss: 0.325990315379\n",
            " Testing Accuray at 49 iterations is 0.704700000000 with loss: 0.312925499189\n",
            " Testing Accuray at 50 iterations is 0.695800000000 with loss: 0.296731908575\n",
            " Testing Accuray at 51 iterations is 0.695400000000 with loss: 0.273180187850\n",
            " Testing Accuray at 52 iterations is 0.704400000000 with loss: 0.237700772123\n",
            " Testing Accuray at 53 iterations is 0.727300000000 with loss: 0.192412014210\n",
            " Testing Accuray at 54 iterations is 0.758800000000 with loss: 0.146974241267\n",
            " Testing Accuray at 55 iterations is 0.795400000000 with loss: 0.113337315579\n",
            " Testing Accuray at 56 iterations is 0.804600000000 with loss: 0.100939076731\n",
            " Testing Accuray at 57 iterations is 0.792200000000 with loss: 0.113195953893\n",
            " Testing Accuray at 58 iterations is 0.761200000000 with loss: 0.146339485894\n",
            " Testing Accuray at 59 iterations is 0.722600000000 with loss: 0.187776935315\n",
            " Testing Accuray at 60 iterations is 0.693200000000 with loss: 0.218656116371\n",
            " Testing Accuray at 61 iterations is 0.690000000000 with loss: 0.223337504552\n",
            " Testing Accuray at 62 iterations is 0.713800000000 with loss: 0.201043721139\n",
            " Testing Accuray at 63 iterations is 0.748900000000 with loss: 0.165502988518\n",
            " Testing Accuray at 64 iterations is 0.787700000000 with loss: 0.133070484923\n",
            " Testing Accuray at 65 iterations is 0.817900000000 with loss: 0.112506820467\n",
            " Testing Accuray at 66 iterations is 0.830600000000 with loss: 0.102764970107\n",
            " Testing Accuray at 67 iterations is 0.832400000000 with loss: 0.099780928402\n",
            " Testing Accuray at 68 iterations is 0.826300000000 with loss: 0.101173155761\n",
            " Testing Accuray at 69 iterations is 0.817600000000 with loss: 0.105127863109\n",
            " Testing Accuray at 70 iterations is 0.809100000000 with loss: 0.110446053593\n",
            " Testing Accuray at 71 iterations is 0.803500000000 with loss: 0.115956087292\n",
            " Testing Accuray at 72 iterations is 0.799000000000 with loss: 0.120907260089\n",
            " Testing Accuray at 73 iterations is 0.795600000000 with loss: 0.125237815089\n",
            " Testing Accuray at 74 iterations is 0.795300000000 with loss: 0.129145754288\n",
            " Testing Accuray at 75 iterations is 0.794200000000 with loss: 0.132584431616\n",
            " Testing Accuray at 76 iterations is 0.789900000000 with loss: 0.134879452459\n",
            " Testing Accuray at 77 iterations is 0.785700000000 with loss: 0.134939059351\n",
            " Testing Accuray at 78 iterations is 0.786600000000 with loss: 0.131774182221\n",
            " Testing Accuray at 79 iterations is 0.793000000000 with loss: 0.125053051009\n",
            " Testing Accuray at 80 iterations is 0.803900000000 with loss: 0.115309333605\n",
            " Testing Accuray at 81 iterations is 0.816900000000 with loss: 0.103786886475\n",
            " Testing Accuray at 82 iterations is 0.829300000000 with loss: 0.092065626543\n",
            " Testing Accuray at 83 iterations is 0.842800000000 with loss: 0.081662142339\n",
            " Testing Accuray at 84 iterations is 0.853000000000 with loss: 0.073652420352\n",
            " Testing Accuray at 85 iterations is 0.861100000000 with loss: 0.068465285339\n",
            " Testing Accuray at 86 iterations is 0.864300000000 with loss: 0.065989789239\n",
            " Testing Accuray at 87 iterations is 0.864900000000 with loss: 0.065839153170\n",
            " Testing Accuray at 88 iterations is 0.862700000000 with loss: 0.067644413860\n",
            " Testing Accuray at 89 iterations is 0.856100000000 with loss: 0.071145282382\n",
            " Testing Accuray at 90 iterations is 0.848800000000 with loss: 0.076022399760\n",
            " Testing Accuray at 91 iterations is 0.838700000000 with loss: 0.081718333543\n",
            " Testing Accuray at 92 iterations is 0.831000000000 with loss: 0.087362102776\n",
            " Testing Accuray at 93 iterations is 0.824600000000 with loss: 0.091909627247\n",
            " Testing Accuray at 94 iterations is 0.821100000000 with loss: 0.094390719183\n",
            " Testing Accuray at 95 iterations is 0.820900000000 with loss: 0.094170082115\n",
            " Testing Accuray at 96 iterations is 0.823300000000 with loss: 0.091126469819\n",
            " Testing Accuray at 97 iterations is 0.829700000000 with loss: 0.085667911697\n",
            " Testing Accuray at 98 iterations is 0.838600000000 with loss: 0.078609359425\n",
            " Testing Accuray at 99 iterations is 0.851300000000 with loss: 0.070975484026\n",
            " Testing Accuray at 100 iterations is 0.864400000000 with loss: 0.063767064707\n",
            " Testing Accuray at 101 iterations is 0.873100000000 with loss: 0.057751383866\n",
            " Testing Accuray at 102 iterations is 0.880700000000 with loss: 0.053381505835\n",
            " Testing Accuray at 103 iterations is 0.884500000000 with loss: 0.050830583678\n",
            " Testing Accuray at 104 iterations is 0.886700000000 with loss: 0.050047855805\n",
            " Testing Accuray at 105 iterations is 0.884000000000 with loss: 0.050842047088\n",
            " Testing Accuray at 106 iterations is 0.880700000000 with loss: 0.052853386916\n",
            " Testing Accuray at 107 iterations is 0.877200000000 with loss: 0.055602108735\n",
            " Testing Accuray at 108 iterations is 0.872500000000 with loss: 0.058589818527\n",
            " Testing Accuray at 109 iterations is 0.868600000000 with loss: 0.061393828750\n",
            " Testing Accuray at 110 iterations is 0.865700000000 with loss: 0.063742922435\n",
            " Testing Accuray at 111 iterations is 0.863300000000 with loss: 0.065523570831\n",
            " Testing Accuray at 112 iterations is 0.859100000000 with loss: 0.066740884548\n",
            " Testing Accuray at 113 iterations is 0.858500000000 with loss: 0.067462542529\n",
            " Testing Accuray at 114 iterations is 0.860300000000 with loss: 0.067756799979\n",
            " Testing Accuray at 115 iterations is 0.858500000000 with loss: 0.067646825105\n",
            " Testing Accuray at 116 iterations is 0.857800000000 with loss: 0.067108493681\n",
            " Testing Accuray at 117 iterations is 0.859200000000 with loss: 0.066109762410\n",
            " Testing Accuray at 118 iterations is 0.861000000000 with loss: 0.064661451893\n",
            " Testing Accuray at 119 iterations is 0.864100000000 with loss: 0.062846129408\n",
            " Testing Accuray at 120 iterations is 0.868300000000 with loss: 0.060811944882\n",
            " Testing Accuray at 121 iterations is 0.871000000000 with loss: 0.058738809368\n",
            " Testing Accuray at 122 iterations is 0.874000000000 with loss: 0.056793609614\n",
            " Testing Accuray at 123 iterations is 0.878600000000 with loss: 0.055092237110\n",
            " Testing Accuray at 124 iterations is 0.879200000000 with loss: 0.053681694921\n",
            " Testing Accuray at 125 iterations is 0.881100000000 with loss: 0.052543214919\n",
            " Testing Accuray at 126 iterations is 0.883000000000 with loss: 0.051611090216\n",
            " Testing Accuray at 127 iterations is 0.884100000000 with loss: 0.050798869084\n",
            " Testing Accuray at 128 iterations is 0.885400000000 with loss: 0.050025166384\n",
            " Testing Accuray at 129 iterations is 0.886700000000 with loss: 0.049235698267\n",
            " Testing Accuray at 130 iterations is 0.886800000000 with loss: 0.048417946499\n",
            " Testing Accuray at 131 iterations is 0.889200000000 with loss: 0.047603648040\n",
            " Testing Accuray at 132 iterations is 0.891400000000 with loss: 0.046852761595\n",
            " Testing Accuray at 133 iterations is 0.892500000000 with loss: 0.046231175170\n",
            " Testing Accuray at 134 iterations is 0.893700000000 with loss: 0.045795880957\n",
            " Testing Accuray at 135 iterations is 0.893700000000 with loss: 0.045584394547\n",
            " Testing Accuray at 136 iterations is 0.892500000000 with loss: 0.045605524503\n",
            " Testing Accuray at 137 iterations is 0.890800000000 with loss: 0.045833555862\n",
            " Testing Accuray at 138 iterations is 0.889700000000 with loss: 0.046210056732\n",
            " Testing Accuray at 139 iterations is 0.888800000000 with loss: 0.046656066555\n",
            " Testing Accuray at 140 iterations is 0.888000000000 with loss: 0.047090416476\n",
            " Testing Accuray at 141 iterations is 0.887700000000 with loss: 0.047447930024\n",
            " Testing Accuray at 142 iterations is 0.887100000000 with loss: 0.047693271817\n",
            " Testing Accuray at 143 iterations is 0.886700000000 with loss: 0.047825933841\n",
            " Testing Accuray at 144 iterations is 0.886600000000 with loss: 0.047872781713\n",
            " Testing Accuray at 145 iterations is 0.886300000000 with loss: 0.047870072180\n",
            " Testing Accuray at 146 iterations is 0.886200000000 with loss: 0.047842433359\n",
            " Testing Accuray at 147 iterations is 0.886200000000 with loss: 0.047788121395\n",
            " Testing Accuray at 148 iterations is 0.887800000000 with loss: 0.047675991548\n",
            " Testing Accuray at 149 iterations is 0.886600000000 with loss: 0.047453748163\n",
            " Testing Accuray at 150 iterations is 0.888700000000 with loss: 0.047063868048\n",
            " Testing Accuray at 151 iterations is 0.888900000000 with loss: 0.046462386751\n",
            " Testing Accuray at 152 iterations is 0.890900000000 with loss: 0.045635269705\n",
            " Testing Accuray at 153 iterations is 0.893800000000 with loss: 0.044607659472\n",
            " Testing Accuray at 154 iterations is 0.896200000000 with loss: 0.043443218696\n",
            " Testing Accuray at 155 iterations is 0.899000000000 with loss: 0.042233744339\n",
            " Testing Accuray at 156 iterations is 0.901100000000 with loss: 0.041082029476\n",
            " Testing Accuray at 157 iterations is 0.905300000000 with loss: 0.040082771135\n",
            " Testing Accuray at 158 iterations is 0.906000000000 with loss: 0.039306950109\n",
            " Testing Accuray at 159 iterations is 0.907700000000 with loss: 0.038793547059\n",
            " Testing Accuray at 160 iterations is 0.908300000000 with loss: 0.038548645365\n",
            " Testing Accuray at 161 iterations is 0.908700000000 with loss: 0.038549515111\n",
            " Testing Accuray at 162 iterations is 0.908500000000 with loss: 0.038751660799\n",
            " Testing Accuray at 163 iterations is 0.907600000000 with loss: 0.039097661850\n",
            " Testing Accuray at 164 iterations is 0.905600000000 with loss: 0.039526754555\n",
            " Testing Accuray at 165 iterations is 0.904300000000 with loss: 0.039983535765\n",
            " Testing Accuray at 166 iterations is 0.902900000000 with loss: 0.040423862544\n",
            " Testing Accuray at 167 iterations is 0.902600000000 with loss: 0.040817118867\n",
            " Testing Accuray at 168 iterations is 0.902400000000 with loss: 0.041145318636\n",
            " Testing Accuray at 169 iterations is 0.902200000000 with loss: 0.041400002989\n",
            " Testing Accuray at 170 iterations is 0.901600000000 with loss: 0.041578054106\n",
            " Testing Accuray at 171 iterations is 0.900300000000 with loss: 0.041677613567\n",
            " Testing Accuray at 172 iterations is 0.899700000000 with loss: 0.041695129784\n",
            " Testing Accuray at 173 iterations is 0.901300000000 with loss: 0.041624164170\n",
            " Testing Accuray at 174 iterations is 0.901800000000 with loss: 0.041456091535\n",
            " Testing Accuray at 175 iterations is 0.901300000000 with loss: 0.041182374888\n",
            " Testing Accuray at 176 iterations is 0.901700000000 with loss: 0.040797753869\n",
            " Testing Accuray at 177 iterations is 0.902600000000 with loss: 0.040303495522\n",
            " Testing Accuray at 178 iterations is 0.902700000000 with loss: 0.039709844649\n",
            " Testing Accuray at 179 iterations is 0.903600000000 with loss: 0.039036989431\n",
            " Testing Accuray at 180 iterations is 0.905300000000 with loss: 0.038314198932\n",
            " Testing Accuray at 181 iterations is 0.906400000000 with loss: 0.037577225628\n",
            " Testing Accuray at 182 iterations is 0.907700000000 with loss: 0.036864491652\n",
            " Testing Accuray at 183 iterations is 0.909600000000 with loss: 0.036212846684\n",
            " Testing Accuray at 184 iterations is 0.910500000000 with loss: 0.035653689786\n",
            " Testing Accuray at 185 iterations is 0.911000000000 with loss: 0.035210036449\n",
            " Testing Accuray at 186 iterations is 0.911300000000 with loss: 0.034894834670\n",
            " Testing Accuray at 187 iterations is 0.910500000000 with loss: 0.034710576622\n",
            " Testing Accuray at 188 iterations is 0.910000000000 with loss: 0.034650067888\n",
            " Testing Accuray at 189 iterations is 0.910500000000 with loss: 0.034698125225\n",
            " Testing Accuray at 190 iterations is 0.910800000000 with loss: 0.034833886036\n",
            " Testing Accuray at 191 iterations is 0.910300000000 with loss: 0.035033320164\n",
            " Testing Accuray at 192 iterations is 0.910400000000 with loss: 0.035271553586\n",
            " Testing Accuray at 193 iterations is 0.910300000000 with loss: 0.035524745053\n",
            " Testing Accuray at 194 iterations is 0.909600000000 with loss: 0.035771394502\n",
            " Testing Accuray at 195 iterations is 0.909500000000 with loss: 0.035993066260\n",
            " Testing Accuray at 196 iterations is 0.908500000000 with loss: 0.036174611067\n",
            " Testing Accuray at 197 iterations is 0.908300000000 with loss: 0.036304053410\n",
            " Testing Accuray at 198 iterations is 0.908300000000 with loss: 0.036372329214\n",
            " Testing Accuray at 199 iterations is 0.908300000000 with loss: 0.036373017462\n",
            " Testing Accuray at 200 iterations is 0.909300000000 with loss: 0.036302145556\n",
            " Testing Accuray at 201 iterations is 0.909300000000 with loss: 0.036158090268\n",
            " Testing Accuray at 202 iterations is 0.910600000000 with loss: 0.035941556862\n",
            " Testing Accuray at 203 iterations is 0.911600000000 with loss: 0.035655600487\n",
            " Testing Accuray at 204 iterations is 0.913000000000 with loss: 0.035305649646\n",
            " Testing Accuray at 205 iterations is 0.912500000000 with loss: 0.034899491511\n",
            " Testing Accuray at 206 iterations is 0.912600000000 with loss: 0.034447177065\n",
            " Testing Accuray at 207 iterations is 0.913600000000 with loss: 0.033960802312\n",
            " Testing Accuray at 208 iterations is 0.914100000000 with loss: 0.033454127568\n",
            " Testing Accuray at 209 iterations is 0.915200000000 with loss: 0.032942017293\n",
            " Testing Accuray at 210 iterations is 0.915700000000 with loss: 0.032439717321\n",
            " Testing Accuray at 211 iterations is 0.916800000000 with loss: 0.031962023551\n",
            " Testing Accuray at 212 iterations is 0.918300000000 with loss: 0.031522422150\n",
            " Testing Accuray at 213 iterations is 0.918400000000 with loss: 0.031132289039\n",
            " Testing Accuray at 214 iterations is 0.919400000000 with loss: 0.030800229316\n",
            " Testing Accuray at 215 iterations is 0.919500000000 with loss: 0.030531622444\n",
            " Testing Accuray at 216 iterations is 0.919900000000 with loss: 0.030328420058\n",
            " Testing Accuray at 217 iterations is 0.918900000000 with loss: 0.030189220775\n",
            " Testing Accuray at 218 iterations is 0.918700000000 with loss: 0.030109619766\n",
            " Testing Accuray at 219 iterations is 0.918300000000 with loss: 0.030082796955\n",
            " Testing Accuray at 220 iterations is 0.918100000000 with loss: 0.030100270684\n",
            " Testing Accuray at 221 iterations is 0.917800000000 with loss: 0.030152721280\n",
            " Testing Accuray at 222 iterations is 0.918800000000 with loss: 0.030230791048\n",
            " Testing Accuray at 223 iterations is 0.918600000000 with loss: 0.030325776900\n",
            " Testing Accuray at 224 iterations is 0.917700000000 with loss: 0.030430137034\n",
            " Testing Accuray at 225 iterations is 0.917600000000 with loss: 0.030537753190\n",
            " Testing Accuray at 226 iterations is 0.917000000000 with loss: 0.030643936754\n",
            " Testing Accuray at 227 iterations is 0.918300000000 with loss: 0.030745218732\n",
            " Testing Accuray at 228 iterations is 0.918600000000 with loss: 0.030838996985\n",
            " Testing Accuray at 229 iterations is 0.918500000000 with loss: 0.030923127292\n",
            " Testing Accuray at 230 iterations is 0.918000000000 with loss: 0.030995544188\n",
            " Testing Accuray at 231 iterations is 0.917300000000 with loss: 0.031053985481\n",
            " Testing Accuray at 232 iterations is 0.917800000000 with loss: 0.031095871257\n",
            " Testing Accuray at 233 iterations is 0.918300000000 with loss: 0.031118357470\n",
            " Testing Accuray at 234 iterations is 0.917900000000 with loss: 0.031118551422\n",
            " Testing Accuray at 235 iterations is 0.918100000000 with loss: 0.031093846872\n",
            " Testing Accuray at 236 iterations is 0.918700000000 with loss: 0.031042314320\n",
            " Testing Accuray at 237 iterations is 0.918900000000 with loss: 0.030963070488\n",
            " Testing Accuray at 238 iterations is 0.919000000000 with loss: 0.030856552380\n",
            " Testing Accuray at 239 iterations is 0.920100000000 with loss: 0.030724635765\n",
            " Testing Accuray at 240 iterations is 0.919900000000 with loss: 0.030570563702\n",
            " Testing Accuray at 241 iterations is 0.920000000000 with loss: 0.030398683143\n",
            " Testing Accuray at 242 iterations is 0.920200000000 with loss: 0.030214020593\n",
            " Testing Accuray at 243 iterations is 0.921400000000 with loss: 0.030021755174\n",
            " Testing Accuray at 244 iterations is 0.922000000000 with loss: 0.029826664997\n",
            " Testing Accuray at 245 iterations is 0.922000000000 with loss: 0.029632628341\n",
            " Testing Accuray at 246 iterations is 0.922400000000 with loss: 0.029442253951\n",
            " Testing Accuray at 247 iterations is 0.922900000000 with loss: 0.029256695893\n",
            " Testing Accuray at 248 iterations is 0.923200000000 with loss: 0.029075680283\n",
            " Testing Accuray at 249 iterations is 0.923300000000 with loss: 0.028897738584\n",
            " Testing Accuray at 250 iterations is 0.923100000000 with loss: 0.028720610379\n",
            " Testing Accuray at 251 iterations is 0.923500000000 with loss: 0.028541753134\n",
            " Testing Accuray at 252 iterations is 0.923800000000 with loss: 0.028358881779\n",
            " Testing Accuray at 253 iterations is 0.924900000000 with loss: 0.028170458990\n",
            " Testing Accuray at 254 iterations is 0.925400000000 with loss: 0.027976067871\n",
            " Testing Accuray at 255 iterations is 0.925500000000 with loss: 0.027776619633\n",
            " Testing Accuray at 256 iterations is 0.925700000000 with loss: 0.027574375734\n",
            " Testing Accuray at 257 iterations is 0.925500000000 with loss: 0.027372791971\n",
            " Testing Accuray at 258 iterations is 0.924900000000 with loss: 0.027176216434\n",
            " Testing Accuray at 259 iterations is 0.925800000000 with loss: 0.026989490462\n",
            " Testing Accuray at 260 iterations is 0.926000000000 with loss: 0.026817509629\n",
            " Testing Accuray at 261 iterations is 0.925900000000 with loss: 0.026664800217\n",
            " Testing Accuray at 262 iterations is 0.926600000000 with loss: 0.026535157416\n",
            " Testing Accuray at 263 iterations is 0.927800000000 with loss: 0.026431377659\n",
            " Testing Accuray at 264 iterations is 0.927200000000 with loss: 0.026355101780\n",
            " Testing Accuray at 265 iterations is 0.926900000000 with loss: 0.026306770343\n",
            " Testing Accuray at 266 iterations is 0.926900000000 with loss: 0.026285679409\n",
            " Testing Accuray at 267 iterations is 0.926400000000 with loss: 0.026290115510\n",
            " Testing Accuray at 268 iterations is 0.927200000000 with loss: 0.026317543174\n",
            " Testing Accuray at 269 iterations is 0.927300000000 with loss: 0.026364816903\n",
            " Testing Accuray at 270 iterations is 0.926700000000 with loss: 0.026428392039\n",
            " Testing Accuray at 271 iterations is 0.926900000000 with loss: 0.026504514923\n",
            " Testing Accuray at 272 iterations is 0.926100000000 with loss: 0.026589380715\n",
            " Testing Accuray at 273 iterations is 0.925100000000 with loss: 0.026679255009\n",
            " Testing Accuray at 274 iterations is 0.924400000000 with loss: 0.026770561228\n",
            " Testing Accuray at 275 iterations is 0.924200000000 with loss: 0.026859939157\n",
            " Testing Accuray at 276 iterations is 0.924600000000 with loss: 0.026944281180\n",
            " Testing Accuray at 277 iterations is 0.924100000000 with loss: 0.027020752611\n",
            " Testing Accuray at 278 iterations is 0.923600000000 with loss: 0.027086801605\n",
            " Testing Accuray at 279 iterations is 0.923400000000 with loss: 0.027140163275\n",
            " Testing Accuray at 280 iterations is 0.923500000000 with loss: 0.027178862061\n",
            " Testing Accuray at 281 iterations is 0.923800000000 with loss: 0.027201216134\n",
            " Testing Accuray at 282 iterations is 0.924200000000 with loss: 0.027205847487\n",
            " Testing Accuray at 283 iterations is 0.924000000000 with loss: 0.027191700845\n",
            " Testing Accuray at 284 iterations is 0.924500000000 with loss: 0.027158073332\n",
            " Testing Accuray at 285 iterations is 0.924400000000 with loss: 0.027104654697\n",
            " Testing Accuray at 286 iterations is 0.924300000000 with loss: 0.027031575107\n",
            " Testing Accuray at 287 iterations is 0.924300000000 with loss: 0.026939454389\n",
            " Testing Accuray at 288 iterations is 0.924400000000 with loss: 0.026829443954\n",
            " Testing Accuray at 289 iterations is 0.924700000000 with loss: 0.026703251010\n",
            " Testing Accuray at 290 iterations is 0.925300000000 with loss: 0.026563134782\n",
            " Testing Accuray at 291 iterations is 0.926000000000 with loss: 0.026411866458\n",
            " Testing Accuray at 292 iterations is 0.926300000000 with loss: 0.026252648374\n",
            " Testing Accuray at 293 iterations is 0.926500000000 with loss: 0.026088993074\n",
            " Testing Accuray at 294 iterations is 0.926600000000 with loss: 0.025924568418\n",
            " Testing Accuray at 295 iterations is 0.927100000000 with loss: 0.025763020015\n",
            " Testing Accuray at 296 iterations is 0.927000000000 with loss: 0.025607785984\n",
            " Testing Accuray at 297 iterations is 0.927300000000 with loss: 0.025461920873\n",
            " Testing Accuray at 298 iterations is 0.928400000000 with loss: 0.025327945022\n",
            " Testing Accuray at 299 iterations is 0.927800000000 with loss: 0.025207733110\n",
            " Testing Accuray at 300 iterations is 0.927500000000 with loss: 0.025102451133\n",
            " Testing Accuray at 301 iterations is 0.928600000000 with loss: 0.025012545544\n",
            " Testing Accuray at 302 iterations is 0.928800000000 with loss: 0.024937782398\n",
            " Testing Accuray at 303 iterations is 0.929100000000 with loss: 0.024877328938\n",
            " Testing Accuray at 304 iterations is 0.929000000000 with loss: 0.024829865943\n",
            " Testing Accuray at 305 iterations is 0.928600000000 with loss: 0.024793716794\n",
            " Testing Accuray at 306 iterations is 0.928500000000 with loss: 0.024766978809\n",
            " Testing Accuray at 307 iterations is 0.928200000000 with loss: 0.024747643833\n",
            " Testing Accuray at 308 iterations is 0.927700000000 with loss: 0.024733698054\n",
            " Testing Accuray at 309 iterations is 0.928200000000 with loss: 0.024723195078\n",
            " Testing Accuray at 310 iterations is 0.928500000000 with loss: 0.024714300716\n",
            " Testing Accuray at 311 iterations is 0.927900000000 with loss: 0.024705312156\n",
            " Testing Accuray at 312 iterations is 0.928100000000 with loss: 0.024694657427\n",
            " Testing Accuray at 313 iterations is 0.928200000000 with loss: 0.024680883024\n",
            " Testing Accuray at 314 iterations is 0.928100000000 with loss: 0.024662638129\n",
            " Testing Accuray at 315 iterations is 0.928600000000 with loss: 0.024638663206\n",
            " Testing Accuray at 316 iterations is 0.928700000000 with loss: 0.024607789088\n",
            " Testing Accuray at 317 iterations is 0.929500000000 with loss: 0.024568950319\n",
            " Testing Accuray at 318 iterations is 0.930200000000 with loss: 0.024521213828\n",
            " Testing Accuray at 319 iterations is 0.930700000000 with loss: 0.024463821285\n",
            " Testing Accuray at 320 iterations is 0.930600000000 with loss: 0.024396241120\n",
            " Testing Accuray at 321 iterations is 0.930700000000 with loss: 0.024318224443\n",
            " Testing Accuray at 322 iterations is 0.930900000000 with loss: 0.024229858064\n",
            " Testing Accuray at 323 iterations is 0.931200000000 with loss: 0.024131607711\n",
            " Testing Accuray at 324 iterations is 0.931500000000 with loss: 0.024024345128\n",
            " Testing Accuray at 325 iterations is 0.931900000000 with loss: 0.023909354039\n",
            " Testing Accuray at 326 iterations is 0.932300000000 with loss: 0.023788311709\n",
            " Testing Accuray at 327 iterations is 0.933000000000 with loss: 0.023663244934\n",
            " Testing Accuray at 328 iterations is 0.932900000000 with loss: 0.023536461412\n",
            " Testing Accuray at 329 iterations is 0.932800000000 with loss: 0.023410459616\n",
            " Testing Accuray at 330 iterations is 0.932800000000 with loss: 0.023287822136\n",
            " Testing Accuray at 331 iterations is 0.933400000000 with loss: 0.023171098989\n",
            " Testing Accuray at 332 iterations is 0.933700000000 with loss: 0.023062688380\n",
            " Testing Accuray at 333 iterations is 0.934000000000 with loss: 0.022964722778\n",
            " Testing Accuray at 334 iterations is 0.934600000000 with loss: 0.022878967943\n",
            " Testing Accuray at 335 iterations is 0.934900000000 with loss: 0.022806741660\n",
            " Testing Accuray at 336 iterations is 0.935500000000 with loss: 0.022748857648\n",
            " Testing Accuray at 337 iterations is 0.935800000000 with loss: 0.022705598337\n",
            " Testing Accuray at 338 iterations is 0.935800000000 with loss: 0.022676718204\n",
            " Testing Accuray at 339 iterations is 0.936200000000 with loss: 0.022661477086\n",
            " Testing Accuray at 340 iterations is 0.936400000000 with loss: 0.022658700482\n",
            " Testing Accuray at 341 iterations is 0.936800000000 with loss: 0.022666861594\n",
            " Testing Accuray at 342 iterations is 0.936600000000 with loss: 0.022684177921\n",
            " Testing Accuray at 343 iterations is 0.936700000000 with loss: 0.022708714030\n",
            " Testing Accuray at 344 iterations is 0.936700000000 with loss: 0.022738481899\n",
            " Testing Accuray at 345 iterations is 0.936700000000 with loss: 0.022771531027\n",
            " Testing Accuray at 346 iterations is 0.936000000000 with loss: 0.022806022220\n",
            " Testing Accuray at 347 iterations is 0.935600000000 with loss: 0.022840281298\n",
            " Testing Accuray at 348 iterations is 0.934700000000 with loss: 0.022872831531\n",
            " Testing Accuray at 349 iterations is 0.934600000000 with loss: 0.022902406023\n",
            " Testing Accuray at 350 iterations is 0.934700000000 with loss: 0.022927943233\n",
            " Testing Accuray at 351 iterations is 0.933900000000 with loss: 0.022948570155\n",
            " Testing Accuray at 352 iterations is 0.933900000000 with loss: 0.022963578269\n",
            " Testing Accuray at 353 iterations is 0.934100000000 with loss: 0.022972397299\n",
            " Testing Accuray at 354 iterations is 0.933700000000 with loss: 0.022974571148\n",
            " Testing Accuray at 355 iterations is 0.933300000000 with loss: 0.022969739340\n",
            " Testing Accuray at 356 iterations is 0.933100000000 with loss: 0.022957625993\n",
            " Testing Accuray at 357 iterations is 0.933000000000 with loss: 0.022938037046\n",
            " Testing Accuray at 358 iterations is 0.932800000000 with loss: 0.022910865237\n",
            " Testing Accuray at 359 iterations is 0.931800000000 with loss: 0.022876101268\n",
            " Testing Accuray at 360 iterations is 0.931700000000 with loss: 0.022833848882\n",
            " Testing Accuray at 361 iterations is 0.932300000000 with loss: 0.022784341036\n",
            " Testing Accuray at 362 iterations is 0.932300000000 with loss: 0.022727954255\n",
            " Testing Accuray at 363 iterations is 0.932800000000 with loss: 0.022665218331\n",
            " Testing Accuray at 364 iterations is 0.933400000000 with loss: 0.022596818969\n",
            " Testing Accuray at 365 iterations is 0.933400000000 with loss: 0.022523591597\n",
            " Testing Accuray at 366 iterations is 0.933500000000 with loss: 0.022446505351\n",
            " Testing Accuray at 367 iterations is 0.933800000000 with loss: 0.022366637123\n",
            " Testing Accuray at 368 iterations is 0.934100000000 with loss: 0.022285136494\n",
            " Testing Accuray at 369 iterations is 0.934300000000 with loss: 0.022203183182\n",
            " Testing Accuray at 370 iterations is 0.934900000000 with loss: 0.022121939394\n",
            " Testing Accuray at 371 iterations is 0.935000000000 with loss: 0.022042500011\n",
            " Testing Accuray at 372 iterations is 0.935200000000 with loss: 0.021965843876\n",
            " Testing Accuray at 373 iterations is 0.935300000000 with loss: 0.021892789512\n",
            " Testing Accuray at 374 iterations is 0.934900000000 with loss: 0.021823958480\n",
            " Testing Accuray at 375 iterations is 0.934900000000 with loss: 0.021759749107\n",
            " Testing Accuray at 376 iterations is 0.935000000000 with loss: 0.021700322718\n",
            " Testing Accuray at 377 iterations is 0.935300000000 with loss: 0.021645603671\n",
            " Testing Accuray at 378 iterations is 0.935500000000 with loss: 0.021595293570\n",
            " Testing Accuray at 379 iterations is 0.936000000000 with loss: 0.021548899032\n",
            " Testing Accuray at 380 iterations is 0.936600000000 with loss: 0.021505771439\n",
            " Testing Accuray at 381 iterations is 0.936400000000 with loss: 0.021465156218\n",
            " Testing Accuray at 382 iterations is 0.937100000000 with loss: 0.021426248454\n",
            " Testing Accuray at 383 iterations is 0.937300000000 with loss: 0.021388251127\n",
            " Testing Accuray at 384 iterations is 0.937200000000 with loss: 0.021350431997\n",
            " Testing Accuray at 385 iterations is 0.937300000000 with loss: 0.021312175178\n",
            " Testing Accuray at 386 iterations is 0.937500000000 with loss: 0.021273023803\n",
            " Testing Accuray at 387 iterations is 0.937500000000 with loss: 0.021232710811\n",
            " Testing Accuray at 388 iterations is 0.937700000000 with loss: 0.021191175775\n",
            " Testing Accuray at 389 iterations is 0.937800000000 with loss: 0.021148566780\n",
            " Testing Accuray at 390 iterations is 0.938000000000 with loss: 0.021105227482\n",
            " Testing Accuray at 391 iterations is 0.938000000000 with loss: 0.021061670640\n",
            " Testing Accuray at 392 iterations is 0.938100000000 with loss: 0.021018540362\n",
            " Testing Accuray at 393 iterations is 0.937900000000 with loss: 0.020976566124\n",
            " Testing Accuray at 394 iterations is 0.938300000000 with loss: 0.020936512071\n",
            " Testing Accuray at 395 iterations is 0.938500000000 with loss: 0.020899125331\n",
            " Testing Accuray at 396 iterations is 0.938700000000 with loss: 0.020865086944\n",
            " Testing Accuray at 397 iterations is 0.939000000000 with loss: 0.020834968620\n",
            " Testing Accuray at 398 iterations is 0.939200000000 with loss: 0.020809197970\n",
            " Testing Accuray at 399 iterations is 0.939100000000 with loss: 0.020788034102\n",
            " Testing Accuray at 400 iterations is 0.939700000000 with loss: 0.020771554650\n",
            " Testing Accuray at 401 iterations is 0.939400000000 with loss: 0.020759654503\n",
            " Testing Accuray at 402 iterations is 0.939700000000 with loss: 0.020752055672\n",
            " Testing Accuray at 403 iterations is 0.939800000000 with loss: 0.020748327057\n",
            " Testing Accuray at 404 iterations is 0.940100000000 with loss: 0.020747912244\n",
            " Testing Accuray at 405 iterations is 0.940200000000 with loss: 0.020750163046\n",
            " Testing Accuray at 406 iterations is 0.940000000000 with loss: 0.020754376176\n",
            " Testing Accuray at 407 iterations is 0.940000000000 with loss: 0.020759830347\n",
            " Testing Accuray at 408 iterations is 0.939300000000 with loss: 0.020765821155\n",
            " Testing Accuray at 409 iterations is 0.938900000000 with loss: 0.020771691367\n",
            " Testing Accuray at 410 iterations is 0.938800000000 with loss: 0.020776854682\n",
            " Testing Accuray at 411 iterations is 0.938700000000 with loss: 0.020780811634\n",
            " Testing Accuray at 412 iterations is 0.938200000000 with loss: 0.020783156984\n",
            " Testing Accuray at 413 iterations is 0.938200000000 with loss: 0.020783578751\n",
            " Testing Accuray at 414 iterations is 0.938700000000 with loss: 0.020781849738\n",
            " Testing Accuray at 415 iterations is 0.938900000000 with loss: 0.020777813104\n",
            " Testing Accuray at 416 iterations is 0.938800000000 with loss: 0.020771364007\n",
            " Testing Accuray at 417 iterations is 0.939400000000 with loss: 0.020762429685\n",
            " Testing Accuray at 418 iterations is 0.939500000000 with loss: 0.020750950381\n",
            " Testing Accuray at 419 iterations is 0.939400000000 with loss: 0.020736863341\n",
            " Testing Accuray at 420 iterations is 0.939600000000 with loss: 0.020720091730\n",
            " Testing Accuray at 421 iterations is 0.939800000000 with loss: 0.020700539698\n",
            " Testing Accuray at 422 iterations is 0.940100000000 with loss: 0.020678094134\n",
            " Testing Accuray at 423 iterations is 0.940300000000 with loss: 0.020652632918\n",
            " Testing Accuray at 424 iterations is 0.940600000000 with loss: 0.020624038733\n",
            " Testing Accuray at 425 iterations is 0.940800000000 with loss: 0.020592216933\n",
            " Testing Accuray at 426 iterations is 0.941300000000 with loss: 0.020557115524\n",
            " Testing Accuray at 427 iterations is 0.941400000000 with loss: 0.020518745085\n",
            " Testing Accuray at 428 iterations is 0.941200000000 with loss: 0.020477196477\n",
            " Testing Accuray at 429 iterations is 0.941100000000 with loss: 0.020432654397\n",
            " Testing Accuray at 430 iterations is 0.941300000000 with loss: 0.020385405249\n",
            " Testing Accuray at 431 iterations is 0.941100000000 with loss: 0.020335838353\n",
            " Testing Accuray at 432 iterations is 0.940100000000 with loss: 0.020284440105\n",
            " Testing Accuray at 433 iterations is 0.940300000000 with loss: 0.020231781374\n",
            " Testing Accuray at 434 iterations is 0.940800000000 with loss: 0.020178498933\n",
            " Testing Accuray at 435 iterations is 0.941100000000 with loss: 0.020125272243\n",
            " Testing Accuray at 436 iterations is 0.940400000000 with loss: 0.020072797202\n",
            " Testing Accuray at 437 iterations is 0.940800000000 with loss: 0.020021758640\n",
            " Testing Accuray at 438 iterations is 0.940800000000 with loss: 0.019972803340\n",
            " Testing Accuray at 439 iterations is 0.941100000000 with loss: 0.019926515234\n",
            " Testing Accuray at 440 iterations is 0.941200000000 with loss: 0.019883394148\n",
            " Testing Accuray at 441 iterations is 0.941400000000 with loss: 0.019843839128\n",
            " Testing Accuray at 442 iterations is 0.941700000000 with loss: 0.019808136998\n",
            " Testing Accuray at 443 iterations is 0.942000000000 with loss: 0.019776456386\n",
            " Testing Accuray at 444 iterations is 0.941800000000 with loss: 0.019748847096\n",
            " Testing Accuray at 445 iterations is 0.941400000000 with loss: 0.019725244373\n",
            " Testing Accuray at 446 iterations is 0.941800000000 with loss: 0.019705477336\n",
            " Testing Accuray at 447 iterations is 0.942400000000 with loss: 0.019689280713\n",
            " Testing Accuray at 448 iterations is 0.942500000000 with loss: 0.019676308881\n",
            " Testing Accuray at 449 iterations is 0.942700000000 with loss: 0.019666151261\n",
            " Testing Accuray at 450 iterations is 0.942300000000 with loss: 0.019658348146\n",
            " Testing Accuray at 451 iterations is 0.942200000000 with loss: 0.019652406212\n",
            " Testing Accuray at 452 iterations is 0.942100000000 with loss: 0.019647813109\n",
            " Testing Accuray at 453 iterations is 0.942200000000 with loss: 0.019644050761\n",
            " Testing Accuray at 454 iterations is 0.941900000000 with loss: 0.019640607144\n",
            " Testing Accuray at 455 iterations is 0.942100000000 with loss: 0.019636986541\n",
            " Testing Accuray at 456 iterations is 0.941800000000 with loss: 0.019632718335\n",
            " Testing Accuray at 457 iterations is 0.942000000000 with loss: 0.019627364532\n",
            " Testing Accuray at 458 iterations is 0.942000000000 with loss: 0.019620526213\n",
            " Testing Accuray at 459 iterations is 0.942100000000 with loss: 0.019611849090\n",
            " Testing Accuray at 460 iterations is 0.942100000000 with loss: 0.019601028339\n",
            " Testing Accuray at 461 iterations is 0.942000000000 with loss: 0.019587812771\n",
            " Testing Accuray at 462 iterations is 0.942100000000 with loss: 0.019572008375\n",
            " Testing Accuray at 463 iterations is 0.942500000000 with loss: 0.019553481201\n",
            " Testing Accuray at 464 iterations is 0.942200000000 with loss: 0.019532159499\n",
            " Testing Accuray at 465 iterations is 0.941800000000 with loss: 0.019508035052\n",
            " Testing Accuray at 466 iterations is 0.942100000000 with loss: 0.019481163589\n",
            " Testing Accuray at 467 iterations is 0.941900000000 with loss: 0.019451664240\n",
            " Testing Accuray at 468 iterations is 0.941800000000 with loss: 0.019419717985\n",
            " Testing Accuray at 469 iterations is 0.941900000000 with loss: 0.019385565084\n",
            " Testing Accuray at 470 iterations is 0.942300000000 with loss: 0.019349501509\n",
            " Testing Accuray at 471 iterations is 0.942200000000 with loss: 0.019311874394\n",
            " Testing Accuray at 472 iterations is 0.942000000000 with loss: 0.019273076520\n",
            " Testing Accuray at 473 iterations is 0.942600000000 with loss: 0.019233539868\n",
            " Testing Accuray at 474 iterations is 0.942500000000 with loss: 0.019193728216\n",
            " Testing Accuray at 475 iterations is 0.942700000000 with loss: 0.019154128801\n",
            " Testing Accuray at 476 iterations is 0.943000000000 with loss: 0.019115243032\n",
            " Testing Accuray at 477 iterations is 0.943400000000 with loss: 0.019077576287\n",
            " Testing Accuray at 478 iterations is 0.943300000000 with loss: 0.019041626843\n",
            " Testing Accuray at 479 iterations is 0.943100000000 with loss: 0.019007874075\n",
            " Testing Accuray at 480 iterations is 0.942800000000 with loss: 0.018976766125\n",
            " Testing Accuray at 481 iterations is 0.942900000000 with loss: 0.018948707316\n",
            " Testing Accuray at 482 iterations is 0.942900000000 with loss: 0.018924045709\n",
            " Testing Accuray at 483 iterations is 0.942800000000 with loss: 0.018903061242\n",
            " Testing Accuray at 484 iterations is 0.943400000000 with loss: 0.018885954983\n",
            " Testing Accuray at 485 iterations is 0.943300000000 with loss: 0.018872840048\n",
            " Testing Accuray at 486 iterations is 0.943200000000 with loss: 0.018863734725\n",
            " Testing Accuray at 487 iterations is 0.942900000000 with loss: 0.018858558323\n",
            " Testing Accuray at 488 iterations is 0.942800000000 with loss: 0.018857130139\n",
            " Testing Accuray at 489 iterations is 0.943300000000 with loss: 0.018859171854\n",
            " Testing Accuray at 490 iterations is 0.943900000000 with loss: 0.018864313459\n",
            " Testing Accuray at 491 iterations is 0.943600000000 with loss: 0.018872102657\n",
            " Testing Accuray at 492 iterations is 0.943700000000 with loss: 0.018882017476\n",
            " Testing Accuray at 493 iterations is 0.943800000000 with loss: 0.018893481627\n",
            " Testing Accuray at 494 iterations is 0.944200000000 with loss: 0.018905881979\n",
            " Testing Accuray at 495 iterations is 0.944000000000 with loss: 0.018918587380\n",
            " Testing Accuray at 496 iterations is 0.944300000000 with loss: 0.018930967944\n",
            " Testing Accuray at 497 iterations is 0.944200000000 with loss: 0.018942413919\n",
            " Testing Accuray at 498 iterations is 0.943600000000 with loss: 0.018952353242\n",
            " Testing Accuray at 499 iterations is 0.943300000000 with loss: 0.018960266992\n",
            " Testing Accuray at 500 iterations is 0.943000000000 with loss: 0.018965702078\n",
            "SigmoidNAG without QG Testing Accuray at   1 iterations is 0.566900000000 with loss: -34607.321180874569\n",
            "SigmoidNAG without QG Testing Accuray at   2 iterations is 0.566900000000 with loss: -1570610.291611544788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-f8446c689701>:389: RuntimeWarning: divide by zero encountered in log\n",
            "  loss =  np.sum( np.log( np.absolute(pp -1 + Y) )  )\n",
            "<ipython-input-1-f8446c689701>:396: RuntimeWarning: divide by zero encountered in log\n",
            "  loss =  np.sum( np.log( np.absolute(pp -1 + Ytest) )  )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SigmoidNAG without QG Testing Accuray at   3 iterations is 0.566900000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at   4 iterations is 0.566900000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at   5 iterations is 0.566900000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at   6 iterations is 0.566900000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at   7 iterations is 0.566900000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at   8 iterations is 0.566900000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at   9 iterations is 0.566900000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  10 iterations is 0.566900000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  11 iterations is 0.566900000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  12 iterations is 0.566900000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  13 iterations is 0.566900000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  14 iterations is 0.567000000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  15 iterations is 0.576500000000 with loss: -304196.084702289721\n",
            "SigmoidNAG without QG Testing Accuray at  16 iterations is 0.669200000000 with loss: -547315.236570483656\n",
            "SigmoidNAG without QG Testing Accuray at  17 iterations is 0.537400000000 with loss: -257404.420278581529\n",
            "SigmoidNAG without QG Testing Accuray at  18 iterations is 0.430000000000 with loss: -151861.070714220521\n",
            "SigmoidNAG without QG Testing Accuray at  19 iterations is 0.474200000000 with loss: -93103.274155737061\n",
            "SigmoidNAG without QG Testing Accuray at  20 iterations is 0.655100000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  21 iterations is 0.787300000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  22 iterations is 0.745700000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  23 iterations is 0.636200000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  24 iterations is 0.521600000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  25 iterations is 0.416300000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  26 iterations is 0.410500000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  27 iterations is 0.562800000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  28 iterations is 0.597200000000 with loss: -inf\n",
            "SigmoidNAG without QG Testing Accuray at  29 iterations is 0.660800000000 with loss: -49135.673323169191\n",
            "SigmoidNAG without QG Testing Accuray at  30 iterations is 0.679100000000 with loss: -51962.018835707095\n",
            "SigmoidNAG without QG Testing Accuray at  31 iterations is 0.640900000000 with loss: -58069.930576739556\n",
            "SigmoidNAG without QG Testing Accuray at  32 iterations is 0.618600000000 with loss: -55248.117789450087\n",
            "SigmoidNAG without QG Testing Accuray at  33 iterations is 0.641200000000 with loss: -45597.640085391140\n",
            "SigmoidNAG without QG Testing Accuray at  34 iterations is 0.684700000000 with loss: -36864.770977320040\n",
            "SigmoidNAG without QG Testing Accuray at  35 iterations is 0.671100000000 with loss: -34901.712514813858\n",
            "SigmoidNAG without QG Testing Accuray at  36 iterations is 0.598200000000 with loss: -45143.879444772545\n",
            "SigmoidNAG without QG Testing Accuray at  37 iterations is 0.626700000000 with loss: -39653.052275386901\n",
            "SigmoidNAG without QG Testing Accuray at  38 iterations is 0.679900000000 with loss: -32519.854928980971\n",
            "SigmoidNAG without QG Testing Accuray at  39 iterations is 0.687700000000 with loss: -33570.396557705586\n",
            "SigmoidNAG without QG Testing Accuray at  40 iterations is 0.729800000000 with loss: -30761.797513798057\n",
            "SigmoidNAG without QG Testing Accuray at  41 iterations is 0.725200000000 with loss: -30278.913215278844\n",
            "SigmoidNAG without QG Testing Accuray at  42 iterations is 0.698700000000 with loss: -30945.392166970454\n",
            "SigmoidNAG without QG Testing Accuray at  43 iterations is 0.698800000000 with loss: -29654.307722946378\n",
            "SigmoidNAG without QG Testing Accuray at  44 iterations is 0.729000000000 with loss: -26730.545251065938\n",
            "SigmoidNAG without QG Testing Accuray at  45 iterations is 0.756700000000 with loss: -24457.756250801165\n",
            "SigmoidNAG without QG Testing Accuray at  46 iterations is 0.756800000000 with loss: -25330.468576170762\n",
            "SigmoidNAG without QG Testing Accuray at  47 iterations is 0.733800000000 with loss: -29468.827171409615\n",
            "SigmoidNAG without QG Testing Accuray at  48 iterations is 0.745800000000 with loss: -28719.848793577738\n",
            "SigmoidNAG without QG Testing Accuray at  49 iterations is 0.786500000000 with loss: -23309.843278996646\n",
            "SigmoidNAG without QG Testing Accuray at  50 iterations is 0.797500000000 with loss: -21332.005065024066\n",
            "SigmoidNAG without QG Testing Accuray at  51 iterations is 0.784400000000 with loss: -21494.590801900784\n",
            "SigmoidNAG without QG Testing Accuray at  52 iterations is 0.766400000000 with loss: -22246.672205018956\n",
            "SigmoidNAG without QG Testing Accuray at  53 iterations is 0.760700000000 with loss: -22418.160200991973\n",
            "SigmoidNAG without QG Testing Accuray at  54 iterations is 0.774700000000 with loss: -21038.737449484175\n",
            "SigmoidNAG without QG Testing Accuray at  55 iterations is 0.802900000000 with loss: -18722.217902614630\n",
            "SigmoidNAG without QG Testing Accuray at  56 iterations is 0.828100000000 with loss: -16813.813649682168\n",
            "SigmoidNAG without QG Testing Accuray at  57 iterations is 0.831600000000 with loss: -15989.773496795355\n",
            "SigmoidNAG without QG Testing Accuray at  58 iterations is 0.814900000000 with loss: -16591.648374121509\n",
            "SigmoidNAG without QG Testing Accuray at  59 iterations is 0.792300000000 with loss: -18533.792386311554\n",
            "SigmoidNAG without QG Testing Accuray at  60 iterations is 0.782600000000 with loss: -19938.209369398824\n",
            "SigmoidNAG without QG Testing Accuray at  61 iterations is 0.793200000000 with loss: -18940.314714809341\n",
            "SigmoidNAG without QG Testing Accuray at  62 iterations is 0.808000000000 with loss: -17254.130400660950\n",
            "SigmoidNAG without QG Testing Accuray at  63 iterations is 0.816600000000 with loss: -16438.655925654220\n",
            "SigmoidNAG without QG Testing Accuray at  64 iterations is 0.822200000000 with loss: -16156.718656794073\n",
            "SigmoidNAG without QG Testing Accuray at  65 iterations is 0.824700000000 with loss: -16037.091120214798\n",
            "SigmoidNAG without QG Testing Accuray at  66 iterations is 0.828800000000 with loss: -15947.253850681163\n",
            "SigmoidNAG without QG Testing Accuray at  67 iterations is 0.831500000000 with loss: -15801.726523611471\n",
            "SigmoidNAG without QG Testing Accuray at  68 iterations is 0.831900000000 with loss: -15533.989046001687\n",
            "SigmoidNAG without QG Testing Accuray at  69 iterations is 0.831900000000 with loss: -15145.356593089447\n",
            "SigmoidNAG without QG Testing Accuray at  70 iterations is 0.833600000000 with loss: -14710.805645557211\n",
            "SigmoidNAG without QG Testing Accuray at  71 iterations is 0.835700000000 with loss: -14340.691910429274\n",
            "SigmoidNAG without QG Testing Accuray at  72 iterations is 0.831300000000 with loss: -14194.789038240078\n",
            "SigmoidNAG without QG Testing Accuray at  73 iterations is 0.823500000000 with loss: -14335.780526595227\n",
            "SigmoidNAG without QG Testing Accuray at  74 iterations is 0.818700000000 with loss: -14361.918865713553\n",
            "SigmoidNAG without QG Testing Accuray at  75 iterations is 0.821900000000 with loss: -13771.523404080901\n",
            "SigmoidNAG without QG Testing Accuray at  76 iterations is 0.834900000000 with loss: -12856.402174833194\n",
            "SigmoidNAG without QG Testing Accuray at  77 iterations is 0.850700000000 with loss: -12246.150917290155\n",
            "SigmoidNAG without QG Testing Accuray at  78 iterations is 0.862700000000 with loss: -12055.050740298584\n",
            "SigmoidNAG without QG Testing Accuray at  79 iterations is 0.865300000000 with loss: -12067.710575103487\n",
            "SigmoidNAG without QG Testing Accuray at  80 iterations is 0.865000000000 with loss: -12093.907813501688\n",
            "SigmoidNAG without QG Testing Accuray at  81 iterations is 0.861200000000 with loss: -12046.139713676495\n",
            "SigmoidNAG without QG Testing Accuray at  82 iterations is 0.857400000000 with loss: -11918.504299340791\n",
            "SigmoidNAG without QG Testing Accuray at  83 iterations is 0.854600000000 with loss: -11751.555064428627\n",
            "SigmoidNAG without QG Testing Accuray at  84 iterations is 0.856000000000 with loss: -11595.459693427263\n",
            "SigmoidNAG without QG Testing Accuray at  85 iterations is 0.856900000000 with loss: -11483.678867279879\n",
            "SigmoidNAG without QG Testing Accuray at  86 iterations is 0.859800000000 with loss: -11426.282853037917\n",
            "SigmoidNAG without QG Testing Accuray at  87 iterations is 0.863900000000 with loss: -11417.925935495805\n",
            "SigmoidNAG without QG Testing Accuray at  88 iterations is 0.868200000000 with loss: -11433.061682967005\n",
            "SigmoidNAG without QG Testing Accuray at  89 iterations is 0.872700000000 with loss: -11387.112805108431\n",
            "SigmoidNAG without QG Testing Accuray at  90 iterations is 0.874100000000 with loss: -11135.724926825549\n",
            "SigmoidNAG without QG Testing Accuray at  91 iterations is 0.873700000000 with loss: -10623.995406044703\n",
            "SigmoidNAG without QG Testing Accuray at  92 iterations is 0.876600000000 with loss: -10030.011367647585\n",
            "SigmoidNAG without QG Testing Accuray at  93 iterations is 0.876200000000 with loss: -9632.114730642617\n",
            "SigmoidNAG without QG Testing Accuray at  94 iterations is 0.873400000000 with loss: -9571.325700497984\n",
            "SigmoidNAG without QG Testing Accuray at  95 iterations is 0.871700000000 with loss: -9804.826723868378\n",
            "SigmoidNAG without QG Testing Accuray at  96 iterations is 0.867600000000 with loss: -10191.658687767193\n",
            "SigmoidNAG without QG Testing Accuray at  97 iterations is 0.866300000000 with loss: -10571.790984792993\n",
            "SigmoidNAG without QG Testing Accuray at  98 iterations is 0.863100000000 with loss: -10813.558089474847\n",
            "SigmoidNAG without QG Testing Accuray at  99 iterations is 0.863800000000 with loss: -10838.695860597556\n",
            "SigmoidNAG without QG Testing Accuray at 100 iterations is 0.866000000000 with loss: -10631.114611451299\n",
            "SigmoidNAG without QG Testing Accuray at 101 iterations is 0.870700000000 with loss: -10234.448217769379\n",
            "SigmoidNAG without QG Testing Accuray at 102 iterations is 0.876200000000 with loss: -9740.689667550007\n",
            "SigmoidNAG without QG Testing Accuray at 103 iterations is 0.881100000000 with loss: -9269.247564300955\n",
            "SigmoidNAG without QG Testing Accuray at 104 iterations is 0.885900000000 with loss: -8935.515588930126\n",
            "SigmoidNAG without QG Testing Accuray at 105 iterations is 0.886500000000 with loss: -8809.641179700195\n",
            "SigmoidNAG without QG Testing Accuray at 106 iterations is 0.885000000000 with loss: -8873.669073883757\n",
            "SigmoidNAG without QG Testing Accuray at 107 iterations is 0.884900000000 with loss: -9008.220331210587\n",
            "SigmoidNAG without QG Testing Accuray at 108 iterations is 0.886500000000 with loss: -9052.918549827540\n",
            "SigmoidNAG without QG Testing Accuray at 109 iterations is 0.888500000000 with loss: -8925.316020489332\n",
            "SigmoidNAG without QG Testing Accuray at 110 iterations is 0.890500000000 with loss: -8688.334726464431\n",
            "SigmoidNAG without QG Testing Accuray at 111 iterations is 0.890500000000 with loss: -8488.156618338102\n",
            "SigmoidNAG without QG Testing Accuray at 112 iterations is 0.890500000000 with loss: -8434.352101367775\n",
            "SigmoidNAG without QG Testing Accuray at 113 iterations is 0.888000000000 with loss: -8539.113865180823\n",
            "SigmoidNAG without QG Testing Accuray at 114 iterations is 0.885200000000 with loss: -8735.775097350435\n",
            "SigmoidNAG without QG Testing Accuray at 115 iterations is 0.882600000000 with loss: -8928.492423823889\n",
            "SigmoidNAG without QG Testing Accuray at 116 iterations is 0.881500000000 with loss: -9035.781321729190\n",
            "SigmoidNAG without QG Testing Accuray at 117 iterations is 0.882600000000 with loss: -9014.712189837235\n",
            "SigmoidNAG without QG Testing Accuray at 118 iterations is 0.886000000000 with loss: -8864.254598935540\n",
            "SigmoidNAG without QG Testing Accuray at 119 iterations is 0.890100000000 with loss: -8614.615531384641\n",
            "SigmoidNAG without QG Testing Accuray at 120 iterations is 0.893900000000 with loss: -8313.617986761417\n",
            "SigmoidNAG without QG Testing Accuray at 121 iterations is 0.897300000000 with loss: -8016.925931303500\n",
            "SigmoidNAG without QG Testing Accuray at 122 iterations is 0.898600000000 with loss: -7781.324377570407\n",
            "SigmoidNAG without QG Testing Accuray at 123 iterations is 0.900000000000 with loss: -7656.493083402325\n",
            "SigmoidNAG without QG Testing Accuray at 124 iterations is 0.900500000000 with loss: -7671.978256760380\n",
            "SigmoidNAG without QG Testing Accuray at 125 iterations is 0.898600000000 with loss: -7820.778647361126\n",
            "SigmoidNAG without QG Testing Accuray at 126 iterations is 0.897000000000 with loss: -8048.368463335971\n",
            "SigmoidNAG without QG Testing Accuray at 127 iterations is 0.896600000000 with loss: -8261.914638513083\n",
            "SigmoidNAG without QG Testing Accuray at 128 iterations is 0.895900000000 with loss: -8367.446050503282\n",
            "SigmoidNAG without QG Testing Accuray at 129 iterations is 0.895500000000 with loss: -8318.147960284210\n",
            "SigmoidNAG without QG Testing Accuray at 130 iterations is 0.896900000000 with loss: -8138.078376586842\n",
            "SigmoidNAG without QG Testing Accuray at 131 iterations is 0.897900000000 with loss: -7902.118180683303\n",
            "SigmoidNAG without QG Testing Accuray at 132 iterations is 0.900000000000 with loss: -7691.090596346569\n",
            "SigmoidNAG without QG Testing Accuray at 133 iterations is 0.898700000000 with loss: -7556.524647157410\n",
            "SigmoidNAG without QG Testing Accuray at 134 iterations is 0.897900000000 with loss: -7511.184336988771\n",
            "SigmoidNAG without QG Testing Accuray at 135 iterations is 0.898400000000 with loss: -7538.243859081281\n",
            "SigmoidNAG without QG Testing Accuray at 136 iterations is 0.897800000000 with loss: -7605.612745738662\n",
            "SigmoidNAG without QG Testing Accuray at 137 iterations is 0.898300000000 with loss: -7677.673860374788\n",
            "SigmoidNAG without QG Testing Accuray at 138 iterations is 0.898000000000 with loss: -7722.924784642793\n",
            "SigmoidNAG without QG Testing Accuray at 139 iterations is 0.898300000000 with loss: -7718.690376186489\n",
            "SigmoidNAG without QG Testing Accuray at 140 iterations is 0.900300000000 with loss: -7654.332964646953\n",
            "SigmoidNAG without QG Testing Accuray at 141 iterations is 0.903100000000 with loss: -7533.521623097149\n",
            "SigmoidNAG without QG Testing Accuray at 142 iterations is 0.904800000000 with loss: -7374.800287364295\n",
            "SigmoidNAG without QG Testing Accuray at 143 iterations is 0.906300000000 with loss: -7208.829311411003\n",
            "SigmoidNAG without QG Testing Accuray at 144 iterations is 0.908600000000 with loss: -7071.180337424178\n",
            "SigmoidNAG without QG Testing Accuray at 145 iterations is 0.907800000000 with loss: -6991.477899777317\n",
            "SigmoidNAG without QG Testing Accuray at 146 iterations is 0.907100000000 with loss: -6982.060054092291\n",
            "SigmoidNAG without QG Testing Accuray at 147 iterations is 0.907400000000 with loss: -7031.021035450318\n",
            "SigmoidNAG without QG Testing Accuray at 148 iterations is 0.906700000000 with loss: -7104.398578639114\n",
            "SigmoidNAG without QG Testing Accuray at 149 iterations is 0.905900000000 with loss: -7159.063654837904\n",
            "SigmoidNAG without QG Testing Accuray at 150 iterations is 0.905900000000 with loss: -7161.571407412139\n",
            "SigmoidNAG without QG Testing Accuray at 151 iterations is 0.906300000000 with loss: -7102.875884917904\n",
            "SigmoidNAG without QG Testing Accuray at 152 iterations is 0.908100000000 with loss: -7000.278832711572\n",
            "SigmoidNAG without QG Testing Accuray at 153 iterations is 0.910100000000 with loss: -6886.497979206931\n",
            "SigmoidNAG without QG Testing Accuray at 154 iterations is 0.910700000000 with loss: -6793.866408305373\n",
            "SigmoidNAG without QG Testing Accuray at 155 iterations is 0.911200000000 with loss: -6742.615837769702\n",
            "SigmoidNAG without QG Testing Accuray at 156 iterations is 0.911000000000 with loss: -6737.051884074192\n",
            "SigmoidNAG without QG Testing Accuray at 157 iterations is 0.910000000000 with loss: -6768.093104535486\n",
            "SigmoidNAG without QG Testing Accuray at 158 iterations is 0.910600000000 with loss: -6818.703482552492\n",
            "SigmoidNAG without QG Testing Accuray at 159 iterations is 0.908900000000 with loss: -6869.530537584002\n",
            "SigmoidNAG without QG Testing Accuray at 160 iterations is 0.908200000000 with loss: -6903.465067954459\n",
            "SigmoidNAG without QG Testing Accuray at 161 iterations is 0.906800000000 with loss: -6908.750518993413\n",
            "SigmoidNAG without QG Testing Accuray at 162 iterations is 0.906400000000 with loss: -6880.651386702645\n",
            "SigmoidNAG without QG Testing Accuray at 163 iterations is 0.906400000000 with loss: -6821.800127441267\n",
            "SigmoidNAG without QG Testing Accuray at 164 iterations is 0.907300000000 with loss: -6741.348577494771\n",
            "SigmoidNAG without QG Testing Accuray at 165 iterations is 0.908400000000 with loss: -6653.037474485438\n",
            "SigmoidNAG without QG Testing Accuray at 166 iterations is 0.909300000000 with loss: -6572.333702282081\n",
            "SigmoidNAG without QG Testing Accuray at 167 iterations is 0.910400000000 with loss: -6512.929071246627\n",
            "SigmoidNAG without QG Testing Accuray at 168 iterations is 0.912600000000 with loss: -6483.169270034093\n",
            "SigmoidNAG without QG Testing Accuray at 169 iterations is 0.913900000000 with loss: -6483.332911601021\n",
            "SigmoidNAG without QG Testing Accuray at 170 iterations is 0.915600000000 with loss: -6504.895813548418\n",
            "SigmoidNAG without QG Testing Accuray at 171 iterations is 0.916300000000 with loss: -6532.635870564714\n",
            "SigmoidNAG without QG Testing Accuray at 172 iterations is 0.917400000000 with loss: -6549.420417453252\n",
            "SigmoidNAG without QG Testing Accuray at 173 iterations is 0.917500000000 with loss: -6542.089079098573\n",
            "SigmoidNAG without QG Testing Accuray at 174 iterations is 0.918500000000 with loss: -6505.958601569692\n",
            "SigmoidNAG without QG Testing Accuray at 175 iterations is 0.919000000000 with loss: -6445.998162252980\n",
            "SigmoidNAG without QG Testing Accuray at 176 iterations is 0.918100000000 with loss: -6374.450087471840\n",
            "SigmoidNAG without QG Testing Accuray at 177 iterations is 0.917400000000 with loss: -6306.355871385614\n",
            "SigmoidNAG without QG Testing Accuray at 178 iterations is 0.916300000000 with loss: -6255.006759642813\n",
            "SigmoidNAG without QG Testing Accuray at 179 iterations is 0.916200000000 with loss: -6228.768100274597\n",
            "SigmoidNAG without QG Testing Accuray at 180 iterations is 0.915900000000 with loss: -6229.771728880865\n",
            "SigmoidNAG without QG Testing Accuray at 181 iterations is 0.916100000000 with loss: -6254.256459653955\n",
            "SigmoidNAG without QG Testing Accuray at 182 iterations is 0.914800000000 with loss: -6294.024403275018\n",
            "SigmoidNAG without QG Testing Accuray at 183 iterations is 0.915200000000 with loss: -6338.455069411198\n",
            "SigmoidNAG without QG Testing Accuray at 184 iterations is 0.914300000000 with loss: -6376.623713847594\n",
            "SigmoidNAG without QG Testing Accuray at 185 iterations is 0.913700000000 with loss: -6399.197509981440\n",
            "SigmoidNAG without QG Testing Accuray at 186 iterations is 0.913700000000 with loss: -6399.888516724339\n",
            "SigmoidNAG without QG Testing Accuray at 187 iterations is 0.914100000000 with loss: -6376.325309420323\n",
            "SigmoidNAG without QG Testing Accuray at 188 iterations is 0.914400000000 with loss: -6330.276593362953\n",
            "SigmoidNAG without QG Testing Accuray at 189 iterations is 0.916100000000 with loss: -6267.226461996575\n",
            "SigmoidNAG without QG Testing Accuray at 190 iterations is 0.917700000000 with loss: -6195.363203842600\n",
            "SigmoidNAG without QG Testing Accuray at 191 iterations is 0.917900000000 with loss: -6124.106302864720\n",
            "SigmoidNAG without QG Testing Accuray at 192 iterations is 0.918500000000 with loss: -6062.371769445667\n",
            "SigmoidNAG without QG Testing Accuray at 193 iterations is 0.919500000000 with loss: -6016.872347322601\n",
            "SigmoidNAG without QG Testing Accuray at 194 iterations is 0.919900000000 with loss: -5990.840501484596\n",
            "SigmoidNAG without QG Testing Accuray at 195 iterations is 0.919700000000 with loss: -5983.567813429187\n",
            "SigmoidNAG without QG Testing Accuray at 196 iterations is 0.920300000000 with loss: -5990.976521940901\n",
            "SigmoidNAG without QG Testing Accuray at 197 iterations is 0.920900000000 with loss: -6007.065397511315\n",
            "SigmoidNAG without QG Testing Accuray at 198 iterations is 0.921300000000 with loss: -6025.671444733704\n",
            "SigmoidNAG without QG Testing Accuray at 199 iterations is 0.921300000000 with loss: -6041.845050312360\n",
            "SigmoidNAG without QG Testing Accuray at 200 iterations is 0.920100000000 with loss: -6052.402757184736\n",
            "SigmoidNAG without QG Testing Accuray at 201 iterations is 0.921300000000 with loss: -6055.728710438285\n",
            "SigmoidNAG without QG Testing Accuray at 202 iterations is 0.921400000000 with loss: -6051.264237437337\n",
            "SigmoidNAG without QG Testing Accuray at 203 iterations is 0.921000000000 with loss: -6039.113737229014\n",
            "SigmoidNAG without QG Testing Accuray at 204 iterations is 0.921400000000 with loss: -6019.905719765979\n",
            "SigmoidNAG without QG Testing Accuray at 205 iterations is 0.921100000000 with loss: -5994.776981169168\n",
            "SigmoidNAG without QG Testing Accuray at 206 iterations is 0.921000000000 with loss: -5965.289575750839\n",
            "SigmoidNAG without QG Testing Accuray at 207 iterations is 0.921000000000 with loss: -5933.209301363306\n",
            "SigmoidNAG without QG Testing Accuray at 208 iterations is 0.921800000000 with loss: -5900.215761195329\n",
            "SigmoidNAG without QG Testing Accuray at 209 iterations is 0.921700000000 with loss: -5867.666912224206\n",
            "SigmoidNAG without QG Testing Accuray at 210 iterations is 0.921700000000 with loss: -5836.501233242304\n",
            "SigmoidNAG without QG Testing Accuray at 211 iterations is 0.922500000000 with loss: -5807.285901205686\n",
            "SigmoidNAG without QG Testing Accuray at 212 iterations is 0.924000000000 with loss: -5780.361744786484\n",
            "SigmoidNAG without QG Testing Accuray at 213 iterations is 0.924600000000 with loss: -5756.013992339613\n",
            "SigmoidNAG without QG Testing Accuray at 214 iterations is 0.925200000000 with loss: -5734.603481011522\n",
            "SigmoidNAG without QG Testing Accuray at 215 iterations is 0.925600000000 with loss: -5716.611572524102\n",
            "SigmoidNAG without QG Testing Accuray at 216 iterations is 0.925200000000 with loss: -5702.574841645892\n",
            "SigmoidNAG without QG Testing Accuray at 217 iterations is 0.924800000000 with loss: -5692.911162328751\n",
            "SigmoidNAG without QG Testing Accuray at 218 iterations is 0.924700000000 with loss: -5687.670447194621\n",
            "SigmoidNAG without QG Testing Accuray at 219 iterations is 0.924600000000 with loss: -5686.281009259360\n",
            "SigmoidNAG without QG Testing Accuray at 220 iterations is 0.925400000000 with loss: -5687.397347494962\n",
            "SigmoidNAG without QG Testing Accuray at 221 iterations is 0.925900000000 with loss: -5688.963983225808\n",
            "SigmoidNAG without QG Testing Accuray at 222 iterations is 0.925400000000 with loss: -5688.565245917748\n",
            "SigmoidNAG without QG Testing Accuray at 223 iterations is 0.925700000000 with loss: -5684.022447657427\n",
            "SigmoidNAG without QG Testing Accuray at 224 iterations is 0.925400000000 with loss: -5674.062451099429\n",
            "SigmoidNAG without QG Testing Accuray at 225 iterations is 0.925200000000 with loss: -5658.793704662265\n",
            "SigmoidNAG without QG Testing Accuray at 226 iterations is 0.925400000000 with loss: -5639.761946177712\n",
            "SigmoidNAG without QG Testing Accuray at 227 iterations is 0.925100000000 with loss: -5619.524728357718\n",
            "SigmoidNAG without QG Testing Accuray at 228 iterations is 0.925100000000 with loss: -5600.897291003522\n",
            "SigmoidNAG without QG Testing Accuray at 229 iterations is 0.926000000000 with loss: -5586.160007096694\n",
            "SigmoidNAG without QG Testing Accuray at 230 iterations is 0.926700000000 with loss: -5576.507688626883\n",
            "SigmoidNAG without QG Testing Accuray at 231 iterations is 0.926700000000 with loss: -5571.887026525700\n",
            "SigmoidNAG without QG Testing Accuray at 232 iterations is 0.926800000000 with loss: -5571.198789395241\n",
            "SigmoidNAG without QG Testing Accuray at 233 iterations is 0.927900000000 with loss: -5572.721836936154\n",
            "SigmoidNAG without QG Testing Accuray at 234 iterations is 0.928600000000 with loss: -5574.582862582603\n",
            "SigmoidNAG without QG Testing Accuray at 235 iterations is 0.928000000000 with loss: -5575.135270066720\n",
            "SigmoidNAG without QG Testing Accuray at 236 iterations is 0.927400000000 with loss: -5573.183424795631\n",
            "SigmoidNAG without QG Testing Accuray at 237 iterations is 0.926700000000 with loss: -5568.056546777271\n",
            "SigmoidNAG without QG Testing Accuray at 238 iterations is 0.926300000000 with loss: -5559.576418223593\n",
            "SigmoidNAG without QG Testing Accuray at 239 iterations is 0.926800000000 with loss: -5547.971200888643\n",
            "SigmoidNAG without QG Testing Accuray at 240 iterations is 0.927700000000 with loss: -5533.773774845575\n",
            "SigmoidNAG without QG Testing Accuray at 241 iterations is 0.928000000000 with loss: -5517.721918491480\n",
            "SigmoidNAG without QG Testing Accuray at 242 iterations is 0.928100000000 with loss: -5500.661439213039\n",
            "SigmoidNAG without QG Testing Accuray at 243 iterations is 0.928000000000 with loss: -5483.446120856504\n",
            "SigmoidNAG without QG Testing Accuray at 244 iterations is 0.927800000000 with loss: -5466.829434317538\n",
            "SigmoidNAG without QG Testing Accuray at 245 iterations is 0.928800000000 with loss: -5451.349343976010\n",
            "SigmoidNAG without QG Testing Accuray at 246 iterations is 0.929900000000 with loss: -5437.217558791918\n",
            "SigmoidNAG without QG Testing Accuray at 247 iterations is 0.931100000000 with loss: -5424.236731439029\n",
            "SigmoidNAG without QG Testing Accuray at 248 iterations is 0.931400000000 with loss: -5411.780068453641\n",
            "SigmoidNAG without QG Testing Accuray at 249 iterations is 0.931600000000 with loss: -5398.870261994259\n",
            "SigmoidNAG without QG Testing Accuray at 250 iterations is 0.931800000000 with loss: -5384.379237750513\n",
            "SigmoidNAG without QG Testing Accuray at 251 iterations is 0.931600000000 with loss: -5367.332899902139\n",
            "SigmoidNAG without QG Testing Accuray at 252 iterations is 0.931800000000 with loss: -5347.254784478922\n",
            "SigmoidNAG without QG Testing Accuray at 253 iterations is 0.932100000000 with loss: -5324.442526717900\n",
            "SigmoidNAG without QG Testing Accuray at 254 iterations is 0.932300000000 with loss: -5300.067765823945\n",
            "SigmoidNAG without QG Testing Accuray at 255 iterations is 0.932400000000 with loss: -5276.035759086688\n",
            "SigmoidNAG without QG Testing Accuray at 256 iterations is 0.932500000000 with loss: -5254.621010361994\n",
            "SigmoidNAG without QG Testing Accuray at 257 iterations is 0.931900000000 with loss: -5237.973952535616\n",
            "SigmoidNAG without QG Testing Accuray at 258 iterations is 0.931000000000 with loss: -5227.635619729492\n",
            "SigmoidNAG without QG Testing Accuray at 259 iterations is 0.930500000000 with loss: -5224.187822383979\n",
            "SigmoidNAG without QG Testing Accuray at 260 iterations is 0.931100000000 with loss: -5227.117183417489\n",
            "SigmoidNAG without QG Testing Accuray at 261 iterations is 0.931000000000 with loss: -5234.908397856534\n",
            "SigmoidNAG without QG Testing Accuray at 262 iterations is 0.931000000000 with loss: -5245.328339528713\n",
            "SigmoidNAG without QG Testing Accuray at 263 iterations is 0.930600000000 with loss: -5255.828852009174\n",
            "SigmoidNAG without QG Testing Accuray at 264 iterations is 0.930500000000 with loss: -5263.983019488653\n",
            "SigmoidNAG without QG Testing Accuray at 265 iterations is 0.930200000000 with loss: -5267.873190012138\n",
            "SigmoidNAG without QG Testing Accuray at 266 iterations is 0.930500000000 with loss: -5266.365967375374\n",
            "SigmoidNAG without QG Testing Accuray at 267 iterations is 0.930700000000 with loss: -5259.236090919085\n",
            "SigmoidNAG without QG Testing Accuray at 268 iterations is 0.930900000000 with loss: -5247.132976245008\n",
            "SigmoidNAG without QG Testing Accuray at 269 iterations is 0.931300000000 with loss: -5231.413385924463\n",
            "SigmoidNAG without QG Testing Accuray at 270 iterations is 0.932000000000 with loss: -5213.884542844237\n",
            "SigmoidNAG without QG Testing Accuray at 271 iterations is 0.932500000000 with loss: -5196.509723813984\n",
            "SigmoidNAG without QG Testing Accuray at 272 iterations is 0.933100000000 with loss: -5181.123620047516\n",
            "SigmoidNAG without QG Testing Accuray at 273 iterations is 0.932400000000 with loss: -5169.192096218429\n",
            "SigmoidNAG without QG Testing Accuray at 274 iterations is 0.933200000000 with loss: -5161.636244460532\n",
            "SigmoidNAG without QG Testing Accuray at 275 iterations is 0.932600000000 with loss: -5158.728784730062\n",
            "SigmoidNAG without QG Testing Accuray at 276 iterations is 0.932400000000 with loss: -5160.064134904310\n",
            "SigmoidNAG without QG Testing Accuray at 277 iterations is 0.932300000000 with loss: -5164.601611534810\n",
            "SigmoidNAG without QG Testing Accuray at 278 iterations is 0.932100000000 with loss: -5170.781475150177\n",
            "SigmoidNAG without QG Testing Accuray at 279 iterations is 0.932400000000 with loss: -5176.712162299380\n",
            "SigmoidNAG without QG Testing Accuray at 280 iterations is 0.932300000000 with loss: -5180.420297306907\n",
            "SigmoidNAG without QG Testing Accuray at 281 iterations is 0.932400000000 with loss: -5180.141600805879\n",
            "SigmoidNAG without QG Testing Accuray at 282 iterations is 0.932800000000 with loss: -5174.613410387396\n",
            "SigmoidNAG without QG Testing Accuray at 283 iterations is 0.932800000000 with loss: -5163.315462276228\n",
            "SigmoidNAG without QG Testing Accuray at 284 iterations is 0.932700000000 with loss: -5146.603744396730\n",
            "SigmoidNAG without QG Testing Accuray at 285 iterations is 0.932400000000 with loss: -5125.698018676997\n",
            "SigmoidNAG without QG Testing Accuray at 286 iterations is 0.932500000000 with loss: -5102.514830068594\n",
            "SigmoidNAG without QG Testing Accuray at 287 iterations is 0.932800000000 with loss: -5079.374391486796\n",
            "SigmoidNAG without QG Testing Accuray at 288 iterations is 0.933000000000 with loss: -5058.638706397061\n",
            "SigmoidNAG without QG Testing Accuray at 289 iterations is 0.933800000000 with loss: -5042.350203171317\n",
            "SigmoidNAG without QG Testing Accuray at 290 iterations is 0.933800000000 with loss: -5031.933499698845\n",
            "SigmoidNAG without QG Testing Accuray at 291 iterations is 0.933900000000 with loss: -5028.003460713937\n",
            "SigmoidNAG without QG Testing Accuray at 292 iterations is 0.933700000000 with loss: -5030.299112569221\n",
            "SigmoidNAG without QG Testing Accuray at 293 iterations is 0.933700000000 with loss: -5037.741870616514\n",
            "SigmoidNAG without QG Testing Accuray at 294 iterations is 0.933500000000 with loss: -5048.600921712078\n",
            "SigmoidNAG without QG Testing Accuray at 295 iterations is 0.933800000000 with loss: -5060.737802638823\n",
            "SigmoidNAG without QG Testing Accuray at 296 iterations is 0.933300000000 with loss: -5071.894906957478\n",
            "SigmoidNAG without QG Testing Accuray at 297 iterations is 0.933100000000 with loss: -5079.988368608281\n",
            "SigmoidNAG without QG Testing Accuray at 298 iterations is 0.933300000000 with loss: -5083.365510746093\n",
            "SigmoidNAG without QG Testing Accuray at 299 iterations is 0.933200000000 with loss: -5080.992353686416\n",
            "SigmoidNAG without QG Testing Accuray at 300 iterations is 0.933300000000 with loss: -5072.547997410025\n",
            "SigmoidNAG without QG Testing Accuray at 301 iterations is 0.934200000000 with loss: -5058.418395592174\n",
            "SigmoidNAG without QG Testing Accuray at 302 iterations is 0.934500000000 with loss: -5039.598624257179\n",
            "SigmoidNAG without QG Testing Accuray at 303 iterations is 0.934800000000 with loss: -5017.526242934881\n",
            "SigmoidNAG without QG Testing Accuray at 304 iterations is 0.935200000000 with loss: -4993.875790301465\n",
            "SigmoidNAG without QG Testing Accuray at 305 iterations is 0.935900000000 with loss: -4970.345226299297\n",
            "SigmoidNAG without QG Testing Accuray at 306 iterations is 0.936000000000 with loss: -4948.460743393391\n",
            "SigmoidNAG without QG Testing Accuray at 307 iterations is 0.937200000000 with loss: -4929.419478785012\n",
            "SigmoidNAG without QG Testing Accuray at 308 iterations is 0.937200000000 with loss: -4913.982792371431\n",
            "SigmoidNAG without QG Testing Accuray at 309 iterations is 0.936800000000 with loss: -4902.427207596451\n",
            "SigmoidNAG without QG Testing Accuray at 310 iterations is 0.937200000000 with loss: -4894.555680042066\n",
            "SigmoidNAG without QG Testing Accuray at 311 iterations is 0.937000000000 with loss: -4889.767496091106\n",
            "SigmoidNAG without QG Testing Accuray at 312 iterations is 0.936800000000 with loss: -4887.179486563377\n",
            "SigmoidNAG without QG Testing Accuray at 313 iterations is 0.937000000000 with loss: -4885.784080585021\n",
            "SigmoidNAG without QG Testing Accuray at 314 iterations is 0.937300000000 with loss: -4884.622102014725\n",
            "SigmoidNAG without QG Testing Accuray at 315 iterations is 0.937400000000 with loss: -4882.942665797768\n",
            "SigmoidNAG without QG Testing Accuray at 316 iterations is 0.937300000000 with loss: -4880.321985931233\n",
            "SigmoidNAG without QG Testing Accuray at 317 iterations is 0.937200000000 with loss: -4876.719113224858\n",
            "SigmoidNAG without QG Testing Accuray at 318 iterations is 0.937200000000 with loss: -4872.458898356617\n",
            "SigmoidNAG without QG Testing Accuray at 319 iterations is 0.937300000000 with loss: -4868.147659086351\n",
            "SigmoidNAG without QG Testing Accuray at 320 iterations is 0.937200000000 with loss: -4864.540565174378\n",
            "SigmoidNAG without QG Testing Accuray at 321 iterations is 0.937200000000 with loss: -4862.387830851212\n",
            "SigmoidNAG without QG Testing Accuray at 322 iterations is 0.936700000000 with loss: -4862.287794371800\n",
            "SigmoidNAG without QG Testing Accuray at 323 iterations is 0.937000000000 with loss: -4864.569941852576\n",
            "SigmoidNAG without QG Testing Accuray at 324 iterations is 0.937200000000 with loss: -4869.222650757067\n",
            "SigmoidNAG without QG Testing Accuray at 325 iterations is 0.936400000000 with loss: -4875.871778023256\n",
            "SigmoidNAG without QG Testing Accuray at 326 iterations is 0.936000000000 with loss: -4883.809147152661\n",
            "SigmoidNAG without QG Testing Accuray at 327 iterations is 0.935100000000 with loss: -4892.064998493145\n",
            "SigmoidNAG without QG Testing Accuray at 328 iterations is 0.935600000000 with loss: -4899.515016121291\n",
            "SigmoidNAG without QG Testing Accuray at 329 iterations is 0.935600000000 with loss: -4905.009832090594\n",
            "SigmoidNAG without QG Testing Accuray at 330 iterations is 0.935300000000 with loss: -4907.512516079486\n",
            "SigmoidNAG without QG Testing Accuray at 331 iterations is 0.935000000000 with loss: -4906.227717903890\n",
            "SigmoidNAG without QG Testing Accuray at 332 iterations is 0.934900000000 with loss: -4900.705743027335\n",
            "SigmoidNAG without QG Testing Accuray at 333 iterations is 0.934900000000 with loss: -4890.906777559670\n",
            "SigmoidNAG without QG Testing Accuray at 334 iterations is 0.934900000000 with loss: -4877.215196991620\n",
            "SigmoidNAG without QG Testing Accuray at 335 iterations is 0.935500000000 with loss: -4860.400917315862\n",
            "SigmoidNAG without QG Testing Accuray at 336 iterations is 0.935000000000 with loss: -4841.532737871583\n",
            "SigmoidNAG without QG Testing Accuray at 337 iterations is 0.935200000000 with loss: -4821.855800298965\n",
            "SigmoidNAG without QG Testing Accuray at 338 iterations is 0.935700000000 with loss: -4802.650090627542\n",
            "SigmoidNAG without QG Testing Accuray at 339 iterations is 0.935800000000 with loss: -4785.088534099765\n",
            "SigmoidNAG without QG Testing Accuray at 340 iterations is 0.936700000000 with loss: -4770.111864192024\n",
            "SigmoidNAG without QG Testing Accuray at 341 iterations is 0.937600000000 with loss: -4758.333968532048\n",
            "SigmoidNAG without QG Testing Accuray at 342 iterations is 0.938400000000 with loss: -4749.986914172497\n",
            "SigmoidNAG without QG Testing Accuray at 343 iterations is 0.938800000000 with loss: -4744.910241707606\n",
            "SigmoidNAG without QG Testing Accuray at 344 iterations is 0.939400000000 with loss: -4742.584746765866\n",
            "SigmoidNAG without QG Testing Accuray at 345 iterations is 0.939900000000 with loss: -4742.206814882107\n",
            "SigmoidNAG without QG Testing Accuray at 346 iterations is 0.940100000000 with loss: -4742.795269233403\n",
            "SigmoidNAG without QG Testing Accuray at 347 iterations is 0.940300000000 with loss: -4743.318708185332\n",
            "SigmoidNAG without QG Testing Accuray at 348 iterations is 0.940100000000 with loss: -4742.827988343302\n",
            "SigmoidNAG without QG Testing Accuray at 349 iterations is 0.940100000000 with loss: -4740.576728553511\n",
            "SigmoidNAG without QG Testing Accuray at 350 iterations is 0.940400000000 with loss: -4736.113426480199\n",
            "SigmoidNAG without QG Testing Accuray at 351 iterations is 0.940300000000 with loss: -4729.332463765806\n",
            "SigmoidNAG without QG Testing Accuray at 352 iterations is 0.940500000000 with loss: -4720.477542604970\n",
            "SigmoidNAG without QG Testing Accuray at 353 iterations is 0.940800000000 with loss: -4710.098710301067\n",
            "SigmoidNAG without QG Testing Accuray at 354 iterations is 0.941000000000 with loss: -4698.971322899853\n",
            "SigmoidNAG without QG Testing Accuray at 355 iterations is 0.940500000000 with loss: -4687.990463416734\n",
            "SigmoidNAG without QG Testing Accuray at 356 iterations is 0.940100000000 with loss: -4678.056528725876\n",
            "SigmoidNAG without QG Testing Accuray at 357 iterations is 0.939600000000 with loss: -4669.966954370450\n",
            "SigmoidNAG without QG Testing Accuray at 358 iterations is 0.939200000000 with loss: -4664.326138407348\n",
            "SigmoidNAG without QG Testing Accuray at 359 iterations is 0.939000000000 with loss: -4661.481706019680\n",
            "SigmoidNAG without QG Testing Accuray at 360 iterations is 0.938900000000 with loss: -4661.491329832770\n",
            "SigmoidNAG without QG Testing Accuray at 361 iterations is 0.938800000000 with loss: -4664.120980271818\n",
            "SigmoidNAG without QG Testing Accuray at 362 iterations is 0.938100000000 with loss: -4668.872881425182\n",
            "SigmoidNAG without QG Testing Accuray at 363 iterations is 0.937600000000 with loss: -4675.039400934265\n",
            "SigmoidNAG without QG Testing Accuray at 364 iterations is 0.937500000000 with loss: -4681.777339172764\n",
            "SigmoidNAG without QG Testing Accuray at 365 iterations is 0.937200000000 with loss: -4688.195445244633\n",
            "SigmoidNAG without QG Testing Accuray at 366 iterations is 0.937100000000 with loss: -4693.446582401690\n",
            "SigmoidNAG without QG Testing Accuray at 367 iterations is 0.937100000000 with loss: -4696.815014809777\n",
            "SigmoidNAG without QG Testing Accuray at 368 iterations is 0.937100000000 with loss: -4697.789226667387\n",
            "SigmoidNAG without QG Testing Accuray at 369 iterations is 0.937000000000 with loss: -4696.111840585313\n",
            "SigmoidNAG without QG Testing Accuray at 370 iterations is 0.937300000000 with loss: -4691.800644035761\n",
            "SigmoidNAG without QG Testing Accuray at 371 iterations is 0.937500000000 with loss: -4685.138253442536\n",
            "SigmoidNAG without QG Testing Accuray at 372 iterations is 0.937900000000 with loss: -4676.631988381821\n",
            "SigmoidNAG without QG Testing Accuray at 373 iterations is 0.937700000000 with loss: -4666.949366288122\n",
            "SigmoidNAG without QG Testing Accuray at 374 iterations is 0.937800000000 with loss: -4656.837603439172\n",
            "SigmoidNAG without QG Testing Accuray at 375 iterations is 0.938300000000 with loss: -4647.037168592522\n",
            "SigmoidNAG without QG Testing Accuray at 376 iterations is 0.938600000000 with loss: -4638.199638502502\n",
            "SigmoidNAG without QG Testing Accuray at 377 iterations is 0.938600000000 with loss: -4630.819034893654\n",
            "SigmoidNAG without QG Testing Accuray at 378 iterations is 0.938900000000 with loss: -4625.183812002936\n",
            "SigmoidNAG without QG Testing Accuray at 379 iterations is 0.939700000000 with loss: -4621.354076548783\n",
            "SigmoidNAG without QG Testing Accuray at 380 iterations is 0.940000000000 with loss: -4619.165759940058\n",
            "SigmoidNAG without QG Testing Accuray at 381 iterations is 0.940100000000 with loss: -4618.260532577758\n",
            "SigmoidNAG without QG Testing Accuray at 382 iterations is 0.940500000000 with loss: -4618.137424951351\n",
            "SigmoidNAG without QG Testing Accuray at 383 iterations is 0.940500000000 with loss: -4618.219611481027\n",
            "SigmoidNAG without QG Testing Accuray at 384 iterations is 0.940900000000 with loss: -4617.927937869023\n",
            "SigmoidNAG without QG Testing Accuray at 385 iterations is 0.940600000000 with loss: -4616.751874182029\n",
            "SigmoidNAG without QG Testing Accuray at 386 iterations is 0.940800000000 with loss: -4614.308966443336\n",
            "SigmoidNAG without QG Testing Accuray at 387 iterations is 0.940600000000 with loss: -4610.385609781200\n",
            "SigmoidNAG without QG Testing Accuray at 388 iterations is 0.940700000000 with loss: -4604.954859366257\n",
            "SigmoidNAG without QG Testing Accuray at 389 iterations is 0.940600000000 with loss: -4598.170493286243\n",
            "SigmoidNAG without QG Testing Accuray at 390 iterations is 0.940200000000 with loss: -4590.339961129293\n",
            "SigmoidNAG without QG Testing Accuray at 391 iterations is 0.940200000000 with loss: -4581.881513308159\n",
            "SigmoidNAG without QG Testing Accuray at 392 iterations is 0.940000000000 with loss: -4573.272291074410\n",
            "SigmoidNAG without QG Testing Accuray at 393 iterations is 0.940500000000 with loss: -4564.994351634651\n",
            "SigmoidNAG without QG Testing Accuray at 394 iterations is 0.940300000000 with loss: -4557.484740028303\n",
            "SigmoidNAG without QG Testing Accuray at 395 iterations is 0.940100000000 with loss: -4551.094197975834\n",
            "SigmoidNAG without QG Testing Accuray at 396 iterations is 0.940200000000 with loss: -4546.057382583103\n",
            "SigmoidNAG without QG Testing Accuray at 397 iterations is 0.939900000000 with loss: -4542.475893451090\n",
            "SigmoidNAG without QG Testing Accuray at 398 iterations is 0.939700000000 with loss: -4540.314166576551\n",
            "SigmoidNAG without QG Testing Accuray at 399 iterations is 0.939200000000 with loss: -4539.407412488788\n",
            "SigmoidNAG without QG Testing Accuray at 400 iterations is 0.938800000000 with loss: -4539.480178847031\n",
            "SigmoidNAG without QG Testing Accuray at 401 iterations is 0.938900000000 with loss: -4540.173652802695\n",
            "SigmoidNAG without QG Testing Accuray at 402 iterations is 0.939200000000 with loss: -4541.079428642996\n",
            "SigmoidNAG without QG Testing Accuray at 403 iterations is 0.938900000000 with loss: -4541.777067961663\n",
            "SigmoidNAG without QG Testing Accuray at 404 iterations is 0.938300000000 with loss: -4541.872414103197\n",
            "SigmoidNAG without QG Testing Accuray at 405 iterations is 0.938300000000 with loss: -4541.033368818751\n",
            "SigmoidNAG without QG Testing Accuray at 406 iterations is 0.938300000000 with loss: -4539.019816812425\n",
            "SigmoidNAG without QG Testing Accuray at 407 iterations is 0.938400000000 with loss: -4535.704704590891\n",
            "SigmoidNAG without QG Testing Accuray at 408 iterations is 0.938500000000 with loss: -4531.083993371916\n",
            "SigmoidNAG without QG Testing Accuray at 409 iterations is 0.938700000000 with loss: -4525.274296352298\n",
            "SigmoidNAG without QG Testing Accuray at 410 iterations is 0.938800000000 with loss: -4518.498351833218\n",
            "SigmoidNAG without QG Testing Accuray at 411 iterations is 0.938900000000 with loss: -4511.059890718407\n",
            "SigmoidNAG without QG Testing Accuray at 412 iterations is 0.939200000000 with loss: -4503.310717826153\n",
            "SigmoidNAG without QG Testing Accuray at 413 iterations is 0.939100000000 with loss: -4495.613743396249\n",
            "SigmoidNAG without QG Testing Accuray at 414 iterations is 0.939700000000 with loss: -4488.306142510649\n",
            "SigmoidNAG without QG Testing Accuray at 415 iterations is 0.939500000000 with loss: -4481.666746581426\n",
            "SigmoidNAG without QG Testing Accuray at 416 iterations is 0.940100000000 with loss: -4475.891206878749\n",
            "SigmoidNAG without QG Testing Accuray at 417 iterations is 0.940500000000 with loss: -4471.077506774170\n",
            "SigmoidNAG without QG Testing Accuray at 418 iterations is 0.941300000000 with loss: -4467.223150719439\n",
            "SigmoidNAG without QG Testing Accuray at 419 iterations is 0.941400000000 with loss: -4464.233963281396\n",
            "SigmoidNAG without QG Testing Accuray at 420 iterations is 0.941900000000 with loss: -4461.943040950968\n",
            "SigmoidNAG without QG Testing Accuray at 421 iterations is 0.942200000000 with loss: -4460.137161538935\n",
            "SigmoidNAG without QG Testing Accuray at 422 iterations is 0.942700000000 with loss: -4458.587048057831\n",
            "SigmoidNAG without QG Testing Accuray at 423 iterations is 0.942700000000 with loss: -4457.077451631216\n",
            "SigmoidNAG without QG Testing Accuray at 424 iterations is 0.942300000000 with loss: -4455.433158701071\n",
            "SigmoidNAG without QG Testing Accuray at 425 iterations is 0.942600000000 with loss: -4453.537747712256\n",
            "SigmoidNAG without QG Testing Accuray at 426 iterations is 0.942500000000 with loss: -4451.343113197256\n",
            "SigmoidNAG without QG Testing Accuray at 427 iterations is 0.942600000000 with loss: -4448.869222902184\n",
            "SigmoidNAG without QG Testing Accuray at 428 iterations is 0.942200000000 with loss: -4446.195003974043\n",
            "SigmoidNAG without QG Testing Accuray at 429 iterations is 0.942100000000 with loss: -4443.442412232539\n",
            "SigmoidNAG without QG Testing Accuray at 430 iterations is 0.941400000000 with loss: -4440.756430948381\n",
            "SigmoidNAG without QG Testing Accuray at 431 iterations is 0.941500000000 with loss: -4438.283917319156\n",
            "SigmoidNAG without QG Testing Accuray at 432 iterations is 0.941600000000 with loss: -4436.153909169155\n",
            "SigmoidNAG without QG Testing Accuray at 433 iterations is 0.940800000000 with loss: -4434.461383280721\n",
            "SigmoidNAG without QG Testing Accuray at 434 iterations is 0.941200000000 with loss: -4433.255694276900\n",
            "SigmoidNAG without QG Testing Accuray at 435 iterations is 0.940900000000 with loss: -4432.534185293491\n",
            "SigmoidNAG without QG Testing Accuray at 436 iterations is 0.940400000000 with loss: -4432.240871865851\n",
            "SigmoidNAG without QG Testing Accuray at 437 iterations is 0.940400000000 with loss: -4432.269698902201\n",
            "SigmoidNAG without QG Testing Accuray at 438 iterations is 0.940400000000 with loss: -4432.471660535623\n",
            "SigmoidNAG without QG Testing Accuray at 439 iterations is 0.940700000000 with loss: -4432.664998280035\n",
            "SigmoidNAG without QG Testing Accuray at 440 iterations is 0.940400000000 with loss: -4432.647703113260\n",
            "SigmoidNAG without QG Testing Accuray at 441 iterations is 0.940200000000 with loss: -4432.211573438559\n",
            "SigmoidNAG without QG Testing Accuray at 442 iterations is 0.940900000000 with loss: -4431.157094648287\n",
            "SigmoidNAG without QG Testing Accuray at 443 iterations is 0.941000000000 with loss: -4429.308364310475\n",
            "SigmoidNAG without QG Testing Accuray at 444 iterations is 0.941000000000 with loss: -4426.527215545601\n",
            "SigmoidNAG without QG Testing Accuray at 445 iterations is 0.940900000000 with loss: -4422.725603041301\n",
            "SigmoidNAG without QG Testing Accuray at 446 iterations is 0.941300000000 with loss: -4417.875249387546\n",
            "SigmoidNAG without QG Testing Accuray at 447 iterations is 0.941900000000 with loss: -4412.013557095116\n",
            "SigmoidNAG without QG Testing Accuray at 448 iterations is 0.941700000000 with loss: -4405.244917936043\n",
            "SigmoidNAG without QG Testing Accuray at 449 iterations is 0.941900000000 with loss: -4397.736815468988\n",
            "SigmoidNAG without QG Testing Accuray at 450 iterations is 0.941900000000 with loss: -4389.710528247667\n",
            "SigmoidNAG without QG Testing Accuray at 451 iterations is 0.942300000000 with loss: -4381.426763470488\n",
            "SigmoidNAG without QG Testing Accuray at 452 iterations is 0.942300000000 with loss: -4373.167129511653\n",
            "SigmoidNAG without QG Testing Accuray at 453 iterations is 0.942700000000 with loss: -4365.212903142328\n",
            "SigmoidNAG without QG Testing Accuray at 454 iterations is 0.942800000000 with loss: -4357.822982874801\n",
            "SigmoidNAG without QG Testing Accuray at 455 iterations is 0.942900000000 with loss: -4351.213159555993\n",
            "SigmoidNAG without QG Testing Accuray at 456 iterations is 0.943100000000 with loss: -4345.538836597994\n",
            "SigmoidNAG without QG Testing Accuray at 457 iterations is 0.943400000000 with loss: -4340.883054281733\n",
            "SigmoidNAG without QG Testing Accuray at 458 iterations is 0.943900000000 with loss: -4337.251152410506\n",
            "SigmoidNAG without QG Testing Accuray at 459 iterations is 0.943800000000 with loss: -4334.572688559369\n",
            "SigmoidNAG without QG Testing Accuray at 460 iterations is 0.943800000000 with loss: -4332.710407369823\n",
            "SigmoidNAG without QG Testing Accuray at 461 iterations is 0.944300000000 with loss: -4331.475243388435\n",
            "SigmoidNAG without QG Testing Accuray at 462 iterations is 0.944300000000 with loss: -4330.645650028218\n",
            "SigmoidNAG without QG Testing Accuray at 463 iterations is 0.944300000000 with loss: -4329.989083402955\n",
            "SigmoidNAG without QG Testing Accuray at 464 iterations is 0.944100000000 with loss: -4329.283301345959\n",
            "SigmoidNAG without QG Testing Accuray at 465 iterations is 0.944600000000 with loss: -4328.335290951900\n",
            "SigmoidNAG without QG Testing Accuray at 466 iterations is 0.944500000000 with loss: -4326.996070635705\n",
            "SigmoidNAG without QG Testing Accuray at 467 iterations is 0.944900000000 with loss: -4325.170239668327\n",
            "SigmoidNAG without QG Testing Accuray at 468 iterations is 0.944600000000 with loss: -4322.819865830916\n",
            "SigmoidNAG without QG Testing Accuray at 469 iterations is 0.944500000000 with loss: -4319.962974993243\n",
            "SigmoidNAG without QG Testing Accuray at 470 iterations is 0.944000000000 with loss: -4316.667444733637\n",
            "SigmoidNAG without QG Testing Accuray at 471 iterations is 0.944200000000 with loss: -4313.041446099402\n",
            "SigmoidNAG without QG Testing Accuray at 472 iterations is 0.944200000000 with loss: -4309.221711468308\n",
            "SigmoidNAG without QG Testing Accuray at 473 iterations is 0.943800000000 with loss: -4305.360859940452\n",
            "SigmoidNAG without QG Testing Accuray at 474 iterations is 0.943400000000 with loss: -4301.614841731144\n",
            "SigmoidNAG without QG Testing Accuray at 475 iterations is 0.943500000000 with loss: -4298.131326557867\n",
            "SigmoidNAG without QG Testing Accuray at 476 iterations is 0.943300000000 with loss: -4295.039620007577\n",
            "SigmoidNAG without QG Testing Accuray at 477 iterations is 0.943100000000 with loss: -4292.442477475508\n",
            "SigmoidNAG without QG Testing Accuray at 478 iterations is 0.943000000000 with loss: -4290.410020529477\n",
            "SigmoidNAG without QG Testing Accuray at 479 iterations is 0.942400000000 with loss: -4288.975839228422\n",
            "SigmoidNAG without QG Testing Accuray at 480 iterations is 0.942200000000 with loss: -4288.135285847196\n",
            "SigmoidNAG without QG Testing Accuray at 481 iterations is 0.942200000000 with loss: -4287.845907475416\n",
            "SigmoidNAG without QG Testing Accuray at 482 iterations is 0.942300000000 with loss: -4288.029918598133\n",
            "SigmoidNAG without QG Testing Accuray at 483 iterations is 0.942200000000 with loss: -4288.578562771690\n",
            "SigmoidNAG without QG Testing Accuray at 484 iterations is 0.942800000000 with loss: -4289.358152518182\n",
            "SigmoidNAG without QG Testing Accuray at 485 iterations is 0.942600000000 with loss: -4290.217508857205\n",
            "SigmoidNAG without QG Testing Accuray at 486 iterations is 0.942900000000 with loss: -4290.996439256535\n",
            "SigmoidNAG without QG Testing Accuray at 487 iterations is 0.942600000000 with loss: -4291.534810331657\n",
            "SigmoidNAG without QG Testing Accuray at 488 iterations is 0.942800000000 with loss: -4291.681695374995\n",
            "SigmoidNAG without QG Testing Accuray at 489 iterations is 0.943100000000 with loss: -4291.304022670962\n",
            "SigmoidNAG without QG Testing Accuray at 490 iterations is 0.943300000000 with loss: -4290.294130048022\n",
            "SigmoidNAG without QG Testing Accuray at 491 iterations is 0.943100000000 with loss: -4288.575666194262\n",
            "SigmoidNAG without QG Testing Accuray at 492 iterations is 0.943300000000 with loss: -4286.107367341546\n",
            "SigmoidNAG without QG Testing Accuray at 493 iterations is 0.943400000000 with loss: -4282.884395077434\n",
            "SigmoidNAG without QG Testing Accuray at 494 iterations is 0.943700000000 with loss: -4278.937131816039\n",
            "SigmoidNAG without QG Testing Accuray at 495 iterations is 0.943500000000 with loss: -4274.327573123291\n",
            "SigmoidNAG without QG Testing Accuray at 496 iterations is 0.943700000000 with loss: -4269.143707834563\n",
            "SigmoidNAG without QG Testing Accuray at 497 iterations is 0.944100000000 with loss: -4263.492492428913\n",
            "SigmoidNAG without QG Testing Accuray at 498 iterations is 0.944200000000 with loss: -4257.492177589921\n",
            "SigmoidNAG without QG Testing Accuray at 499 iterations is 0.944500000000 with loss: -4251.264796540844\n",
            "SigmoidNAG without QG Testing Accuray at 500 iterations is 0.944900000000 with loss: -4244.929562608124\n",
            "X := \n",
            "[[1.         4.1531086  0.23424676 ... 0.5409698  0.00226453 5.1135426 ]\n",
            " [1.         3.1332304  1.2525738  ... 0.43917853 0.33902243 0.39484203]\n",
            " [1.         2.338178   0.21895619 ... 0.0468287  0.08023783 1.2495486 ]\n",
            " ...\n",
            " [1.         3.9063234  0.16548267 ... 0.17413217 0.02997403 2.9336329 ]\n",
            " [1.         3.8503726  0.10146441 ... 0.05712994 0.6157975  1.206366  ]\n",
            " [1.         2.8931568  0.08539145 ... 0.5368696  0.60432035 1.2711462 ]]\n",
            "XTX := \n",
            "[[ 60000.         179262.39857524  28305.00317187 ...  28212.81060076\n",
            "   11026.96964265 115438.03978926]\n",
            " [179262.39857524 556317.28758324  86762.00892237 ...  86116.52821465\n",
            "   33352.52691562 345457.55867444]\n",
            " [ 28305.00317187  86762.00892237  25213.07599876 ...  13176.12340958\n",
            "    5409.54617363  42473.82524258]\n",
            " ...\n",
            " [ 28212.81060076  86116.52821465  13176.12340958 ...  21323.70203095\n",
            "    4706.60782798  53236.79772101]\n",
            " [ 11026.96964265  33352.52691562   5409.54617363 ...   4706.60782798\n",
            "    5668.55982252  16814.26129102]\n",
            " [115438.03978926 345457.55867444  42473.82524258 ...  53236.79772101\n",
            "   16814.26129102 314287.71566024]]\n",
            "invBrow := \n",
            "[[0.00000028 0.00000009 0.00000058 0.00000039 0.00000045 0.00000024\n",
            "  0.00000217 0.00000028 0.00000051 0.00000034 0.00000014 0.00000062\n",
            "  0.00000374 0.00000184 0.00000115 0.00000405 0.00000084 0.00000038\n",
            "  0.00000046 0.00000011 0.00000049 0.00000149 0.00000177 0.00000189\n",
            "  0.00000072 0.00000052 0.00000053 0.00000037 0.00000039 0.00001238\n",
            "  0.00000021 0.00000022 0.00000017 0.00000468 0.00000013 0.00000028\n",
            "  0.00000012 0.00000048 0.00001046 0.00000085 0.00000193 0.00000024\n",
            "  0.00000021 0.00000156 0.00000091 0.00000079 0.00000203 0.00000054\n",
            "  0.00000092 0.00000176 0.00000154 0.00000033 0.00000043 0.00000069\n",
            "  0.00000145 0.00000017 0.00000133 0.00000207 0.0000003  0.00000017\n",
            "  0.00000028 0.00000073 0.00000078 0.00000484 0.00000024 0.00000046\n",
            "  0.00000041 0.00000298 0.00000044 0.00000026 0.00000011 0.00000158\n",
            "  0.00000059 0.00000123 0.00000097 0.0000009  0.00000027 0.00000263\n",
            "  0.00000094 0.00000153 0.00000021 0.0000003  0.00000054 0.00000272\n",
            "  0.00000206 0.00000813 0.00000175 0.00000051 0.00000037 0.00000076\n",
            "  0.00000075 0.00000017 0.00001044 0.00003291 0.00000025 0.00000091\n",
            "  0.00000112 0.00000195 0.0000129  0.00000057 0.00000183 0.00000022\n",
            "  0.00000191 0.00000099 0.00000075 0.00000181 0.00000018 0.00000058\n",
            "  0.00000053 0.00000094 0.00000066 0.00000519 0.00000034 0.00000049\n",
            "  0.00000076 0.00000018 0.00000053 0.00000012 0.00000041 0.00000159\n",
            "  0.00000005 0.00000042 0.00000037 0.00000041 0.00000135 0.00000128\n",
            "  0.0000003  0.00000032 0.00000329 0.00000484 0.00000059 0.00000021\n",
            "  0.0000007  0.00000714 0.0000004  0.00000038 0.0000005  0.00000007\n",
            "  0.00000032 0.00000048 0.00000072 0.00000162 0.00000484 0.00000016\n",
            "  0.00000009 0.00000102 0.00000054 0.00000212 0.00000028 0.00000016\n",
            "  0.00000067 0.00000031 0.00000505 0.00000432 0.00003359 0.00000067\n",
            "  0.00000092 0.00000106 0.00000043 0.00000024 0.00006621 0.00000062\n",
            "  0.00000019 0.00000058 0.00000054 0.00000316 0.00000091 0.00000053\n",
            "  0.00000259 0.00000035 0.0000004  0.00000147 0.00000044 0.00000175\n",
            "  0.00000073 0.0000004  0.0000017  0.00000118 0.00000161 0.00000013\n",
            "  0.00000136 0.00000408 0.00000065 0.00000037 0.00000099 0.00000162\n",
            "  0.00000096 0.000003   0.00000048 0.00000814 0.00000095 0.00000204\n",
            "  0.00000445 0.00000058 0.00000035 0.00000026 0.00000034 0.00000072\n",
            "  0.00000706 0.00000273 0.00000013 0.00000028 0.00000017 0.00000197\n",
            "  0.0000005  0.00000023 0.00000138 0.00000201 0.00000142 0.00000069\n",
            "  0.00000078 0.00000068 0.00000075 0.00000298 0.00000017 0.00000029\n",
            "  0.00000172 0.00000073 0.0000026  0.00000279 0.00000034 0.00000064\n",
            "  0.00000025 0.00000882 0.00000123 0.00000117 0.00000159 0.00000036\n",
            "  0.00000046 0.00000147 0.0000031  0.00000146 0.00000033 0.0000006\n",
            "  0.00000143 0.00000424 0.00000149 0.00000753 0.00000208 0.00000046\n",
            "  0.00000031 0.00000092 0.00000062 0.00000045 0.00000025 0.00000187\n",
            "  0.00000047 0.00000073 0.00000043 0.00001987 0.0000002  0.00000059\n",
            "  0.00000346 0.00000011 0.00000026 0.00000273 0.00000239 0.00000277\n",
            "  0.00000056 0.00000437 0.0000009  0.00000578 0.00000183 0.00000434\n",
            "  0.00000025 0.00000284 0.00000029 0.0000002  0.00000039 0.00000254\n",
            "  0.0000012  0.00000037 0.0000003  0.00000181 0.00000062 0.00000099\n",
            "  0.00000158 0.0000021  0.00000016 0.00000307 0.00000179 0.00000039\n",
            "  0.00000023 0.00000045 0.00000066 0.00001568 0.00000057 0.00000171\n",
            "  0.00000102 0.0000099  0.00000103 0.00000058 0.00000192 0.00000023\n",
            "  0.00000073 0.0000003  0.00000029 0.00000013 0.00000057 0.0000006\n",
            "  0.00000543 0.00000015 0.00000039 0.0000001  0.00000363 0.00000043\n",
            "  0.00000475 0.00000034 0.00000283 0.00000047 0.00000042 0.0000003\n",
            "  0.00000082 0.00001021 0.00000027 0.00000032 0.0000015  0.00000019\n",
            "  0.00000026 0.00000095 0.00000017 0.00000021 0.0000221  0.00000098\n",
            "  0.00000124 0.00000493 0.00000183 0.00000442 0.00000164 0.0000006\n",
            "  0.00000033 0.00000124 0.00000097 0.00000051 0.00000071 0.00000195\n",
            "  0.00000299 0.00000017 0.0000138  0.00000108 0.00000042 0.00000163\n",
            "  0.00000017 0.00001116 0.00000145 0.00002642 0.00000449 0.00000073\n",
            "  0.00000096 0.0000027  0.00000049 0.00001606 0.00000079 0.00000082\n",
            "  0.00000104 0.00000077 0.00000048 0.00000285 0.00000163 0.0000068\n",
            "  0.00000062 0.00000664 0.0000009  0.00000121 0.00000102 0.00000009\n",
            "  0.00000023 0.00000059 0.00000017 0.00000039 0.00000065 0.00000024\n",
            "  0.00000036 0.00000118 0.0000002  0.00000033 0.00000013 0.00000263\n",
            "  0.00000036 0.00000453 0.00000025 0.00000003 0.00000032 0.00000096\n",
            "  0.00000076 0.00000187 0.00000047 0.00000127 0.00000055 0.00000032\n",
            "  0.00000047 0.00000063 0.00000028 0.00000051 0.00000079 0.00000103\n",
            "  0.00000137 0.00000018 0.00000056 0.00000145 0.00000014]]\n",
            "SigmoidNAG with QG Testing Accuray at   1 iterations is 0.620600000000 with loss: -68673.752157726107\n",
            "SigmoidNAG with QG Testing Accuray at   2 iterations is 0.700300000000 with loss: -58238.731763275428\n",
            "SigmoidNAG with QG Testing Accuray at   3 iterations is 0.693000000000 with loss: -31836.216130369634\n",
            "SigmoidNAG with QG Testing Accuray at   4 iterations is 0.698400000000 with loss: -31128.572757555517\n",
            "SigmoidNAG with QG Testing Accuray at   5 iterations is 0.703200000000 with loss: -30598.193464197218\n",
            "SigmoidNAG with QG Testing Accuray at   6 iterations is 0.709100000000 with loss: -29426.684041750283\n",
            "SigmoidNAG with QG Testing Accuray at   7 iterations is 0.719600000000 with loss: -28048.990530740932\n",
            "SigmoidNAG with QG Testing Accuray at   8 iterations is 0.733600000000 with loss: -26836.858395563457\n",
            "SigmoidNAG with QG Testing Accuray at   9 iterations is 0.745700000000 with loss: -25863.048189389396\n",
            "SigmoidNAG with QG Testing Accuray at  10 iterations is 0.759500000000 with loss: -24943.262822061130\n",
            "SigmoidNAG with QG Testing Accuray at  11 iterations is 0.770100000000 with loss: -23928.784179228722\n",
            "SigmoidNAG with QG Testing Accuray at  12 iterations is 0.778800000000 with loss: -22876.416127965480\n",
            "SigmoidNAG with QG Testing Accuray at  13 iterations is 0.784000000000 with loss: -21902.802499460573\n",
            "SigmoidNAG with QG Testing Accuray at  14 iterations is 0.792000000000 with loss: -21032.501555125338\n",
            "SigmoidNAG with QG Testing Accuray at  15 iterations is 0.799300000000 with loss: -20223.927019671846\n",
            "SigmoidNAG with QG Testing Accuray at  16 iterations is 0.805500000000 with loss: -19447.834770036519\n",
            "SigmoidNAG with QG Testing Accuray at  17 iterations is 0.809700000000 with loss: -18711.312714141648\n",
            "SigmoidNAG with QG Testing Accuray at  18 iterations is 0.814100000000 with loss: -18033.938215187041\n",
            "SigmoidNAG with QG Testing Accuray at  19 iterations is 0.817600000000 with loss: -17419.941001182335\n",
            "SigmoidNAG with QG Testing Accuray at  20 iterations is 0.820200000000 with loss: -16854.949004789374\n",
            "SigmoidNAG with QG Testing Accuray at  21 iterations is 0.822300000000 with loss: -16323.211530251127\n",
            "SigmoidNAG with QG Testing Accuray at  22 iterations is 0.824400000000 with loss: -15820.557550634603\n",
            "SigmoidNAG with QG Testing Accuray at  23 iterations is 0.827600000000 with loss: -15350.531556560130\n",
            "SigmoidNAG with QG Testing Accuray at  24 iterations is 0.830300000000 with loss: -14914.566554880619\n",
            "SigmoidNAG with QG Testing Accuray at  25 iterations is 0.833800000000 with loss: -14509.072957726015\n",
            "SigmoidNAG with QG Testing Accuray at  26 iterations is 0.837400000000 with loss: -14129.140818303027\n",
            "SigmoidNAG with QG Testing Accuray at  27 iterations is 0.839300000000 with loss: -13772.054618824795\n",
            "SigmoidNAG with QG Testing Accuray at  28 iterations is 0.842400000000 with loss: -13437.175308031650\n",
            "SigmoidNAG with QG Testing Accuray at  29 iterations is 0.844700000000 with loss: -13123.776531078831\n",
            "SigmoidNAG with QG Testing Accuray at  30 iterations is 0.847100000000 with loss: -12829.888025636195\n",
            "SigmoidNAG with QG Testing Accuray at  31 iterations is 0.848500000000 with loss: -12553.054126304114\n",
            "SigmoidNAG with QG Testing Accuray at  32 iterations is 0.849900000000 with loss: -12291.421456479888\n",
            "SigmoidNAG with QG Testing Accuray at  33 iterations is 0.852400000000 with loss: -12043.702642385266\n",
            "SigmoidNAG with QG Testing Accuray at  34 iterations is 0.853700000000 with loss: -11808.473362195826\n",
            "SigmoidNAG with QG Testing Accuray at  35 iterations is 0.855000000000 with loss: -11584.002943712459\n",
            "SigmoidNAG with QG Testing Accuray at  36 iterations is 0.856200000000 with loss: -11368.816216038902\n",
            "SigmoidNAG with QG Testing Accuray at  37 iterations is 0.858400000000 with loss: -11162.231390190320\n",
            "SigmoidNAG with QG Testing Accuray at  38 iterations is 0.860000000000 with loss: -10964.282268715266\n",
            "SigmoidNAG with QG Testing Accuray at  39 iterations is 0.862500000000 with loss: -10775.166196930817\n",
            "SigmoidNAG with QG Testing Accuray at  40 iterations is 0.865000000000 with loss: -10594.766859319981\n",
            "SigmoidNAG with QG Testing Accuray at  41 iterations is 0.866900000000 with loss: -10422.594876929381\n",
            "SigmoidNAG with QG Testing Accuray at  42 iterations is 0.868400000000 with loss: -10258.021025627579\n",
            "SigmoidNAG with QG Testing Accuray at  43 iterations is 0.870000000000 with loss: -10100.463935553522\n",
            "SigmoidNAG with QG Testing Accuray at  44 iterations is 0.871900000000 with loss: -9949.378774378054\n",
            "SigmoidNAG with QG Testing Accuray at  45 iterations is 0.873600000000 with loss: -9804.165724623261\n",
            "SigmoidNAG with QG Testing Accuray at  46 iterations is 0.874300000000 with loss: -9664.166988581481\n",
            "SigmoidNAG with QG Testing Accuray at  47 iterations is 0.876400000000 with loss: -9528.776933574196\n",
            "SigmoidNAG with QG Testing Accuray at  48 iterations is 0.878200000000 with loss: -9397.565087810521\n",
            "SigmoidNAG with QG Testing Accuray at  49 iterations is 0.879400000000 with loss: -9270.316900527712\n",
            "SigmoidNAG with QG Testing Accuray at  50 iterations is 0.881000000000 with loss: -9146.983650416769\n",
            "SigmoidNAG with QG Testing Accuray at  51 iterations is 0.883000000000 with loss: -9027.599963014860\n",
            "SigmoidNAG with QG Testing Accuray at  52 iterations is 0.883700000000 with loss: -8912.222998631933\n",
            "SigmoidNAG with QG Testing Accuray at  53 iterations is 0.885700000000 with loss: -8800.897196638620\n",
            "SigmoidNAG with QG Testing Accuray at  54 iterations is 0.887200000000 with loss: -8693.617329552229\n",
            "SigmoidNAG with QG Testing Accuray at  55 iterations is 0.888400000000 with loss: -8590.281800349143\n",
            "SigmoidNAG with QG Testing Accuray at  56 iterations is 0.889700000000 with loss: -8490.664191410096\n",
            "SigmoidNAG with QG Testing Accuray at  57 iterations is 0.891100000000 with loss: -8394.434200903039\n",
            "SigmoidNAG with QG Testing Accuray at  58 iterations is 0.892200000000 with loss: -8301.224357327681\n",
            "SigmoidNAG with QG Testing Accuray at  59 iterations is 0.893600000000 with loss: -8210.704880750598\n",
            "SigmoidNAG with QG Testing Accuray at  60 iterations is 0.894700000000 with loss: -8122.627852194529\n",
            "SigmoidNAG with QG Testing Accuray at  61 iterations is 0.895400000000 with loss: -8036.829135181749\n",
            "SigmoidNAG with QG Testing Accuray at  62 iterations is 0.897000000000 with loss: -7953.205006300454\n",
            "SigmoidNAG with QG Testing Accuray at  63 iterations is 0.898300000000 with loss: -7871.688911453349\n",
            "SigmoidNAG with QG Testing Accuray at  64 iterations is 0.899500000000 with loss: -7792.241229629126\n",
            "SigmoidNAG with QG Testing Accuray at  65 iterations is 0.900500000000 with loss: -7714.846503016096\n",
            "SigmoidNAG with QG Testing Accuray at  66 iterations is 0.901400000000 with loss: -7639.504826338605\n",
            "SigmoidNAG with QG Testing Accuray at  67 iterations is 0.902300000000 with loss: -7566.211859032699\n",
            "SigmoidNAG with QG Testing Accuray at  68 iterations is 0.902900000000 with loss: -7494.935746034486\n",
            "SigmoidNAG with QG Testing Accuray at  69 iterations is 0.903900000000 with loss: -7425.605253177685\n",
            "SigmoidNAG with QG Testing Accuray at  70 iterations is 0.904600000000 with loss: -7358.116408093871\n",
            "SigmoidNAG with QG Testing Accuray at  71 iterations is 0.905800000000 with loss: -7292.352020527612\n",
            "SigmoidNAG with QG Testing Accuray at  72 iterations is 0.906700000000 with loss: -7228.200943095576\n",
            "SigmoidNAG with QG Testing Accuray at  73 iterations is 0.907200000000 with loss: -7165.566950357731\n",
            "SigmoidNAG with QG Testing Accuray at  74 iterations is 0.908300000000 with loss: -7104.366608392953\n",
            "SigmoidNAG with QG Testing Accuray at  75 iterations is 0.908900000000 with loss: -7044.523291042302\n",
            "SigmoidNAG with QG Testing Accuray at  76 iterations is 0.909200000000 with loss: -6985.965163231928\n",
            "SigmoidNAG with QG Testing Accuray at  77 iterations is 0.909900000000 with loss: -6928.629359706310\n",
            "SigmoidNAG with QG Testing Accuray at  78 iterations is 0.910800000000 with loss: -6872.468066972479\n",
            "SigmoidNAG with QG Testing Accuray at  79 iterations is 0.911700000000 with loss: -6817.450081752513\n",
            "SigmoidNAG with QG Testing Accuray at  80 iterations is 0.912100000000 with loss: -6763.554958220304\n",
            "SigmoidNAG with QG Testing Accuray at  81 iterations is 0.912600000000 with loss: -6710.762836551801\n",
            "SigmoidNAG with QG Testing Accuray at  82 iterations is 0.913100000000 with loss: -6659.046517509363\n",
            "SigmoidNAG with QG Testing Accuray at  83 iterations is 0.914000000000 with loss: -6608.370824517413\n",
            "SigmoidNAG with QG Testing Accuray at  84 iterations is 0.914700000000 with loss: -6558.699148989297\n",
            "SigmoidNAG with QG Testing Accuray at  85 iterations is 0.916000000000 with loss: -6510.002325361545\n",
            "SigmoidNAG with QG Testing Accuray at  86 iterations is 0.916300000000 with loss: -6462.263771786042\n",
            "SigmoidNAG with QG Testing Accuray at  87 iterations is 0.917000000000 with loss: -6415.477712029834\n",
            "SigmoidNAG with QG Testing Accuray at  88 iterations is 0.917800000000 with loss: -6369.641850353318\n",
            "SigmoidNAG with QG Testing Accuray at  89 iterations is 0.918800000000 with loss: -6324.749127845099\n",
            "SigmoidNAG with QG Testing Accuray at  90 iterations is 0.919200000000 with loss: -6280.783246135354\n",
            "SigmoidNAG with QG Testing Accuray at  91 iterations is 0.919600000000 with loss: -6237.719810385927\n",
            "SigmoidNAG with QG Testing Accuray at  92 iterations is 0.920400000000 with loss: -6195.531291064831\n",
            "SigmoidNAG with QG Testing Accuray at  93 iterations is 0.921200000000 with loss: -6154.191896321950\n",
            "SigmoidNAG with QG Testing Accuray at  94 iterations is 0.921900000000 with loss: -6113.679039622593\n",
            "SigmoidNAG with QG Testing Accuray at  95 iterations is 0.922400000000 with loss: -6073.970792646579\n",
            "SigmoidNAG with QG Testing Accuray at  96 iterations is 0.923300000000 with loss: -6035.041558886118\n",
            "SigmoidNAG with QG Testing Accuray at  97 iterations is 0.923700000000 with loss: -5996.859389926221\n",
            "SigmoidNAG with QG Testing Accuray at  98 iterations is 0.924000000000 with loss: -5959.387279553885\n",
            "SigmoidNAG with QG Testing Accuray at  99 iterations is 0.924600000000 with loss: -5922.588251039851\n",
            "SigmoidNAG with QG Testing Accuray at 100 iterations is 0.925100000000 with loss: -5886.431797304996\n",
            "SigmoidNAG with QG Testing Accuray at 101 iterations is 0.925300000000 with loss: -5850.898461002196\n",
            "SigmoidNAG with QG Testing Accuray at 102 iterations is 0.925600000000 with loss: -5815.980400327026\n",
            "SigmoidNAG with QG Testing Accuray at 103 iterations is 0.926200000000 with loss: -5781.677871795466\n",
            "SigmoidNAG with QG Testing Accuray at 104 iterations is 0.926400000000 with loss: -5747.993480186808\n",
            "SigmoidNAG with QG Testing Accuray at 105 iterations is 0.926700000000 with loss: -5714.926812167057\n",
            "SigmoidNAG with QG Testing Accuray at 106 iterations is 0.927100000000 with loss: -5682.471454620122\n",
            "SigmoidNAG with QG Testing Accuray at 107 iterations is 0.927700000000 with loss: -5650.614915288347\n",
            "SigmoidNAG with QG Testing Accuray at 108 iterations is 0.928000000000 with loss: -5619.340502738362\n",
            "SigmoidNAG with QG Testing Accuray at 109 iterations is 0.928500000000 with loss: -5588.629531486499\n",
            "SigmoidNAG with QG Testing Accuray at 110 iterations is 0.929000000000 with loss: -5558.462564174634\n",
            "SigmoidNAG with QG Testing Accuray at 111 iterations is 0.929500000000 with loss: -5528.819377734400\n",
            "SigmoidNAG with QG Testing Accuray at 112 iterations is 0.929500000000 with loss: -5499.678312215119\n",
            "SigmoidNAG with QG Testing Accuray at 113 iterations is 0.929800000000 with loss: -5471.016046582594\n",
            "SigmoidNAG with QG Testing Accuray at 114 iterations is 0.930700000000 with loss: -5442.808524623383\n",
            "SigmoidNAG with QG Testing Accuray at 115 iterations is 0.931200000000 with loss: -5415.032984516898\n",
            "SigmoidNAG with QG Testing Accuray at 116 iterations is 0.931600000000 with loss: -5387.670328056394\n",
            "SigmoidNAG with QG Testing Accuray at 117 iterations is 0.932000000000 with loss: -5360.706804782567\n",
            "SigmoidNAG with QG Testing Accuray at 118 iterations is 0.932200000000 with loss: -5334.134272239716\n",
            "SigmoidNAG with QG Testing Accuray at 119 iterations is 0.932500000000 with loss: -5307.948937991844\n",
            "SigmoidNAG with QG Testing Accuray at 120 iterations is 0.932900000000 with loss: -5282.149145284358\n",
            "SigmoidNAG with QG Testing Accuray at 121 iterations is 0.933100000000 with loss: -5256.733097852433\n",
            "SigmoidNAG with QG Testing Accuray at 122 iterations is 0.934000000000 with loss: -5231.697311715267\n",
            "SigmoidNAG with QG Testing Accuray at 123 iterations is 0.934300000000 with loss: -5207.036145998739\n",
            "SigmoidNAG with QG Testing Accuray at 124 iterations is 0.934600000000 with loss: -5182.742232447124\n",
            "SigmoidNAG with QG Testing Accuray at 125 iterations is 0.935100000000 with loss: -5158.807287456947\n",
            "SigmoidNAG with QG Testing Accuray at 126 iterations is 0.935700000000 with loss: -5135.222775317488\n",
            "SigmoidNAG with QG Testing Accuray at 127 iterations is 0.936100000000 with loss: -5111.980143441102\n",
            "SigmoidNAG with QG Testing Accuray at 128 iterations is 0.936300000000 with loss: -5089.070690474576\n",
            "SigmoidNAG with QG Testing Accuray at 129 iterations is 0.936600000000 with loss: -5066.485371510160\n",
            "SigmoidNAG with QG Testing Accuray at 130 iterations is 0.936900000000 with loss: -5044.214862905447\n",
            "SigmoidNAG with QG Testing Accuray at 131 iterations is 0.937200000000 with loss: -5022.250022336451\n",
            "SigmoidNAG with QG Testing Accuray at 132 iterations is 0.937700000000 with loss: -5000.582613432488\n",
            "SigmoidNAG with QG Testing Accuray at 133 iterations is 0.937700000000 with loss: -4979.205978513841\n",
            "SigmoidNAG with QG Testing Accuray at 134 iterations is 0.937700000000 with loss: -4958.115328981808\n",
            "SigmoidNAG with QG Testing Accuray at 135 iterations is 0.937900000000 with loss: -4937.307470204096\n",
            "SigmoidNAG with QG Testing Accuray at 136 iterations is 0.938200000000 with loss: -4916.780021140622\n",
            "SigmoidNAG with QG Testing Accuray at 137 iterations is 0.938600000000 with loss: -4896.530388790329\n",
            "SigmoidNAG with QG Testing Accuray at 138 iterations is 0.938700000000 with loss: -4876.554849676614\n",
            "SigmoidNAG with QG Testing Accuray at 139 iterations is 0.938900000000 with loss: -4856.848014038154\n",
            "SigmoidNAG with QG Testing Accuray at 140 iterations is 0.939500000000 with loss: -4837.402785574236\n",
            "SigmoidNAG with QG Testing Accuray at 141 iterations is 0.939700000000 with loss: -4818.210738499449\n",
            "SigmoidNAG with QG Testing Accuray at 142 iterations is 0.939900000000 with loss: -4799.262720608890\n",
            "SigmoidNAG with QG Testing Accuray at 143 iterations is 0.940200000000 with loss: -4780.549472465614\n",
            "SigmoidNAG with QG Testing Accuray at 144 iterations is 0.940300000000 with loss: -4762.062138275509\n",
            "SigmoidNAG with QG Testing Accuray at 145 iterations is 0.940600000000 with loss: -4743.792647153448\n",
            "SigmoidNAG with QG Testing Accuray at 146 iterations is 0.940600000000 with loss: -4725.734021851017\n",
            "SigmoidNAG with QG Testing Accuray at 147 iterations is 0.940800000000 with loss: -4707.880674752269\n",
            "SigmoidNAG with QG Testing Accuray at 148 iterations is 0.940800000000 with loss: -4690.228692858082\n",
            "SigmoidNAG with QG Testing Accuray at 149 iterations is 0.940900000000 with loss: -4672.776037772715\n",
            "SigmoidNAG with QG Testing Accuray at 150 iterations is 0.941100000000 with loss: -4655.522537237734\n",
            "SigmoidNAG with QG Testing Accuray at 151 iterations is 0.941200000000 with loss: -4638.469562529464\n",
            "SigmoidNAG with QG Testing Accuray at 152 iterations is 0.941300000000 with loss: -4621.619367499732\n",
            "SigmoidNAG with QG Testing Accuray at 153 iterations is 0.941600000000 with loss: -4604.974174692691\n",
            "SigmoidNAG with QG Testing Accuray at 154 iterations is 0.941900000000 with loss: -4588.535191627277\n",
            "SigmoidNAG with QG Testing Accuray at 155 iterations is 0.942100000000 with loss: -4572.301772170608\n",
            "SigmoidNAG with QG Testing Accuray at 156 iterations is 0.942200000000 with loss: -4556.270912978581\n",
            "SigmoidNAG with QG Testing Accuray at 157 iterations is 0.942400000000 with loss: -4540.437173463046\n",
            "SigmoidNAG with QG Testing Accuray at 158 iterations is 0.943000000000 with loss: -4524.793001031251\n",
            "SigmoidNAG with QG Testing Accuray at 159 iterations is 0.943200000000 with loss: -4509.329345901525\n",
            "SigmoidNAG with QG Testing Accuray at 160 iterations is 0.943200000000 with loss: -4494.036410673579\n",
            "SigmoidNAG with QG Testing Accuray at 161 iterations is 0.943300000000 with loss: -4478.904394318970\n",
            "SigmoidNAG with QG Testing Accuray at 162 iterations is 0.943200000000 with loss: -4463.924141240886\n",
            "SigmoidNAG with QG Testing Accuray at 163 iterations is 0.943500000000 with loss: -4449.087665698965\n",
            "SigmoidNAG with QG Testing Accuray at 164 iterations is 0.943800000000 with loss: -4434.388564594810\n",
            "SigmoidNAG with QG Testing Accuray at 165 iterations is 0.943800000000 with loss: -4419.822334890629\n",
            "SigmoidNAG with QG Testing Accuray at 166 iterations is 0.944100000000 with loss: -4405.386594500500\n",
            "SigmoidNAG with QG Testing Accuray at 167 iterations is 0.944100000000 with loss: -4391.081168499929\n",
            "SigmoidNAG with QG Testing Accuray at 168 iterations is 0.944300000000 with loss: -4376.907987122325\n",
            "SigmoidNAG with QG Testing Accuray at 169 iterations is 0.944400000000 with loss: -4362.870747389130\n",
            "SigmoidNAG with QG Testing Accuray at 170 iterations is 0.944600000000 with loss: -4348.974329093993\n",
            "SigmoidNAG with QG Testing Accuray at 171 iterations is 0.944700000000 with loss: -4335.224014319463\n",
            "SigmoidNAG with QG Testing Accuray at 172 iterations is 0.944800000000 with loss: -4321.624611069092\n",
            "SigmoidNAG with QG Testing Accuray at 173 iterations is 0.945000000000 with loss: -4308.179613164192\n",
            "SigmoidNAG with QG Testing Accuray at 174 iterations is 0.945300000000 with loss: -4294.890530645919\n",
            "SigmoidNAG with QG Testing Accuray at 175 iterations is 0.945400000000 with loss: -4281.756487228178\n",
            "SigmoidNAG with QG Testing Accuray at 176 iterations is 0.945500000000 with loss: -4268.774130942172\n",
            "SigmoidNAG with QG Testing Accuray at 177 iterations is 0.945700000000 with loss: -4255.937843973492\n",
            "SigmoidNAG with QG Testing Accuray at 178 iterations is 0.945900000000 with loss: -4243.240196572162\n",
            "SigmoidNAG with QG Testing Accuray at 179 iterations is 0.946300000000 with loss: -4230.672561061486\n",
            "SigmoidNAG with QG Testing Accuray at 180 iterations is 0.946300000000 with loss: -4218.225803223443\n",
            "SigmoidNAG with QG Testing Accuray at 181 iterations is 0.946400000000 with loss: -4205.890974923585\n",
            "SigmoidNAG with QG Testing Accuray at 182 iterations is 0.946800000000 with loss: -4193.659951203124\n",
            "SigmoidNAG with QG Testing Accuray at 183 iterations is 0.946800000000 with loss: -4181.525969554652\n",
            "SigmoidNAG with QG Testing Accuray at 184 iterations is 0.947000000000 with loss: -4169.484037998843\n",
            "SigmoidNAG with QG Testing Accuray at 185 iterations is 0.947100000000 with loss: -4157.531185740624\n",
            "SigmoidNAG with QG Testing Accuray at 186 iterations is 0.947500000000 with loss: -4145.666534600534\n",
            "SigmoidNAG with QG Testing Accuray at 187 iterations is 0.947500000000 with loss: -4133.891182711635\n",
            "SigmoidNAG with QG Testing Accuray at 188 iterations is 0.947500000000 with loss: -4122.207906672416\n",
            "SigmoidNAG with QG Testing Accuray at 189 iterations is 0.948100000000 with loss: -4110.620706869522\n",
            "SigmoidNAG with QG Testing Accuray at 190 iterations is 0.948200000000 with loss: -4099.134242667803\n",
            "SigmoidNAG with QG Testing Accuray at 191 iterations is 0.948400000000 with loss: -4087.753216937101\n",
            "SigmoidNAG with QG Testing Accuray at 192 iterations is 0.948600000000 with loss: -4076.481778335626\n",
            "SigmoidNAG with QG Testing Accuray at 193 iterations is 0.948800000000 with loss: -4065.323005010966\n",
            "SigmoidNAG with QG Testing Accuray at 194 iterations is 0.949000000000 with loss: -4054.278521396925\n",
            "SigmoidNAG with QG Testing Accuray at 195 iterations is 0.948900000000 with loss: -4043.348280651409\n",
            "SigmoidNAG with QG Testing Accuray at 196 iterations is 0.949200000000 with loss: -4032.530526719297\n",
            "SigmoidNAG with QG Testing Accuray at 197 iterations is 0.949400000000 with loss: -4021.821928861215\n",
            "SigmoidNAG with QG Testing Accuray at 198 iterations is 0.949400000000 with loss: -4011.217864975391\n",
            "SigmoidNAG with QG Testing Accuray at 199 iterations is 0.949700000000 with loss: -4000.712817011872\n",
            "SigmoidNAG with QG Testing Accuray at 200 iterations is 0.949900000000 with loss: -3990.300832636691\n",
            "SigmoidNAG with QG Testing Accuray at 201 iterations is 0.950100000000 with loss: -3979.976003603882\n",
            "SigmoidNAG with QG Testing Accuray at 202 iterations is 0.950100000000 with loss: -3969.732908531174\n",
            "SigmoidNAG with QG Testing Accuray at 203 iterations is 0.950200000000 with loss: -3959.566976596245\n",
            "SigmoidNAG with QG Testing Accuray at 204 iterations is 0.950300000000 with loss: -3949.474738305804\n",
            "SigmoidNAG with QG Testing Accuray at 205 iterations is 0.950300000000 with loss: -3939.453946534212\n",
            "SigmoidNAG with QG Testing Accuray at 206 iterations is 0.950300000000 with loss: -3929.503566479833\n",
            "SigmoidNAG with QG Testing Accuray at 207 iterations is 0.950600000000 with loss: -3919.623649673146\n",
            "SigmoidNAG with QG Testing Accuray at 208 iterations is 0.950900000000 with loss: -3909.815121125380\n",
            "SigmoidNAG with QG Testing Accuray at 209 iterations is 0.951300000000 with loss: -3900.079509426523\n",
            "SigmoidNAG with QG Testing Accuray at 210 iterations is 0.951500000000 with loss: -3890.418655023915\n",
            "SigmoidNAG with QG Testing Accuray at 211 iterations is 0.951600000000 with loss: -3880.834424853314\n",
            "SigmoidNAG with QG Testing Accuray at 212 iterations is 0.951700000000 with loss: -3871.328453046472\n",
            "SigmoidNAG with QG Testing Accuray at 213 iterations is 0.951800000000 with loss: -3861.901926465717\n",
            "SigmoidNAG with QG Testing Accuray at 214 iterations is 0.952000000000 with loss: -3852.555423748888\n",
            "SigmoidNAG with QG Testing Accuray at 215 iterations is 0.952300000000 with loss: -3843.288817024301\n",
            "SigmoidNAG with QG Testing Accuray at 216 iterations is 0.952300000000 with loss: -3834.101242616081\n",
            "SigmoidNAG with QG Testing Accuray at 217 iterations is 0.952500000000 with loss: -3824.991145799266\n",
            "SigmoidNAG with QG Testing Accuray at 218 iterations is 0.952600000000 with loss: -3815.956398175870\n",
            "SigmoidNAG with QG Testing Accuray at 219 iterations is 0.952700000000 with loss: -3806.994479070372\n",
            "SigmoidNAG with QG Testing Accuray at 220 iterations is 0.952900000000 with loss: -3798.102699574296\n",
            "SigmoidNAG with QG Testing Accuray at 221 iterations is 0.953000000000 with loss: -3789.278443286913\n",
            "SigmoidNAG with QG Testing Accuray at 222 iterations is 0.953100000000 with loss: -3780.519388506284\n",
            "SigmoidNAG with QG Testing Accuray at 223 iterations is 0.953100000000 with loss: -3771.823679650391\n",
            "SigmoidNAG with QG Testing Accuray at 224 iterations is 0.953000000000 with loss: -3763.190022815110\n",
            "SigmoidNAG with QG Testing Accuray at 225 iterations is 0.953200000000 with loss: -3754.617693086806\n",
            "SigmoidNAG with QG Testing Accuray at 226 iterations is 0.953300000000 with loss: -3746.106457797822\n",
            "SigmoidNAG with QG Testing Accuray at 227 iterations is 0.953400000000 with loss: -3737.656433248725\n",
            "SigmoidNAG with QG Testing Accuray at 228 iterations is 0.953700000000 with loss: -3729.267902372959\n",
            "SigmoidNAG with QG Testing Accuray at 229 iterations is 0.953700000000 with loss: -3720.941126237417\n",
            "SigmoidNAG with QG Testing Accuray at 230 iterations is 0.953900000000 with loss: -3712.676175065567\n",
            "SigmoidNAG with QG Testing Accuray at 231 iterations is 0.953700000000 with loss: -3704.472801282264\n",
            "SigmoidNAG with QG Testing Accuray at 232 iterations is 0.953900000000 with loss: -3696.330364924610\n",
            "SigmoidNAG with QG Testing Accuray at 233 iterations is 0.954100000000 with loss: -3688.247811907167\n",
            "SigmoidNAG with QG Testing Accuray at 234 iterations is 0.954200000000 with loss: -3680.223700698977\n",
            "SigmoidNAG with QG Testing Accuray at 235 iterations is 0.954300000000 with loss: -3672.256269602004\n",
            "SigmoidNAG with QG Testing Accuray at 236 iterations is 0.954600000000 with loss: -3664.343536068749\n",
            "SigmoidNAG with QG Testing Accuray at 237 iterations is 0.954700000000 with loss: -3656.483420870610\n",
            "SigmoidNAG with QG Testing Accuray at 238 iterations is 0.954800000000 with loss: -3648.673888920887\n",
            "SigmoidNAG with QG Testing Accuray at 239 iterations is 0.954900000000 with loss: -3640.913097841990\n",
            "SigmoidNAG with QG Testing Accuray at 240 iterations is 0.954800000000 with loss: -3633.199542718395\n",
            "SigmoidNAG with QG Testing Accuray at 241 iterations is 0.954800000000 with loss: -3625.532181636855\n",
            "SigmoidNAG with QG Testing Accuray at 242 iterations is 0.954800000000 with loss: -3617.910526731557\n",
            "SigmoidNAG with QG Testing Accuray at 243 iterations is 0.954800000000 with loss: -3610.334684141304\n",
            "SigmoidNAG with QG Testing Accuray at 244 iterations is 0.954800000000 with loss: -3602.805332951486\n",
            "SigmoidNAG with QG Testing Accuray at 245 iterations is 0.954800000000 with loss: -3595.323639216328\n",
            "SigmoidNAG with QG Testing Accuray at 246 iterations is 0.954900000000 with loss: -3587.891112117601\n",
            "SigmoidNAG with QG Testing Accuray at 247 iterations is 0.955000000000 with loss: -3580.509416859372\n",
            "SigmoidNAG with QG Testing Accuray at 248 iterations is 0.955000000000 with loss: -3573.180163707069\n",
            "SigmoidNAG with QG Testing Accuray at 249 iterations is 0.955400000000 with loss: -3565.904698455240\n",
            "SigmoidNAG with QG Testing Accuray at 250 iterations is 0.955400000000 with loss: -3558.683916185772\n",
            "SigmoidNAG with QG Testing Accuray at 251 iterations is 0.955600000000 with loss: -3551.518114733615\n",
            "SigmoidNAG with QG Testing Accuray at 252 iterations is 0.955800000000 with loss: -3544.406901148770\n",
            "SigmoidNAG with QG Testing Accuray at 253 iterations is 0.956000000000 with loss: -3537.349156719087\n",
            "SigmoidNAG with QG Testing Accuray at 254 iterations is 0.956100000000 with loss: -3530.343060174934\n",
            "SigmoidNAG with QG Testing Accuray at 255 iterations is 0.956200000000 with loss: -3523.386167148651\n",
            "SigmoidNAG with QG Testing Accuray at 256 iterations is 0.956300000000 with loss: -3516.475540683368\n",
            "SigmoidNAG with QG Testing Accuray at 257 iterations is 0.956600000000 with loss: -3509.607924445738\n",
            "SigmoidNAG with QG Testing Accuray at 258 iterations is 0.956700000000 with loss: -3502.779949938297\n",
            "SigmoidNAG with QG Testing Accuray at 259 iterations is 0.956900000000 with loss: -3495.988364780374\n",
            "SigmoidNAG with QG Testing Accuray at 260 iterations is 0.956900000000 with loss: -3489.230264851300\n",
            "SigmoidNAG with QG Testing Accuray at 261 iterations is 0.957100000000 with loss: -3482.503311865532\n",
            "SigmoidNAG with QG Testing Accuray at 262 iterations is 0.957100000000 with loss: -3475.805914374420\n",
            "SigmoidNAG with QG Testing Accuray at 263 iterations is 0.957100000000 with loss: -3469.137352016525\n",
            "SigmoidNAG with QG Testing Accuray at 264 iterations is 0.957200000000 with loss: -3462.497827634249\n",
            "SigmoidNAG with QG Testing Accuray at 265 iterations is 0.957300000000 with loss: -3455.888438615818\n",
            "SigmoidNAG with QG Testing Accuray at 266 iterations is 0.957300000000 with loss: -3449.311068625743\n",
            "SigmoidNAG with QG Testing Accuray at 267 iterations is 0.957300000000 with loss: -3442.768211415515\n",
            "SigmoidNAG with QG Testing Accuray at 268 iterations is 0.957300000000 with loss: -3436.262744863121\n",
            "SigmoidNAG with QG Testing Accuray at 269 iterations is 0.957200000000 with loss: -3429.797678059627\n",
            "SigmoidNAG with QG Testing Accuray at 270 iterations is 0.957200000000 with loss: -3423.375897168592\n",
            "SigmoidNAG with QG Testing Accuray at 271 iterations is 0.957400000000 with loss: -3416.999931872679\n",
            "SigmoidNAG with QG Testing Accuray at 272 iterations is 0.957400000000 with loss: -3410.671759451941\n",
            "SigmoidNAG with QG Testing Accuray at 273 iterations is 0.957500000000 with loss: -3404.392658361605\n",
            "SigmoidNAG with QG Testing Accuray at 274 iterations is 0.957400000000 with loss: -3398.163117344348\n",
            "SigmoidNAG with QG Testing Accuray at 275 iterations is 0.957400000000 with loss: -3391.982800827599\n",
            "SigmoidNAG with QG Testing Accuray at 276 iterations is 0.957400000000 with loss: -3385.850570069536\n",
            "SigmoidNAG with QG Testing Accuray at 277 iterations is 0.957500000000 with loss: -3379.764555981489\n",
            "SigmoidNAG with QG Testing Accuray at 278 iterations is 0.957500000000 with loss: -3373.722280192887\n",
            "SigmoidNAG with QG Testing Accuray at 279 iterations is 0.957700000000 with loss: -3367.720818938895\n",
            "SigmoidNAG with QG Testing Accuray at 280 iterations is 0.957700000000 with loss: -3361.757001278262\n",
            "SigmoidNAG with QG Testing Accuray at 281 iterations is 0.957800000000 with loss: -3355.827630116138\n",
            "SigmoidNAG with QG Testing Accuray at 282 iterations is 0.958200000000 with loss: -3349.929710841191\n",
            "SigmoidNAG with QG Testing Accuray at 283 iterations is 0.958200000000 with loss: -3344.060669024992\n",
            "SigmoidNAG with QG Testing Accuray at 284 iterations is 0.958300000000 with loss: -3338.218538259364\n",
            "SigmoidNAG with QG Testing Accuray at 285 iterations is 0.958300000000 with loss: -3332.402098986089\n",
            "SigmoidNAG with QG Testing Accuray at 286 iterations is 0.958500000000 with loss: -3326.610954993308\n",
            "SigmoidNAG with QG Testing Accuray at 287 iterations is 0.958400000000 with loss: -3320.845539755017\n",
            "SigmoidNAG with QG Testing Accuray at 288 iterations is 0.958300000000 with loss: -3315.107052719726\n",
            "SigmoidNAG with QG Testing Accuray at 289 iterations is 0.958400000000 with loss: -3309.397334076222\n",
            "SigmoidNAG with QG Testing Accuray at 290 iterations is 0.958500000000 with loss: -3303.718692438535\n",
            "SigmoidNAG with QG Testing Accuray at 291 iterations is 0.958600000000 with loss: -3298.073703455552\n",
            "SigmoidNAG with QG Testing Accuray at 292 iterations is 0.958600000000 with loss: -3292.464998529540\n",
            "SigmoidNAG with QG Testing Accuray at 293 iterations is 0.958700000000 with loss: -3286.895061198454\n",
            "SigmoidNAG with QG Testing Accuray at 294 iterations is 0.958700000000 with loss: -3281.366045846153\n",
            "SigmoidNAG with QG Testing Accuray at 295 iterations is 0.958700000000 with loss: -3275.879628913639\n",
            "SigmoidNAG with QG Testing Accuray at 296 iterations is 0.958800000000 with loss: -3270.436899112884\n",
            "SigmoidNAG with QG Testing Accuray at 297 iterations is 0.958700000000 with loss: -3265.038289389888\n",
            "SigmoidNAG with QG Testing Accuray at 298 iterations is 0.959000000000 with loss: -3259.683553612676\n",
            "SigmoidNAG with QG Testing Accuray at 299 iterations is 0.959000000000 with loss: -3254.371786057385\n",
            "SigmoidNAG with QG Testing Accuray at 300 iterations is 0.959100000000 with loss: -3249.101483601399\n",
            "SigmoidNAG with QG Testing Accuray at 301 iterations is 0.959200000000 with loss: -3243.870646622152\n",
            "SigmoidNAG with QG Testing Accuray at 302 iterations is 0.959200000000 with loss: -3238.676913949736\n",
            "SigmoidNAG with QG Testing Accuray at 303 iterations is 0.959200000000 with loss: -3233.517723839588\n",
            "SigmoidNAG with QG Testing Accuray at 304 iterations is 0.959200000000 with loss: -3228.390489194757\n",
            "SigmoidNAG with QG Testing Accuray at 305 iterations is 0.959500000000 with loss: -3223.292773098718\n",
            "SigmoidNAG with QG Testing Accuray at 306 iterations is 0.959500000000 with loss: -3218.222450379766\n",
            "SigmoidNAG with QG Testing Accuray at 307 iterations is 0.959500000000 with loss: -3213.177840693893\n",
            "SigmoidNAG with QG Testing Accuray at 308 iterations is 0.959700000000 with loss: -3208.157801348158\n",
            "SigmoidNAG with QG Testing Accuray at 309 iterations is 0.959700000000 with loss: -3203.161772242538\n",
            "SigmoidNAG with QG Testing Accuray at 310 iterations is 0.959800000000 with loss: -3198.189770917302\n",
            "SigmoidNAG with QG Testing Accuray at 311 iterations is 0.959900000000 with loss: -3193.242341298258\n",
            "SigmoidNAG with QG Testing Accuray at 312 iterations is 0.960000000000 with loss: -3188.320463592685\n",
            "SigmoidNAG with QG Testing Accuray at 313 iterations is 0.960000000000 with loss: -3183.425436182914\n",
            "SigmoidNAG with QG Testing Accuray at 314 iterations is 0.960100000000 with loss: -3178.558741742129\n",
            "SigmoidNAG with QG Testing Accuray at 315 iterations is 0.960100000000 with loss: -3173.721909788912\n",
            "SigmoidNAG with QG Testing Accuray at 316 iterations is 0.960200000000 with loss: -3168.916386082076\n",
            "SigmoidNAG with QG Testing Accuray at 317 iterations is 0.960300000000 with loss: -3164.143416643914\n",
            "SigmoidNAG with QG Testing Accuray at 318 iterations is 0.960300000000 with loss: -3159.403952978894\n",
            "SigmoidNAG with QG Testing Accuray at 319 iterations is 0.960300000000 with loss: -3154.698582391369\n",
            "SigmoidNAG with QG Testing Accuray at 320 iterations is 0.960400000000 with loss: -3150.027485583737\n",
            "SigmoidNAG with QG Testing Accuray at 321 iterations is 0.960400000000 with loss: -3145.390423411395\n",
            "SigmoidNAG with QG Testing Accuray at 322 iterations is 0.960400000000 with loss: -3140.786752910756\n",
            "SigmoidNAG with QG Testing Accuray at 323 iterations is 0.960600000000 with loss: -3136.215471774195\n",
            "SigmoidNAG with QG Testing Accuray at 327 iterations is 0.961000000000 with loss: -3118.226049452256\n",
            "SigmoidNAG with QG Testing Accuray at 328 iterations is 0.961300000000 with loss: -3113.794940824087\n",
            "SigmoidNAG with QG Testing Accuray at 329 iterations is 0.961300000000 with loss: -3109.387585340001\n",
            "SigmoidNAG with QG Testing Accuray at 330 iterations is 0.961300000000 with loss: -3105.003014555958\n",
            "SigmoidNAG with QG Testing Accuray at 331 iterations is 0.961400000000 with loss: -3100.640575593324\n",
            "SigmoidNAG with QG Testing Accuray at 332 iterations is 0.961500000000 with loss: -3096.299941024705\n",
            "SigmoidNAG with QG Testing Accuray at 333 iterations is 0.961500000000 with loss: -3091.981090246494\n",
            "SigmoidNAG with QG Testing Accuray at 334 iterations is 0.961500000000 with loss: -3087.684266978540\n",
            "SigmoidNAG with QG Testing Accuray at 335 iterations is 0.961500000000 with loss: -3083.409918881077\n",
            "SigmoidNAG with QG Testing Accuray at 336 iterations is 0.961600000000 with loss: -3079.158626770302\n",
            "SigmoidNAG with QG Testing Accuray at 337 iterations is 0.961600000000 with loss: -3074.931029980443\n",
            "SigmoidNAG with QG Testing Accuray at 338 iterations is 0.961700000000 with loss: -3070.727754294072\n",
            "SigmoidNAG with QG Testing Accuray at 339 iterations is 0.961700000000 with loss: -3066.549346996417\n",
            "SigmoidNAG with QG Testing Accuray at 340 iterations is 0.961700000000 with loss: -3062.396221977008\n",
            "SigmoidNAG with QG Testing Accuray at 341 iterations is 0.961700000000 with loss: -3058.268616660242\n",
            "SigmoidNAG with QG Testing Accuray at 342 iterations is 0.961800000000 with loss: -3054.166562531651\n",
            "SigmoidNAG with QG Testing Accuray at 343 iterations is 0.961900000000 with loss: -3050.089870576143\n",
            "SigmoidNAG with QG Testing Accuray at 344 iterations is 0.961900000000 with loss: -3046.038132019725\n",
            "SigmoidNAG with QG Testing Accuray at 345 iterations is 0.962100000000 with loss: -3042.010735422805\n",
            "SigmoidNAG with QG Testing Accuray at 346 iterations is 0.962300000000 with loss: -3038.006900093622\n",
            "SigmoidNAG with QG Testing Accuray at 347 iterations is 0.962500000000 with loss: -3034.025724251598\n",
            "SigmoidNAG with QG Testing Accuray at 348 iterations is 0.962600000000 with loss: -3030.066244375925\n",
            "SigmoidNAG with QG Testing Accuray at 349 iterations is 0.962600000000 with loss: -3026.127501358657\n",
            "SigmoidNAG with QG Testing Accuray at 350 iterations is 0.962600000000 with loss: -3022.208607112582\n",
            "SigmoidNAG with QG Testing Accuray at 351 iterations is 0.962600000000 with loss: -3018.308804389720\n",
            "SigmoidNAG with QG Testing Accuray at 352 iterations is 0.962600000000 with loss: -3014.427513698317\n",
            "SigmoidNAG with QG Testing Accuray at 353 iterations is 0.962600000000 with loss: -3010.564361332295\n",
            "SigmoidNAG with QG Testing Accuray at 354 iterations is 0.962700000000 with loss: -3006.719186098388\n",
            "SigmoidNAG with QG Testing Accuray at 355 iterations is 0.962700000000 with loss: -3002.892024091968\n",
            "SigmoidNAG with QG Testing Accuray at 356 iterations is 0.962700000000 with loss: -2999.083074102583\n",
            "SigmoidNAG with QG Testing Accuray at 357 iterations is 0.962700000000 with loss: -2995.292648179157\n",
            "SigmoidNAG with QG Testing Accuray at 358 iterations is 0.962700000000 with loss: -2991.521114115823\n",
            "SigmoidNAG with QG Testing Accuray at 359 iterations is 0.962900000000 with loss: -2987.768835568971\n",
            "SigmoidNAG with QG Testing Accuray at 360 iterations is 0.962900000000 with loss: -2984.036116148404\n",
            "SigmoidNAG with QG Testing Accuray at 361 iterations is 0.962900000000 with loss: -2980.323152762605\n",
            "SigmoidNAG with QG Testing Accuray at 362 iterations is 0.962900000000 with loss: -2976.630000835548\n",
            "SigmoidNAG with QG Testing Accuray at 363 iterations is 0.962900000000 with loss: -2972.956553177416\n",
            "SigmoidNAG with QG Testing Accuray at 364 iterations is 0.962900000000 with loss: -2969.302532840149\n",
            "SigmoidNAG with QG Testing Accuray at 365 iterations is 0.963000000000 with loss: -2965.667498722821\n",
            "SigmoidNAG with QG Testing Accuray at 366 iterations is 0.963000000000 with loss: -2962.050863056852\n",
            "SigmoidNAG with QG Testing Accuray at 367 iterations is 0.963100000000 with loss: -2958.451919333084\n",
            "SigmoidNAG with QG Testing Accuray at 368 iterations is 0.963200000000 with loss: -2954.869879207487\n",
            "SigmoidNAG with QG Testing Accuray at 369 iterations is 0.963200000000 with loss: -2951.303916948370\n",
            "SigmoidNAG with QG Testing Accuray at 370 iterations is 0.963200000000 with loss: -2947.753219155490\n",
            "SigmoidNAG with QG Testing Accuray at 371 iterations is 0.963200000000 with loss: -2944.217037014232\n",
            "SigmoidNAG with QG Testing Accuray at 372 iterations is 0.963300000000 with loss: -2940.694738067771\n",
            "SigmoidNAG with QG Testing Accuray at 373 iterations is 0.963300000000 with loss: -2937.185853319587\n",
            "SigmoidNAG with QG Testing Accuray at 374 iterations is 0.963300000000 with loss: -2933.690114950388\n",
            "SigmoidNAG with QG Testing Accuray at 375 iterations is 0.963400000000 with loss: -2930.207480772859\n",
            "SigmoidNAG with QG Testing Accuray at 376 iterations is 0.963400000000 with loss: -2926.738142394433\n",
            "SigmoidNAG with QG Testing Accuray at 377 iterations is 0.963300000000 with loss: -2923.282514596916\n",
            "SigmoidNAG with QG Testing Accuray at 378 iterations is 0.963500000000 with loss: -2919.841206595909\n",
            "SigmoidNAG with QG Testing Accuray at 379 iterations is 0.963500000000 with loss: -2916.414976388481\n",
            "SigmoidNAG with QG Testing Accuray at 380 iterations is 0.963600000000 with loss: -2913.004672190241\n",
            "SigmoidNAG with QG Testing Accuray at 381 iterations is 0.963600000000 with loss: -2909.611165948133\n",
            "SigmoidNAG with QG Testing Accuray at 382 iterations is 0.963600000000 with loss: -2906.235284234005\n",
            "SigmoidNAG with QG Testing Accuray at 383 iterations is 0.963700000000 with loss: -2902.877742318080\n",
            "SigmoidNAG with QG Testing Accuray at 384 iterations is 0.963800000000 with loss: -2899.539086415496\n",
            "SigmoidNAG with QG Testing Accuray at 385 iterations is 0.963800000000 with loss: -2896.219648158310\n",
            "SigmoidNAG with QG Testing Accuray at 386 iterations is 0.963800000000 with loss: -2892.919513682689\n",
            "SigmoidNAG with QG Testing Accuray at 387 iterations is 0.963900000000 with loss: -2889.638509194578\n",
            "SigmoidNAG with QG Testing Accuray at 388 iterations is 0.964000000000 with loss: -2886.376203358540\n",
            "SigmoidNAG with QG Testing Accuray at 389 iterations is 0.964100000000 with loss: -2883.131926049302\n",
            "SigmoidNAG with QG Testing Accuray at 390 iterations is 0.964100000000 with loss: -2879.904802150776\n",
            "SigmoidNAG with QG Testing Accuray at 391 iterations is 0.964200000000 with loss: -2876.693798670236\n",
            "SigmoidNAG with QG Testing Accuray at 392 iterations is 0.964200000000 with loss: -2873.497782706506\n",
            "SigmoidNAG with QG Testing Accuray at 393 iterations is 0.964200000000 with loss: -2870.315587327270\n",
            "SigmoidNAG with QG Testing Accuray at 394 iterations is 0.964200000000 with loss: -2867.146081686235\n",
            "SigmoidNAG with QG Testing Accuray at 395 iterations is 0.964200000000 with loss: -2863.988240831888\n",
            "SigmoidNAG with QG Testing Accuray at 396 iterations is 0.964300000000 with loss: -2860.841210773119\n",
            "SigmoidNAG with QG Testing Accuray at 397 iterations is 0.964400000000 with loss: -2857.704364155469\n",
            "SigmoidNAG with QG Testing Accuray at 398 iterations is 0.964400000000 with loss: -2854.577341764946\n",
            "SigmoidNAG with QG Testing Accuray at 399 iterations is 0.964600000000 with loss: -2851.460076411005\n",
            "SigmoidNAG with QG Testing Accuray at 400 iterations is 0.964600000000 with loss: -2848.352796779864\n",
            "SigmoidNAG with QG Testing Accuray at 401 iterations is 0.964700000000 with loss: -2845.256010813945\n",
            "SigmoidNAG with QG Testing Accuray at 402 iterations is 0.964700000000 with loss: -2842.170469783787\n",
            "SigmoidNAG with QG Testing Accuray at 403 iterations is 0.964800000000 with loss: -2839.097115643711\n",
            "SigmoidNAG with QG Testing Accuray at 404 iterations is 0.964800000000 with loss: -2836.037016275237\n",
            "SigmoidNAG with QG Testing Accuray at 405 iterations is 0.964900000000 with loss: -2832.991293100125\n",
            "SigmoidNAG with QG Testing Accuray at 406 iterations is 0.964900000000 with loss: -2829.961046881524\n",
            "SigmoidNAG with QG Testing Accuray at 407 iterations is 0.964900000000 with loss: -2826.947286464193\n",
            "SigmoidNAG with QG Testing Accuray at 408 iterations is 0.964900000000 with loss: -2823.950864964462\n",
            "SigmoidNAG with QG Testing Accuray at 409 iterations is 0.964900000000 with loss: -2820.972426946127\n",
            "SigmoidNAG with QG Testing Accuray at 410 iterations is 0.964800000000 with loss: -2818.012368900514\n",
            "SigmoidNAG with QG Testing Accuray at 411 iterations is 0.965000000000 with loss: -2815.070814851497\n",
            "SigmoidNAG with QG Testing Accuray at 412 iterations is 0.965000000000 with loss: -2812.147607254828\n",
            "SigmoidNAG with QG Testing Accuray at 413 iterations is 0.965200000000 with loss: -2809.242313444255\n",
            "SigmoidNAG with QG Testing Accuray at 414 iterations is 0.965200000000 with loss: -2806.354247262124\n",
            "SigmoidNAG with QG Testing Accuray at 415 iterations is 0.965200000000 with loss: -2803.482504454551\n",
            "SigmoidNAG with QG Testing Accuray at 416 iterations is 0.965200000000 with loss: -2800.626010588450\n",
            "SigmoidNAG with QG Testing Accuray at 417 iterations is 0.965200000000 with loss: -2797.783579388981\n",
            "SigmoidNAG with QG Testing Accuray at 418 iterations is 0.965300000000 with loss: -2794.953978185486\n",
            "SigmoidNAG with QG Testing Accuray at 419 iterations is 0.965300000000 with loss: -2792.135997233812\n",
            "SigmoidNAG with QG Testing Accuray at 420 iterations is 0.965400000000 with loss: -2789.328518306148\n",
            "SigmoidNAG with QG Testing Accuray at 421 iterations is 0.965500000000 with loss: -2786.530578478845\n",
            "SigmoidNAG with QG Testing Accuray at 422 iterations is 0.965500000000 with loss: -2783.741424348262\n",
            "SigmoidNAG with QG Testing Accuray at 423 iterations is 0.965500000000 with loss: -2780.960552853134\n",
            "SigmoidNAG with QG Testing Accuray at 424 iterations is 0.965500000000 with loss: -2778.187735637255\n",
            "SigmoidNAG with QG Testing Accuray at 427 iterations is 0.965700000000 with loss: -2769.919442282452\n",
            "SigmoidNAG with QG Testing Accuray at 428 iterations is 0.965700000000 with loss: -2767.181880688172\n",
            "SigmoidNAG with QG Testing Accuray at 429 iterations is 0.965700000000 with loss: -2764.454946340609\n",
            "SigmoidNAG with QG Testing Accuray at 430 iterations is 0.965700000000 with loss: -2761.739605505461\n",
            "SigmoidNAG with QG Testing Accuray at 431 iterations is 0.965800000000 with loss: -2759.036836425244\n",
            "SigmoidNAG with QG Testing Accuray at 432 iterations is 0.965800000000 with loss: -2756.347567156577\n",
            "SigmoidNAG with QG Testing Accuray at 433 iterations is 0.965900000000 with loss: -2753.672619128871\n",
            "SigmoidNAG with QG Testing Accuray at 434 iterations is 0.965900000000 with loss: -2751.012659268281\n",
            "SigmoidNAG with QG Testing Accuray at 435 iterations is 0.966000000000 with loss: -2748.368162870590\n",
            "SigmoidNAG with QG Testing Accuray at 436 iterations is 0.966000000000 with loss: -2745.739388583057\n",
            "SigmoidNAG with QG Testing Accuray at 437 iterations is 0.966000000000 with loss: -2743.126365892026\n",
            "SigmoidNAG with QG Testing Accuray at 438 iterations is 0.966000000000 with loss: -2740.528895370345\n",
            "SigmoidNAG with QG Testing Accuray at 439 iterations is 0.966000000000 with loss: -2737.946561499640\n",
            "SigmoidNAG with QG Testing Accuray at 440 iterations is 0.966000000000 with loss: -2735.378757117359\n",
            "SigmoidNAG with QG Testing Accuray at 441 iterations is 0.966000000000 with loss: -2732.824718080046\n",
            "SigmoidNAG with QG Testing Accuray at 442 iterations is 0.966100000000 with loss: -2730.283566366519\n",
            "SigmoidNAG with QG Testing Accuray at 443 iterations is 0.966000000000 with loss: -2727.754359359140\n",
            "SigmoidNAG with QG Testing Accuray at 444 iterations is 0.966000000000 with loss: -2725.236142252020\n",
            "SigmoidNAG with QG Testing Accuray at 445 iterations is 0.966000000000 with loss: -2722.728000646196\n",
            "SigmoidNAG with QG Testing Accuray at 446 iterations is 0.966000000000 with loss: -2720.229109649334\n",
            "SigmoidNAG with QG Testing Accuray at 447 iterations is 0.966000000000 with loss: -2717.738776364949\n",
            "SigmoidNAG with QG Testing Accuray at 448 iterations is 0.966000000000 with loss: -2715.256473069094\n",
            "SigmoidNAG with QG Testing Accuray at 449 iterations is 0.966000000000 with loss: -2712.781858774507\n",
            "SigmoidNAG with QG Testing Accuray at 450 iterations is 0.966000000000 with loss: -2710.314787813322\n",
            "SigmoidNAG with QG Testing Accuray at 451 iterations is 0.966100000000 with loss: -2707.855305695954\n",
            "SigmoidNAG with QG Testing Accuray at 452 iterations is 0.966200000000 with loss: -2705.403632774025\n",
            "SigmoidNAG with QG Testing Accuray at 453 iterations is 0.966300000000 with loss: -2702.960137300019\n",
            "SigmoidNAG with QG Testing Accuray at 454 iterations is 0.966300000000 with loss: -2700.525300467935\n",
            "SigmoidNAG with QG Testing Accuray at 455 iterations is 0.966200000000 with loss: -2698.099676032679\n",
            "SigmoidNAG with QG Testing Accuray at 456 iterations is 0.966300000000 with loss: -2695.683847440968\n",
            "SigmoidNAG with QG Testing Accuray at 457 iterations is 0.966400000000 with loss: -2693.278385328829\n",
            "SigmoidNAG with QG Testing Accuray at 458 iterations is 0.966400000000 with loss: -2690.883807757758\n",
            "SigmoidNAG with QG Testing Accuray at 459 iterations is 0.966400000000 with loss: -2688.500545347553\n",
            "SigmoidNAG with QG Testing Accuray at 460 iterations is 0.966600000000 with loss: -2686.128913039812\n",
            "SigmoidNAG with QG Testing Accuray at 461 iterations is 0.966500000000 with loss: -2683.769089483257\n",
            "SigmoidNAG with QG Testing Accuray at 462 iterations is 0.966500000000 with loss: -2681.421105018767\n",
            "SigmoidNAG with QG Testing Accuray at 463 iterations is 0.966600000000 with loss: -2679.084838567181\n",
            "SigmoidNAG with QG Testing Accuray at 464 iterations is 0.966600000000 with loss: -2676.760023430939\n",
            "SigmoidNAG with QG Testing Accuray at 465 iterations is 0.966600000000 with loss: -2674.446261621747\n",
            "SigmoidNAG with QG Testing Accuray at 466 iterations is 0.966600000000 with loss: -2672.143045805403\n",
            "SigmoidNAG with QG Testing Accuray at 467 iterations is 0.966600000000 with loss: -2669.849787536341\n",
            "SigmoidNAG with QG Testing Accuray at 468 iterations is 0.966600000000 with loss: -2667.565850065078\n",
            "SigmoidNAG with QG Testing Accuray at 469 iterations is 0.966600000000 with loss: -2665.290583699857\n",
            "SigmoidNAG with QG Testing Accuray at 470 iterations is 0.966700000000 with loss: -2663.023361003768\n",
            "SigmoidNAG with QG Testing Accuray at 471 iterations is 0.966700000000 with loss: -2660.763609839744\n",
            "SigmoidNAG with QG Testing Accuray at 472 iterations is 0.966700000000 with loss: -2658.510841545916\n",
            "SigmoidNAG with QG Testing Accuray at 473 iterations is 0.966900000000 with loss: -2656.264672728045\n",
            "SigmoidNAG with QG Testing Accuray at 474 iterations is 0.967000000000 with loss: -2654.024839195724\n",
            "SigmoidNAG with QG Testing Accuray at 475 iterations is 0.967000000000 with loss: -2651.791201405958\n",
            "SigmoidNAG with QG Testing Accuray at 476 iterations is 0.967000000000 with loss: -2649.563741438039\n",
            "SigmoidNAG with QG Testing Accuray at 477 iterations is 0.967000000000 with loss: -2647.342552179096\n",
            "SigmoidNAG with QG Testing Accuray at 478 iterations is 0.967000000000 with loss: -2645.127820558088\n",
            "SigmoidNAG with QG Testing Accuray at 479 iterations is 0.966900000000 with loss: -2642.919805750238\n",
            "SigmoidNAG with QG Testing Accuray at 480 iterations is 0.966900000000 with loss: -2640.718814628207\n",
            "SigmoidNAG with QG Testing Accuray at 481 iterations is 0.966900000000 with loss: -2638.525176195235\n",
            "SigmoidNAG with QG Testing Accuray at 482 iterations is 0.967000000000 with loss: -2636.339216624014\n",
            "SigmoidNAG with QG Testing Accuray at 483 iterations is 0.967100000000 with loss: -2634.161236275424\n",
            "SigmoidNAG with QG Testing Accuray at 484 iterations is 0.967100000000 with loss: -2631.991489789046\n",
            "SigmoidNAG with QG Testing Accuray at 485 iterations is 0.967100000000 with loss: -2629.830170056642\n",
            "SigmoidNAG with QG Testing Accuray at 486 iterations is 0.967100000000 with loss: -2627.677396625481\n",
            "SigmoidNAG with QG Testing Accuray at 487 iterations is 0.967200000000 with loss: -2625.533208797548\n",
            "SigmoidNAG with QG Testing Accuray at 488 iterations is 0.967200000000 with loss: -2623.397563870180\n",
            "SigmoidNAG with QG Testing Accuray at 489 iterations is 0.967300000000 with loss: -2621.270340384913\n",
            "SigmoidNAG with QG Testing Accuray at 490 iterations is 0.967300000000 with loss: -2619.151346122972\n",
            "SigmoidNAG with QG Testing Accuray at 491 iterations is 0.967300000000 with loss: -2617.040330417520\n",
            "SigmoidNAG with QG Testing Accuray at 492 iterations is 0.967400000000 with loss: -2614.937000501510\n",
            "SigmoidNAG with QG Testing Accuray at 493 iterations is 0.967500000000 with loss: -2612.841040450078\n",
            "SigmoidNAG with QG Testing Accuray at 494 iterations is 0.967600000000 with loss: -2610.752131572738\n",
            "SigmoidNAG with QG Testing Accuray at 495 iterations is 0.967700000000 with loss: -2608.669973297637\n",
            "SigmoidNAG with QG Testing Accuray at 496 iterations is 0.967500000000 with loss: -2606.594302198072\n",
            "SigmoidNAG with QG Testing Accuray at 497 iterations is 0.967500000000 with loss: -2604.524908305295\n",
            "SigmoidNAG with QG Testing Accuray at 498 iterations is 0.967500000000 with loss: -2602.461647064755\n",
            "SigmoidNAG with QG Testing Accuray at 499 iterations is 0.967500000000 with loss: -2600.404445948837\n",
            "SigmoidNAG with QG Testing Accuray at 500 iterations is 0.967500000000 with loss: -2598.353305444435\n",
            "TotalEnAdagradTimeDiff = \n",
            "346.32156109809875\n",
            "AveraEnAdagradTimeDiff = \n",
            "0.6926431221961975\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "\n",
        "import csv\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, datasets\n",
        "import torchvision.transforms as trnsfrms\n",
        "from torchvision.transforms import ToTensor, Resize, Lambda\n",
        "\n",
        "trnsfrms = trnsfrms.Compose([Resize(224), ToTensor(),  Lambda(lambda x: x.repeat(3, 1, 1) ) ])  # Grayscale Images like MNIST and USPS\n",
        "#trnsfrms = trnsfrms.Compose([Resize(224), ToTensor(), ])                                       # Color Images like CIFAR10\n",
        "#trnsfrms = trnsfrms.Compose([ ToTensor(), ]) \n",
        "\n",
        "# Download training data from open datasets.FashionMNIST.MNIST.USPS  / CIFAR10\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform= trnsfrms\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.FashionMNIST.MNIST\n",
        "testing_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform= trnsfrms\n",
        ")\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(testing_data, batch_size=batch_size)\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "#model = models.regnet_x_400mf(pretrained=True)  #Linear(in_features=400,\n",
        "#model = models.regnet_y_400mf(pretrained=True)  #(fc): Linear(in_features=440,\n",
        "#model = models.regnet_x_800mf(pretrained=True)  #Linear(in_features=672,\n",
        "#model = models.regnet_y_800mf(pretrained=True)  #(fc): Linear(in_features=784,\n",
        "#model = models.regnet_y_1_6gf(pretrained=True)  #Linear(in_features=888,\n",
        "#model = models.regnet_x_1_6gf(pretrained=True)  #Linear(in_features=912,\n",
        "#model = models.regnet_x_3_2gf(pretrained=True)  #Linear(in_features=1008,\n",
        "\n",
        "#### ConvNet as fixed feature extractor ####\n",
        "model = models.regnet_x_400mf(pretrained=True)  \n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "model.fc = nn.Sequential(*list(model.fc.children())[:-1])\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "print(model.fc)\n",
        "print(type(model.fc))\n",
        "\n",
        "\n",
        "# Save the raw dataset: USPS MNIST CIFAR10\n",
        "train_dataset = []\n",
        "size = len(train_dataloader.dataset)\n",
        "num_batches = len(train_dataloader)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X, y in train_dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        print(f\"Shape of model(X) [N, C, H, W]: {pred.shape}\")\n",
        "        print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "        pred = torch.reshape(pred, (pred.shape[0], -1) )\n",
        "        y = torch.reshape(y, (y.shape[0], -1) )\n",
        "        print(f\"Shape of model(X): {pred.shape} {pred.dtype}\")\n",
        "        print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "\n",
        "        train_dataset += torch.cat( (y, pred ), 1)\n",
        "        print(f\"Shape of train_dataset: {len(train_dataset)}, {len(train_dataset[0])}\")\n",
        "\n",
        "print(\"train_dataset :\" + str(len(train_dataset)) + \",\\t\" + str(len(train_dataset[0])) )\n",
        "print(type(train_dataset))\n",
        "with open('REGNET_X_400MF_MNIST_TRAINING.csv', 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    for i in range( len(train_dataset)):                                   #len(train_dataset)):\n",
        "      writer.writerow(train_dataset[i].detach().cpu().numpy())\n",
        "csvfile.close()\n",
        "\n",
        "\n",
        "\n",
        "# Save the raw dataset: USPS MNIST CIFAR10\n",
        "test_dataset = []\n",
        "size = len(test_dataloader.dataset)\n",
        "num_batches = len(test_dataloader)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X, y in test_dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        print(f\"Shape of model(X) [N, C, H, W]: {pred.shape}\")\n",
        "        print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "        pred = torch.reshape(pred, (pred.shape[0], -1) )\n",
        "        y = torch.reshape(y, (y.shape[0], -1) )\n",
        "        print(f\"Shape of model(X): {pred.shape} {pred.dtype}\")\n",
        "        print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "\n",
        "        test_dataset += torch.cat( (y, pred ), 1)\n",
        "        print(f\"Shape of test_dataset: {len(test_dataset)}, {len(test_dataset[0])}\")\n",
        "\n",
        "print(\"test_dataset :\" + str(len(test_dataset)) + \",\\t\" + str(len(test_dataset[0])) )\n",
        "print(type(test_dataset))\n",
        "with open('REGNET_X_400MF_MNIST_TESTING.csv', 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    for i in range(len(test_dataset)):\n",
        "      writer.writerow(test_dataset[i].detach().cpu().numpy())\n",
        "csvfile.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# coding=utf8\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "from copy import deepcopy\n",
        "from math import log, exp, pow, sqrt\n",
        "\n",
        "import matplotlib\n",
        "#matplotlib.use('pdf')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "\n",
        "\n",
        "#hlambda = lambda x:1.0/(1+exp(-x))\n",
        "#Sigmoid(x) ~ poly3 = 0.5 + 0.10679534503216294.*x + -0.00038503259805075.*x.^3; (lambda = 128)\n",
        "def hlambda(x):\n",
        "    x[x>+8] = +8\n",
        "    x[x<-8] = -8\n",
        "    res = 1 / (1 + np.exp(-x) )\n",
        "    #res = 5.0000e-01  + 0.10679534503216294 * x  - 0.00038503259805075 * np.multiply(np.multiply(x,  x), x)\n",
        "    return res \n",
        "\n",
        "\n",
        "\n",
        "import csv\n",
        "epsilon = 1e-10\n",
        "num_iter = 500\n",
        "\n",
        "\n",
        "with open(\"REGNET_X_400MF_MNIST_TRAINING.csv\",'r') as csvfile:\n",
        "#with open(\"REGNET_X_400MF_MNIST_TESTING.csv\",'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    traindata = []\n",
        "    for row in reader:\n",
        "        row = [float(x) for x in row]\n",
        "        traindata.append(row)\n",
        "csvfile.close()\n",
        "with open(\"REGNET_X_400MF_MNIST_TESTING.csv\", 'r') as csvfile:\n",
        "#with open(\"REGNET_X_400MF_MNIST_TRAININGfirst8192.csv\",'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    testdata = []\n",
        "    for row in reader:\n",
        "        row = [float(x) for x in row]\n",
        "        testdata.append(row)\n",
        "csvfile.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#traindata = traindata[:6144]\n",
        "TrainX = [row[1:] for row in traindata[:]]\n",
        "TestX = [row[1:] for row in testdata[:]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X = []\n",
        "Xtest  = []\n",
        "\n",
        "\n",
        "for (rowidx, row) in enumerate(TrainX):\n",
        "    #TrainData.append( [ trainy[rowidx] ] + row )\n",
        "    X.append( [ 1.0 ] + row )\n",
        "for (rowidx, row) in enumerate(TestX):\n",
        "    #TestData.append( [ testy[rowidx] ] + row )\n",
        "    Xtest.append( [ 1.0 ] + row )\n",
        "   \n",
        "#X = X[:]\n",
        "X = np.matrix(X)\n",
        "Xtest = np.matrix(Xtest)\n",
        "\n",
        "\n",
        "ytrain = [int(row[0]) for row in traindata[:]]\n",
        "ytest = [int(row[0]) for row in testdata[:]]\n",
        "\n",
        "#y = self.one_hot(y)\n",
        "Y = []\n",
        "for rowidx in range( len(traindata) ):\n",
        "    row = []\n",
        "    for colidx in range( len( set(ytrain + ytest) ) ):\n",
        "        if colidx == ytrain[rowidx]:\n",
        "            row.append( 1 )\n",
        "        else:\n",
        "            row.append( 0 )\n",
        "    Y.append( row )\n",
        "\n",
        "#Y = Y[:]\n",
        "Y = np.matrix(Y)\n",
        "\n",
        "Ytest = []\n",
        "for rowidx in range( len(testdata) ):\n",
        "    row = []\n",
        "    for colidx in range( len( set(ytrain + ytest) ) ):\n",
        "        if colidx == ytest[rowidx]:\n",
        "            row.append( 1 )\n",
        "        else:\n",
        "            row.append( 0 )\n",
        "    Ytest.append( row )\n",
        "Ytest = np.matrix(Ytest)\n",
        "\n",
        "\n",
        "#     Step 2. Extract X and Y from data\n",
        "\n",
        "\n",
        "\n",
        "def precision(vec0, vec1):\n",
        "    if len(vec0) != len(vec1):\n",
        "        return -1\n",
        "    totalnum = len(vec0)\n",
        "    rightnum = 0.0\n",
        "    for idx in range(totalnum):\n",
        "        if vec0[idx] == vec1[idx]:\n",
        "            rightnum += 1\n",
        "    return rightnum / totalnum\n",
        "\n",
        "\n",
        "\n",
        "# ======================================= Raw NAG without QG ======================================= \n",
        "# Build the initial weight matrix\n",
        "velocity = []\n",
        "for rowidx in range( len(set(ytrain)) ):\n",
        "    vrow = []\n",
        "    for colidx in range( X.shape[1] ):\n",
        "        vrow.append( 0.0 )\n",
        "    velocity.append(vrow)\n",
        "velocity = np.matrix( velocity )\n",
        "weights = []\n",
        "for rowidx in range( len(set(ytest+ytrain)) ):\n",
        "    wrow = []\n",
        "    for colidx in range( X.shape[1] ):\n",
        "        wrow.append( 0.0 )\n",
        "    weights.append(wrow)\n",
        "weights = np.matrix( weights )\n",
        "\n",
        "\n",
        "NAG_result_prec_train = []\n",
        "NAG_result_loss_train = []\n",
        "z = np.dot(X, weights.T).reshape(-1,len(set(ytrain+ytest)))\n",
        "probs = np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1,1)\n",
        "loss =   -1 * np.mean(np.multiply(Y,  np.log(probs)) )\n",
        "NAG_result_loss_train.append( loss )\n",
        "NAG_result_prec_train.append( precision(np.argmax(probs, axis=1) ,ytrain) )\n",
        "\n",
        "NAG_result_prec_test = []\n",
        "NAG_result_loss_test = []\n",
        "z = np.dot(Xtest, weights.T).reshape(-1,len(set(ytest)))\n",
        "probs = np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1,1)\n",
        "loss =   -1 * np.mean(np.multiply(Ytest,  np.log(probs)) )\n",
        "NAG_result_loss_test.append( loss )\n",
        "NAG_result_prec_test.append( precision(np.argmax(probs, axis=1) ,ytest) )\n",
        "\n",
        "\n",
        "alpha0 = 0.01\n",
        "alpha1 = (1. + sqrt(1. + 4.0 * alpha0 * alpha0)) / 2.0\n",
        "start_time = time.time()\n",
        "for it in range(num_iter):\n",
        "    # np.dot(X, np.matrix(weights).T)  #.reshape(-1,len(self.classes))\n",
        "    z = np.dot(X, velocity.T).reshape(-1,len(set(ytrain+ytest)) )\n",
        "    \n",
        "    zi = z - np.max(z,-1)\n",
        "    h = np.exp(zi) / np.sum(np.exp(zi), axis=1)\n",
        "\n",
        "    gradient = -(h - Y).T.dot(X)\n",
        "\n",
        "\n",
        "    eta = (1 - alpha0) / alpha1\n",
        "    gamma = 1.0/(it+1)/ X.shape[0]\n",
        "    \n",
        "    MG = gradient          \n",
        "    # should be 'plus', 'cause to compute the MLE  \n",
        "    MtmpW = weights + (gamma + 0.0) * MG            \n",
        "    weights = (1.0-eta)*MtmpW + np.multiply(eta, velocity)\n",
        "    velocity = MtmpW\n",
        "\n",
        "    alpha0 = alpha1\n",
        "    alpha1 = (1. + sqrt(1. + 4.0 * alpha0 * alpha0)) / 2.0\n",
        "\n",
        "    zz = np.dot(X, weights.T).reshape(-1,len(set(ytrain)) )\n",
        "    zzi = zz - np.max(zz,-1)\n",
        "    pp = np.exp(zzi) / np.sum(np.exp(zzi), axis=1)\n",
        "    loss =  -1 * np.mean(np.multiply(Y,  np.log(pp)) )\n",
        "    prec = precision(np.argmax(pp, axis=1) ,ytrain)\n",
        "    NAG_result_loss_train.append( loss )\n",
        "    NAG_result_prec_train.append( prec )\n",
        "    #print(' Training Accuray at {:3d} iterations is {:.12f} with loss: {:.12f}'.format(1+it, prec, loss) )\n",
        "    zz = np.dot(Xtest, weights.T).reshape(-1,len(set(ytest)) )\n",
        "    zzi = zz - np.max(zz,-1)\n",
        "    pp = np.exp(zzi) / np.sum(np.exp(zzi), axis=1)\n",
        "    loss =  -1 * np.mean(np.multiply(Ytest,  np.log(pp)) )\n",
        "    prec = precision(np.argmax(pp, axis=1) ,ytest)\n",
        "    NAG_result_loss_test.append( loss )\n",
        "    NAG_result_prec_test.append( prec )\n",
        "    print(' Testing Accuray at {:2d} iterations is {:.12f} with loss: {:.12f}'.format(1+it, prec, loss) )\n",
        "\n",
        "#############################################################################################################3\n",
        "# ======================================= SigmoidNAG without QG ======================================= \n",
        "# Build the initial weight matrix\n",
        "velocity = []\n",
        "for rowidx in range( len(set(ytrain)) ):\n",
        "    vrow = []\n",
        "    for colidx in range( X.shape[1] ):\n",
        "        vrow.append( 0.0 )\n",
        "    velocity.append(vrow)\n",
        "velocity = np.matrix( velocity )\n",
        "weights = []\n",
        "for rowidx in range( len(set(ytest+ytrain)) ):\n",
        "    wrow = []\n",
        "    for colidx in range( X.shape[1] ):\n",
        "        wrow.append( 0.0 )\n",
        "    weights.append(wrow)\n",
        "weights = np.matrix( weights )\n",
        "\n",
        "\n",
        "\n",
        "SigmoidNAG_result_prec_train = []\n",
        "SigmoidNAG_result_loss_train = []\n",
        "z = np.dot(X, weights.T).reshape(-1,len(set(ytrain+ytest)))\n",
        "probs =  1.0 / ( 1.0 + np.exp(-z) ) \n",
        "loss =   np.sum( np.log( np.absolute(probs -1 + Y) )  )\n",
        "SigmoidNAG_result_loss_train.append( loss )\n",
        "SigmoidNAG_result_prec_train.append( precision(np.argmax(probs, axis=1) ,ytrain) )\n",
        "\n",
        "SigmoidNAG_result_prec_test = []\n",
        "SigmoidNAG_result_loss_test = []\n",
        "z = np.dot(Xtest, weights.T).reshape(-1,len(set(ytest)))\n",
        "probs =  1.0 / ( 1.0 + np.exp(-z) ) \n",
        "loss =   np.sum( np.log( np.absolute(probs -1 + Ytest) )  )\n",
        "SigmoidNAG_result_loss_test.append( loss )\n",
        "SigmoidNAG_result_prec_test.append( precision(np.argmax(probs, axis=1) ,ytest) )\n",
        "\n",
        "\n",
        "alpha0 = 0.01\n",
        "alpha1 = (1. + sqrt(1. + 4.0 * alpha0 * alpha0)) / 2.0\n",
        "start_time = time.time()\n",
        "for it in range(num_iter):\n",
        "\n",
        "    # np.dot(X, np.matrix(weights).T)  #.reshape(-1,len(self.classes))\n",
        "    z = np.dot(X, velocity.T).reshape(-1,len(set(ytest+ytrain)) ) \n",
        "    \n",
        "    #P = 1.0 / ( 1.0 + np.exp(-z) ) \n",
        "    P = hlambda(z)\n",
        "\n",
        "    \n",
        "    gradient = (Y - P).T.dot(X)\n",
        "   \n",
        "\n",
        "    eta = (1 - alpha0) / alpha1\n",
        "    gamma = 1.0/(it+1)/ X.shape[0]\n",
        "            \n",
        "    # should be 'plus', 'cause to compute the MLE\n",
        "    MtmpW = weights + (gamma + 0.0) * gradient               \n",
        "    weights = (1.0-eta)*MtmpW + np.multiply(eta, velocity)\n",
        "    velocity = MtmpW\n",
        "\n",
        "    alpha0 = alpha1\n",
        "    alpha1 = (1. + sqrt(1. + 4.0 * alpha0 * alpha0)) / 2.0\n",
        "\n",
        "\n",
        "\n",
        "    zz = np.dot(X, weights.T).reshape(-1,len(set(ytest+ytrain)) )\n",
        "    pp = 1.0 / ( 1.0 + np.exp(-zz) ) \n",
        "    loss =  np.sum( np.log( np.absolute(pp -1 + Y) )  )\n",
        "    prec = precision(np.argmax(pp, axis=1) ,ytrain)\n",
        "    SigmoidNAG_result_loss_train.append( loss )\n",
        "    SigmoidNAG_result_prec_train.append( prec )\n",
        "    #print('SigmoidNAG without QG Training Accuray at {:3d} iterations is {:.12f} with loss: {:.12f}'.format(1+it, prec, loss) )\n",
        "    zz = np.dot(Xtest, weights.T).reshape(-1,len(set(ytest)) )\n",
        "    pp = 1.0 / ( 1.0 + np.exp(-zz) ) \n",
        "    loss =  np.sum( np.log( np.absolute(pp -1 + Ytest) )  )\n",
        "    prec = precision(np.argmax(pp, axis=1) ,ytest)\n",
        "    SigmoidNAG_result_loss_test.append( loss )\n",
        "    SigmoidNAG_result_prec_test.append( prec )\n",
        "    print('SigmoidNAG without QG Testing Accuray at {:3d} iterations is {:.12f} with loss: {:.12f}'.format(1+it, prec, loss) )\n",
        "\n",
        "\n",
        "#############################################################################################################3\n",
        "\n",
        "# ======================================= SigmoidNAG with QG ======================================= \n",
        "# Build the initial weight matrix\n",
        "velocity = []\n",
        "for rowidx in range( len(set(ytrain)) ):\n",
        "    vrow = []\n",
        "    for colidx in range( X.shape[1] ):\n",
        "        vrow.append( 0.0 )\n",
        "    velocity.append(vrow)\n",
        "velocity = np.matrix( velocity )\n",
        "weights = []\n",
        "for rowidx in range( len(set(ytest+ytrain)) ):\n",
        "    wrow = []\n",
        "    for colidx in range( X.shape[1] ):\n",
        "        wrow.append( 0.0 )\n",
        "    weights.append(wrow)\n",
        "weights = np.matrix( weights )\n",
        "\n",
        "\n",
        "SigmoidNAGQG_result_prec_train = []\n",
        "SigmoidNAGQG_result_loss_train = []\n",
        "z = np.dot(X, weights.T).reshape(-1,len(set(ytrain+ytest)))\n",
        "probs =  1.0 / ( 1.0 + np.exp(-z) ) \n",
        "loss =   np.sum( np.log( np.absolute(probs -1 + Y) )  ) \n",
        "SigmoidNAGQG_result_loss_train.append( loss )\n",
        "SigmoidNAGQG_result_prec_train.append( precision(np.argmax(probs, axis=1) ,ytrain) )\n",
        "\n",
        "SigmoidNAGQG_result_prec_test = []\n",
        "SigmoidNAGQG_result_loss_test = []\n",
        "z = np.dot(Xtest, weights.T).reshape(-1,len(set(ytest)))\n",
        "probs =  1.0 / ( 1.0 + np.exp(-z) ) \n",
        "loss =    np.sum( np.log( np.absolute(probs -1 + Ytest) )  )  \n",
        "SigmoidNAGQG_result_loss_test.append( loss )\n",
        "SigmoidNAGQG_result_prec_test.append( precision(np.argmax(probs, axis=1) ,ytest) )\n",
        "\n",
        "print('X := ')\n",
        "print(X)\n",
        "start_time = time.time()\n",
        "XTX = X.T.dot(X)\n",
        "print('XTX := ')\n",
        "print(XTX)\n",
        "#B = np.sum((XTX * .5), axis=0) + epsilon\n",
        "invBrow = 1.0 / ( epsilon +  .25 * np.sum(XTX, axis=0) )\n",
        "print('invBrow := ')\n",
        "print(invBrow)\n",
        "\n",
        "alpha0 = 0.01\n",
        "alpha1 = (1. + sqrt(1. + 4.0 * alpha0 * alpha0)) / 2.0\n",
        "start_time = time.time()\n",
        "for it in range(num_iter):\n",
        "\n",
        "    # np.dot(X, np.matrix(weights).T)  #.reshape(-1,len(self.classes))\n",
        "    z = np.dot(X, velocity.T).reshape(-1,len(set(ytest+ytrain)) ) \n",
        "    \n",
        "    #P = 1.0 / ( 1.0 + np.exp(-z) ) \n",
        "    P = hlambda(z)\n",
        "\n",
        "    \n",
        "    gradient = (Y - P).T.dot(X)\n",
        "  \n",
        "    MG = np.multiply(invBrow, gradient)  \n",
        "\n",
        "    eta = (1 - alpha0) / alpha1\n",
        "    gamma = 1.0/(it+1)/ X.shape[0]\n",
        "            \n",
        "    # should be 'plus', 'cause to compute the MLE\n",
        "    MtmpW = weights + (gamma + 1.0) * MG               \n",
        "    weights = (1.0-eta)*MtmpW + np.multiply(eta, velocity)\n",
        "    velocity = MtmpW\n",
        "\n",
        "    alpha0 = alpha1\n",
        "    alpha1 = (1. + sqrt(1. + 4.0 * alpha0 * alpha0)) / 2.0\n",
        "\n",
        "\n",
        "\n",
        "    zz = np.dot(X, weights.T).reshape(-1,len(set(ytest+ytrain)) )\n",
        "    pp = 1.0 / ( 1.0 + np.exp(-zz) ) \n",
        "    loss =  np.sum( np.log( np.absolute(pp -1 + Y) )  )\n",
        "    prec = precision(np.argmax(pp, axis=1) ,ytrain)\n",
        "    SigmoidNAGQG_result_loss_train.append( loss )\n",
        "    SigmoidNAGQG_result_prec_train.append( prec )\n",
        "    #print('SigmoidNAG with QG Training Accuray at {:3d} iterations is {:.12f} with loss: {:.12f}'.format(1+it, prec, loss) )\n",
        "    zz = np.dot(Xtest, weights.T).reshape(-1,len(set(ytest)) )\n",
        "    pp = 1.0 / ( 1.0 + np.exp(-zz) ) \n",
        "    loss =  np.sum( np.log( np.absolute(pp -1 + Ytest) )  )\n",
        "    prec = precision(np.argmax(pp, axis=1) ,ytest)\n",
        "    SigmoidNAGQG_result_loss_test.append( loss )\n",
        "    SigmoidNAGQG_result_prec_test.append( prec )\n",
        "    print('SigmoidNAG with QG Testing Accuray at {:3d} iterations is {:.12f} with loss: {:.12f}'.format(1+it, prec, loss) )\n",
        "\n",
        "\n",
        "EnAdagradTimeDiff = time.time() - start_time\n",
        "print(\"TotalEnAdagradTimeDiff = \"), print(EnAdagradTimeDiff)\n",
        "print(\"AveraEnAdagradTimeDiff = \"), print(EnAdagradTimeDiff/num_iter)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "label = [  'NAG', 'SigmoidNAG', 'SigmoidNAGQG' ]\n",
        "plt.plot(range(len(NAG_result_prec_train)), NAG_result_prec_train, 'v-b')\n",
        "plt.plot(range(len(SigmoidNAG_result_prec_train)), SigmoidNAG_result_prec_train, 'v-g')\n",
        "plt.plot(range(len(SigmoidNAGQG_result_prec_train)), SigmoidNAGQG_result_prec_train, 'o-r')\n",
        "plt.title('train precision')\n",
        "plt.legend(label, loc = 0, ncol = 1)  \n",
        "plt.grid()\n",
        "plt.show()\n",
        "#plt.savefig(\"prec_train_.pdf\")\n",
        "plt.close()\n",
        "plt.plot(range(len(NAG_result_prec_test)), NAG_result_prec_test, 'v-b')\n",
        "plt.plot(range(len(SigmoidNAG_result_prec_test)), SigmoidNAG_result_prec_test, 'v-g')\n",
        "plt.plot(range(len(SigmoidNAGQG_result_prec_test)), SigmoidNAGQG_result_prec_test, 'o-r')\n",
        "plt.title('test precision')\n",
        "plt.legend(label, loc = 0, ncol = 1)  \n",
        "plt.grid()\n",
        "plt.show()\n",
        "#plt.savefig(\"prec_test_.pdf\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "plt.plot(range(len(NAG_result_loss_train)), NAG_result_loss_train, 'v-b')\n",
        "plt.plot(range(len(SigmoidNAG_result_loss_train)), SigmoidNAG_result_loss_train, 'v-g')\n",
        "plt.plot(range(len(SigmoidNAGQG_result_loss_train)), SigmoidNAGQG_result_loss_train, 'o-r')\n",
        "plt.title('train loss')\n",
        "plt.legend(label, loc = 0, ncol = 1)  \n",
        "plt.grid()\n",
        "plt.show()\n",
        "plt.close()\n",
        "plt.plot(range(len(NAG_result_loss_test)), NAG_result_loss_test, 'v-b')\n",
        "plt.plot(range(len(SigmoidNAG_result_loss_test)), SigmoidNAG_result_loss_test, 'v-g')\n",
        "plt.plot(range(len(SigmoidNAGQG_result_loss_test)), SigmoidNAGQG_result_loss_test, 'o-r')\n",
        "plt.title('test loss')\n",
        "plt.legend(label, loc = 0, ncol = 1)  \n",
        "plt.grid()\n",
        "plt.show()\n",
        "#plt.savefig(\"loss_test_.pdf\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# -------------- FILE: LOSS training -------------- \n",
        "filePath = \"PythonExperiment_NAGvs.SigmoidNAGvs.SigmoidNAGQG_LOSS_training_MNIST.csv\";\n",
        "PythonExperiment =      open(filePath,      'w')\n",
        "PythonExperiment =      open(filePath,      'a+b')\n",
        "\n",
        "PythonExperiment.write(\"Iter\".encode()); \n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('RawNAG'.encode());\n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('SigmoidNAG'.encode());\n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('SigmoidNAGQG'.encode());   \n",
        "PythonExperiment.write(\"\\n\".encode());\n",
        "\n",
        "for (idx, ele) in enumerate(NAG_result_loss_train):\n",
        "    PythonExperiment.write(str(idx).encode()); \n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(NAG_result_loss_train[idx]).encode());\n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(SigmoidNAG_result_loss_train[idx]).encode());\n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(SigmoidNAGQG_result_loss_train[idx]).encode());\n",
        "    PythonExperiment.write(\"\\n\".encode());\n",
        "PythonExperiment.close();\n",
        "\n",
        "# -------------- FILE: LOSS testing -------------- \n",
        "filePath = \"PythonExperiment_NAGvs.SigmoidNAGvs.SigmoidNAGQG_LOSS_testing_MNIST.csv\";\n",
        "PythonExperiment =      open(filePath,      'w')\n",
        "PythonExperiment =      open(filePath,      'a+b')\n",
        "\n",
        "PythonExperiment.write(\"Iter\".encode()); \n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('RawNAG'.encode());\n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('SigmoidNAG'.encode());\n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('SigmoidNAGQG'.encode());   \n",
        "PythonExperiment.write(\"\\n\".encode());\n",
        "\n",
        "for (idx, ele) in enumerate(NAG_result_loss_test):\n",
        "    PythonExperiment.write(str(idx).encode()); \n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(NAG_result_loss_test[idx]).encode());\n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(SigmoidNAG_result_loss_test[idx]).encode());\n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(SigmoidNAGQG_result_loss_test[idx]).encode());\n",
        "    PythonExperiment.write(\"\\n\".encode());\n",
        "PythonExperiment.close();\n",
        "\n",
        "\n",
        "# -------------- FILE: PREC training -------------- \n",
        "filePath = \"PythonExperiment_NAGvs.SigmoidNAGvs.SigmoidNAGQG_PREC_training_MNIST.csv\";\n",
        "PythonExperiment =      open(filePath,      'w')\n",
        "PythonExperiment =      open(filePath,      'a+b')\n",
        "\n",
        "PythonExperiment.write(\"Iter\".encode()); \n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('RawNAG'.encode());\n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('SigmoidNAG'.encode());\n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('SigmoidNAGQG'.encode());   \n",
        "PythonExperiment.write(\"\\n\".encode());\n",
        "\n",
        "for (idx, ele) in enumerate(NAG_result_prec_train):\n",
        "    PythonExperiment.write(str(idx).encode()); \n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(NAG_result_prec_train[idx]).encode());\n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(SigmoidNAG_result_prec_train[idx]).encode());\n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(SigmoidNAGQG_result_prec_train[idx]).encode());\n",
        "    PythonExperiment.write(\"\\n\".encode());\n",
        "PythonExperiment.close();\n",
        "\n",
        "# -------------- FILE: PREC testing -------------- \n",
        "filePath = \"PythonExperiment_NAGvs.SigmoidNAGvs.SigmoidNAGQG_PREC_testing_MNIST.csv\";\n",
        "PythonExperiment =      open(filePath,      'w')\n",
        "PythonExperiment =      open(filePath,      'a+b')\n",
        "\n",
        "PythonExperiment.write(\"Iter\".encode()); \n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('RawNAG'.encode());\n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('SigmoidNAG'.encode());\n",
        "PythonExperiment.write(','.encode());\n",
        "PythonExperiment.write('SigmoidNAGQG'.encode());   \n",
        "PythonExperiment.write(\"\\n\".encode());\n",
        "\n",
        "for (idx, ele) in enumerate(NAG_result_prec_test):\n",
        "    PythonExperiment.write(str(idx).encode()); \n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(NAG_result_prec_test[idx]).encode());\n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(SigmoidNAG_result_prec_test[idx]).encode());\n",
        "    PythonExperiment.write(','.encode());\n",
        "    PythonExperiment.write(str(SigmoidNAGQG_result_prec_test[idx]).encode());\n",
        "    PythonExperiment.write(\"\\n\".encode());\n",
        "PythonExperiment.close();\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}